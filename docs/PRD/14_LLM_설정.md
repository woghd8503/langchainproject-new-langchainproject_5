# 14. LLM ì„¤ì •

## ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2025-10-30
- **í”„ë¡œì íŠ¸ëª…**: ë…¼ë¬¸ ë¦¬ë·° ì±—ë´‡ (AI Agent + RAG)
- **íŒ€ëª…**: ì—°ê²°ì˜ ë¯¼ì¡±

---

## 1. LLM ëª¨ë¸ ì„ íƒ

### 1.1 ê°œë°œ í™˜ê²½

```python
from langchain_openai import ChatOpenAI
from langchain_upstage import ChatUpstage

# ê°œë°œìš© Option 1: GPT-3.5-turbo (ë¹„ìš© ì ˆê°)
llm_openai_dev = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.0,
    max_tokens=2000,
    streaming=True
)

# ê°œë°œìš© Option 2: Solar-pro (í•œêµ­ì–´ íŠ¹í™”, ë¹„ìš© íš¨ìœ¨ì )
llm_solar_dev = ChatUpstage(
    model="solar-pro",
    temperature=0.0,
    max_tokens=2000,
    streaming=True
)
```

### 1.2 í”„ë¡œë•ì…˜ í™˜ê²½

```python
# í”„ë¡œë•ì…˜ Option 1: GPT-4 (ë†’ì€ í’ˆì§ˆ)
llm_openai_prod = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=3000,
    streaming=True
)

# í”„ë¡œë•ì…˜ Option 2: Solar-pro (í•œêµ­ì–´ ë‹µë³€, ë¹„ìš© íš¨ìœ¨)
llm_solar_prod = ChatUpstage(
    model="solar-pro",
    temperature=0.7,
    max_tokens=3000,
    streaming=True
)
```

### 1.3 í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ (ê¶Œì¥)

```python
# ë‚œì´ë„ë³„ ëª¨ë¸ ì„ íƒ
def get_llm(difficulty="easy", language="ko"):
    """
    ë‚œì´ë„ì™€ ì–¸ì–´ì— ë”°ë¼ ì ì ˆí•œ LLM ì„ íƒ

    - Easy ëª¨ë“œ + í•œêµ­ì–´: Solar (í•œêµ­ì–´ íŠ¹í™”, ì €ë¹„ìš©)
    - Hard ëª¨ë“œ + ì˜ì–´: GPT-4 (ê¸°ìˆ ì  ì •í™•ë„)
    """
    if difficulty == "easy" and language == "ko":
        return ChatUpstage(model="solar-pro", temperature=0.7)
    elif difficulty == "hard":
        return ChatOpenAI(model="gpt-4", temperature=0.7)
    else:
        return ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
```

---

## 2. API í‚¤ ê´€ë¦¬

### 2.1 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼
OPENAI_API_KEY=sk-...
SOLAR_API_KEY=up-...
TAVILY_API_KEY=tvly-...
DATABASE_URL=postgresql://user:password@localhost:5432/papers
```

### 2.2 ì½”ë“œì—ì„œ ë¡œë“œ

```python
import os
from dotenv import load_dotenv

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
solar_api_key = os.getenv("SOLAR_API_KEY")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
if not solar_api_key:
    raise ValueError("SOLAR_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
```

---

## 3. ì—ëŸ¬ í•¸ë“¤ë§

### 3.1 LLM API í˜¸ì¶œ íë¦„

```mermaid
sequenceDiagram
    autonumber
    participant Agent as AI Agent
    participant Client as LLM Client
    participant API1 as OpenAI API
    participant API2 as Solar API

    Agent->>Client: generate_answer(question, difficulty)
    Client->>Client: ë‚œì´ë„ë³„ ëª¨ë¸ ì„ íƒ

    alt Easy ëª¨ë“œ + í•œêµ­ì–´
        Client->>API2: Solar API í˜¸ì¶œ<br/>solar-pro
        API2-->>Client: í•œêµ­ì–´ ë‹µë³€
    else Hard ëª¨ë“œ or ì˜ì–´
        Client->>API1: OpenAI API í˜¸ì¶œ<br/>GPT-4
        API1-->>Client: ìƒì„¸ ë‹µë³€
    end

    Client-->>Agent: ìµœì¢… ë‹µë³€ ë°˜í™˜
```

### 3.2 ì—ëŸ¬ ì²˜ë¦¬ íë¦„

```mermaid
graph TB
    subgraph Request["ğŸ”¸ API ìš”ì²­"]
        direction LR
        A[LLM í˜¸ì¶œ ì‹œë„] --> B{ì—ëŸ¬<br/>ë°œìƒ?}
        B -->|No| C[âœ… ì •ìƒ ì‘ë‹µ<br/>ë°˜í™˜]
    end

    subgraph Retry["ğŸ”¹ ì¬ì‹œë„ ë¡œì§"]
        direction TB
        B -->|Yes| D{ì¬ì‹œë„<br/>íšŸìˆ˜<br/>< 3?}
        D -->|Yes| E[ëŒ€ê¸°<br/>2^nì´ˆ<br/>Exponential Backoff]
        E --> A
        D -->|No| F[ìµœì¢… ì‹¤íŒ¨<br/>ì—ëŸ¬ ë¡œê·¸]
    end

    subgraph Fallback["ğŸ”º ëŒ€ì²´ ì „ëµ"]
        direction LR
        F --> G{ëŒ€ì²´<br/>API<br/>ì‚¬ìš©?}
        G -->|Yes| H[ë‹¤ë¥¸ ëª¨ë¸ë¡œ<br/>ì¬ì‹œë„]
        G -->|No| I[âŒ ì‚¬ìš©ìì—ê²Œ<br/>ì—ëŸ¬ ë©”ì‹œì§€]
        H --> A
    end

    Request --> Retry
    Retry --> Fallback

    %% Subgraph ìŠ¤íƒ€ì¼
    style Request fill:#e1f5ff,stroke:#01579b,stroke-width:3px,color:#000
    style Retry fill:#f3e5f5,stroke:#4a148c,stroke-width:3px,color:#000
    style Fallback fill:#e8f5e9,stroke:#1b5e20,stroke-width:3px,color:#000

    %% ë…¸ë“œ ìŠ¤íƒ€ì¼
    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style C fill:#66bb6a,stroke:#2e7d32,color:#fff
    style D fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style E fill:#ce93d8,stroke:#7b1fa2,color:#000
    style F fill:#ef9a9a,stroke:#c62828,color:#000
    style G fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style H fill:#ffcc80,stroke:#f57c00,color:#000
    style I fill:#ef9a9a,stroke:#c62828,color:#000
```

### 3.3 ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def llm_invoke_with_retry(prompt):
    """LLM í˜¸ì¶œ ì‹œ ìë™ ì¬ì‹œë„"""
    try:
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        logger.write(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise
```

### 3.2 íƒ€ì„ì•„ì›ƒ ì„¤ì •

```python
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    request_timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    max_retries=2
)
```

---

## 4. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 

### 4.1 í† í° ì¹´ìš´íŒ…

```python
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    response = llm.invoke("ë…¼ë¬¸ ìš”ì•½í•´ì¤˜")

    logger.write(f"ì´ í† í°: {cb.total_tokens}")
    logger.write(f"í”„ë¡¬í”„íŠ¸ í† í°: {cb.prompt_tokens}")
    logger.write(f"ì™„ì„± í† í°: {cb.completion_tokens}")
    logger.write(f"ì´ ë¹„ìš©: ${cb.total_cost}")
```

### 4.2 ë¹„ìš© ëª¨ë‹ˆí„°ë§

```python
def monitor_cost(func):
    """ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        with get_openai_callback() as cb:
            result = func(*args, **kwargs)
            logger.write(f"[ë¹„ìš©] {func.__name__}: ${cb.total_cost:.4f}")
            return result
    return wrapper

@monitor_cost
def generate_answer(question):
    return llm.invoke(question)
```

---

## 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

### 5.1 ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°

```python
llm = ChatOpenAI(
    model="gpt-4",
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

for chunk in llm.stream("ë…¼ë¬¸ ìš”ì•½í•´ì¤˜"):
    print(chunk.content, end="", flush=True)
```

### 5.2 Streamlit í†µí•©

```python
from langchain.callbacks import StreamlitCallbackHandler

import streamlit as st

st_callback = StreamlitCallbackHandler(st.container())

llm = ChatOpenAI(
    model="gpt-4",
    streaming=True,
    callbacks=[st_callback]
)
```

---

## 6. ëª¨ë¸ íŒŒë¼ë¯¸í„°

### 6.1 Temperature ì„¤ì •

| ê°’ | ìš©ë„ | ì„¤ëª… |
|----|------|------|
| 0.0 | ê²€ìƒ‰, ë¼ìš°íŒ… | ê²°ì •ë¡ ì , ì¼ê´€ëœ ì¶œë ¥ |
| 0.3-0.5 | ìš”ì•½, ë¶„ë¥˜ | ì•½ê°„ì˜ ì°½ì˜ì„± |
| 0.7-0.9 | ë‹µë³€ ìƒì„± | ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ |

```python
# ë¼ìš°íŒ…ìš©: ë‚®ì€ temperature
router_llm = ChatOpenAI(model="gpt-4", temperature=0.0)

# ë‹µë³€ ìƒì„±ìš©: ë†’ì€ temperature
answer_llm = ChatOpenAI(model="gpt-4", temperature=0.7)
```

### 6.2 Max Tokens ì„¤ì •

```python
llm = ChatOpenAI(
    model="gpt-4",
    max_tokens=3000,  # ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
    stop=["\n\n---\n\n"]  # ì¤‘ë‹¨ ì‹œí€€ìŠ¤
)
```

---

## 7. ì°¸ê³  ìë£Œ

- OpenAI API: https://platform.openai.com/docs/api-reference
- Langchain ChatOpenAI: https://python.langchain.com/docs/integrations/chat/openai/
