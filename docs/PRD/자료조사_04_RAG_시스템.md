# ìë£Œì¡°ì‚¬: RAG ì‹œìŠ¤í…œ ì„¤ê³„

## ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2025-10-29
- **í”„ë¡œì íŠ¸**: ë…¼ë¬¸ ë¦¬ë·° ì±—ë´‡ (AI Agent + RAG)
- **íŒ€ëª…**: ì—°ê²°ì˜ ë¯¼ì¡±

---

## 1. RAG (Retrieval-Augmented Generation) ê°œìš”

### 1.1 RAGë€?

**RAG**ëŠ” ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤(Knowledge Base)ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰(Retrieval)í•˜ì—¬ LLMì˜ ë‹µë³€ ìƒì„±(Generation)ì„ ë³´ê°•í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

### 1.2 RAGì˜ í•„ìš”ì„±

1. **LLMì˜ í•œê³„ ê·¹ë³µ**
   - LLMì€ í•™ìŠµ ë°ì´í„° ê¸°ì¤€ ì‹œì  ì´í›„ì˜ ì •ë³´ë¥¼ ëª¨ë¦„
   - íŠ¹ì • ë„ë©”ì¸(ë…¼ë¬¸)ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì§€ì‹ ë¶€ì¡±
   - Hallucination(í™˜ê°) ë¬¸ì œ í•´ê²°

2. **ë…¼ë¬¸ ë¦¬ë·° ì±—ë´‡ì—ì„œì˜ ì¤‘ìš”ì„±**
   - ìˆ˜ì²œ ê°œì˜ ë…¼ë¬¸ ì •ë³´ë¥¼ LLMì´ ì§ì ‘ í•™ìŠµ ë¶ˆê°€ëŠ¥
   - ì •í™•í•œ ì¶œì²˜ì™€ ì¸ìš© ì œê³µ í•„ìš”
   - ìµœì‹  ë…¼ë¬¸ê¹Œì§€ ì»¤ë²„ ê°€ëŠ¥

---

## 2. RAG ì‹œìŠ¤í…œ êµ¬ì¡°

### 2.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
[ë°ì´í„° ìˆ˜ì§‘] â†’ [ì „ì²˜ë¦¬] â†’ [ì²­í¬ ë¶„í• ] â†’ [ì„ë² ë”©] â†’ [Vector DB ì €ì¥]
                                                            â†“
[ì‚¬ìš©ì ì§ˆë¬¸] â†’ [ì§ˆë¬¸ ì„ë² ë”©] â†’ [ìœ ì‚¬ë„ ê²€ìƒ‰] â†’ [ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ] â†’ [LLMì— ì „ë‹¬] â†’ [ë‹µë³€ ìƒì„±]
```

### 2.2 Mermaid ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph Prepare["ğŸ”¸ ë‹¨ê³„ 1: ë°ì´í„° ì¤€ë¹„ & ì €ì¥"]
        direction LR
        A[ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘<br/>ğŸ“Š arXiv, Scholar] --> B[ë°ì´í„° ì „ì²˜ë¦¬<br/>PDF â†’ í…ìŠ¤íŠ¸]
        B --> C[í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• <br/>1000ì ë‹¨ìœ„]
        C --> D[ì„ë² ë”© ë²¡í„° ìƒì„±<br/>OpenAI Embeddings]
        D --> E[ğŸ’¾ Vector DB ì €ì¥<br/>ChromaDB]
    end

    subgraph Search["ğŸ”¹ ë‹¨ê³„ 2: ê²€ìƒ‰ & ì¡°íšŒ"]
        direction LR
        F[ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥] --> G[ì§ˆë¬¸ ì„ë² ë”©<br/>ë²¡í„° ë³€í™˜]
        G --> H[ìœ ì‚¬ë„ ê²€ìƒ‰<br/>Cosine Similarity]
        H --> I[ê´€ë ¨ ë¬¸ì„œ Top-K<br/>ìƒìœ„ 5ê°œ ì¡°íšŒ]
    end

    subgraph Generate["ğŸ”º ë‹¨ê³„ 3: ë‹µë³€ ìƒì„±"]
        direction LR
        J[ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±<br/>ë¬¸ì„œ + ì§ˆë¬¸] --> K[LLM ë‹µë³€ ìƒì„±<br/>GPT-4]
        K --> L[âœ… ì‚¬ìš©ì ì‘ë‹µ<br/>ë‚œì´ë„ ì ìš©]
    end

    %% ë‹¨ê³„ ê°„ ì—°ê²°
    Prepare --> Search
    Search --> Generate
    E -.-> H
    I --> J

    %% Subgraph ìŠ¤íƒ€ì¼
    style Prepare fill:#e1f5ff,stroke:#01579b,stroke-width:3px,color:#000
    style Search fill:#f3e5f5,stroke:#4a148c,stroke-width:3px,color:#000
    style Generate fill:#e8f5e9,stroke:#1b5e20,stroke-width:3px,color:#000

    %% ë…¸ë“œ ìŠ¤íƒ€ì¼ (ë°ì´í„° ì¤€ë¹„ - íŒŒë‘ ê³„ì—´)
    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#81d4fa,stroke:#0288d1,color:#000
    style C fill:#64b5f6,stroke:#1976d2,color:#000
    style D fill:#42a5f5,stroke:#1565c0,color:#000
    style E fill:#1e88e5,stroke:#0d47a1,color:#fff

    %% ë…¸ë“œ ìŠ¤íƒ€ì¼ (ê²€ìƒ‰ - ë³´ë¼ ê³„ì—´)
    style F fill:#ce93d8,stroke:#7b1fa2,color:#000
    style G fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style H fill:#ab47bc,stroke:#4a148c,color:#fff
    style I fill:#9c27b0,stroke:#4a148c,color:#fff

    %% ë…¸ë“œ ìŠ¤íƒ€ì¼ (ë‹µë³€ ìƒì„± - ë…¹ìƒ‰ ê³„ì—´)
    style J fill:#a5d6a7,stroke:#388e3c,color:#000
    style K fill:#81c784,stroke:#2e7d32,color:#000
    style L fill:#66bb6a,stroke:#2e7d32,color:#fff
```

---

## 3. ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬

### 3.1 ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘

**ë°ì´í„° ì†ŒìŠ¤:**
1. arXiv API (ìµœì‹  ë…¼ë¬¸)
2. Google Scholar (ì¸ìš© ì •ë³´)
3. Semantic Scholar API (ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°)
4. ìˆ˜ë™ ì—…ë¡œë“œ (PDF íŒŒì¼)

### 3.2 ë°ì´í„° í…ìŠ¤íŠ¸í™”

**ì§€ì› í˜•ì‹:**
- PDF â†’ `PyPDF2`, `pdfplumber`
- HTML â†’ `BeautifulSoup4`
- LaTeX â†’ `pandoc`

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
from langchain.document_loaders import PyPDFLoader, ArxivLoader

# PDF íŒŒì¼ ë¡œë“œ
pdf_loader = PyPDFLoader("data/raw/transformer_paper.pdf")
documents = pdf_loader.load()

# arXivì—ì„œ ì§ì ‘ ë¡œë“œ
arxiv_loader = ArxivLoader(query="attention mechanism", max_docs=10)
arxiv_docs = arxiv_loader.load()
```

### 3.3 ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

**ì¶”ì¶œ ì •ë³´:**
- ì œëª© (Title)
- ì €ì (Authors)
- ì¶œíŒì¼ (Publication Date)
- ì¶œì²˜ (Source: arXiv, IEEE, ACL ë“±)
- í‚¤ì›Œë“œ/ì¹´í…Œê³ ë¦¬ (Keywords/Category)
- DOI/URL
- ì´ˆë¡ (Abstract)

**PostgreSQL ì €ì¥:**
```python
import psycopg2

def save_paper_metadata(paper_data):
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO papers (title, authors, publish_date, source, url, abstract, category)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        RETURNING paper_id
    """, (
        paper_data['title'],
        paper_data['authors'],
        paper_data['publish_date'],
        paper_data['source'],
        paper_data['url'],
        paper_data['abstract'],
        paper_data['category']
    ))

    paper_id = cursor.fetchone()[0]
    conn.commit()
    return paper_id
```

---

## 4. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (Text Splitting)

### 4.1 ì²­í¬ ë¶„í• ì˜ í•„ìš”ì„±

- LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ
- ì„ë² ë”© ëª¨ë¸ì˜ í† í° ì œí•œ
- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (ì‘ì€ ì²­í¬ê°€ ë” ê´€ë ¨ì„± ë†’ì€ ì •ë³´ í¬í•¨)

### 4.2 ì²­í¬ ë¶„í•  ì „ëµ

**RecursiveCharacterTextSplitter (ì¶”ì²œ)**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
    chunk_overlap=200,  # ì²­í¬ ê°„ ì¤‘ë³µ (ë§¥ë½ ìœ ì§€)
    separators=["\n\n", "\n", ". ", " ", ""],  # ë¶„í•  ìš°ì„ ìˆœìœ„
    length_function=len
)

chunks = text_splitter.split_documents(documents)
```

**ë…¼ë¬¸ êµ¬ì¡° ê¸°ë°˜ ë¶„í•  (ê³ ê¸‰)**

ë…¼ë¬¸ì˜ ì„¹ì…˜ êµ¬ì¡°ë¥¼ í™œìš©í•œ ë¶„í• :
- Abstract (ì´ˆë¡)
- Introduction (ì„œë¡ )
- Related Work (ê´€ë ¨ ì—°êµ¬)
- Method (ë°©ë²•ë¡ )
- Experiments (ì‹¤í—˜)
- Conclusion (ê²°ë¡ )

```python
def split_by_sections(paper_text):
    sections = {
        "abstract": extract_section(paper_text, "Abstract"),
        "introduction": extract_section(paper_text, "Introduction"),
        "method": extract_section(paper_text, "Method"),
        "experiments": extract_section(paper_text, "Experiments"),
        "conclusion": extract_section(paper_text, "Conclusion")
    }
    return sections
```

**ì²­í¬ í¬ê¸° ê¶Œì¥ì‚¬í•­:**
- **Small (500-800ì)**: ì •í™•í•œ ê²€ìƒ‰, ì‘ì€ ì§ˆë¬¸ì— ì í•©
- **Medium (1000-1500ì)**: ê· í˜•ì¡íŒ ì„ íƒ (ì¶”ì²œ)
- **Large (2000-3000ì)**: ë„“ì€ ë§¥ë½, ìš”ì•½ ì‘ì—…ì— ì í•©

---

## 5. ì„ë² ë”© (Embedding)

### 5.1 ì„ë² ë”© ëª¨ë¸ ì„ íƒ

**OpenAI Embedding Models:**

| ëª¨ë¸ | ì°¨ì› | ë¹„ìš© (1M tokens) | ì‚¬ìš© ê¶Œì¥ |
|------|------|------------------|-----------|
| text-embedding-3-small | 1536 | $0.02 | âœ… ê¶Œì¥ (ë¹„ìš© íš¨ìœ¨) |
| text-embedding-3-large | 3072 | $0.13 | ë†’ì€ ì •í™•ë„ í•„ìš” ì‹œ |
| text-embedding-ada-002 | 1536 | $0.10 | êµ¬ë²„ì „ |

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key="your-api-key"
)

# í…ìŠ¤íŠ¸ ì„ë² ë”©
vector = embeddings.embed_query("Transformer architecture")
print(len(vector))  # 1536
```

### 5.2 ì„ë² ë”© ì €ì¥

**Vector DBì— ì €ì¥:**

```python
from langchain.vectorstores import Chroma

# ChromaDB ì´ˆê¸°í™”
vectorstore = Chroma(
    collection_name="paper_embeddings",
    embedding_function=embeddings,
    persist_directory="data/vectordb"
)

# ë¬¸ì„œ ì¶”ê°€
vectorstore.add_documents(chunks)
```

---

## 6. Vector Database ì„¤ê³„

### 6.1 ì»¬ë ‰ì…˜ êµ¬ì¡°

**3ê°œì˜ ì»¬ë ‰ì…˜ ìš´ì˜:**

#### 1. `paper_chunks` (ë…¼ë¬¸ ë³¸ë¬¸)
- **ìš©ë„**: ë…¼ë¬¸ ì „ì²´ ë‚´ìš© ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„°**: paper_id, section, page_num

#### 2. `paper_abstracts` (ë…¼ë¬¸ ì´ˆë¡)
- **ìš©ë„**: ë¹ ë¥¸ ë…¼ë¬¸ ê°œìš” ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„°**: paper_id, title, authors

#### 3. `glossary_embeddings` (ìš©ì–´ì§‘)
- **ìš©ë„**: ì „ë¬¸ ìš©ì–´ ì •ì˜ ê²€ìƒ‰
- **ë©”íƒ€ë°ì´í„°**: term, category, difficulty_level

### 6.2 ì»¬ë ‰ì…˜ë³„ êµ¬í˜„

```python
# 1. ë…¼ë¬¸ ë³¸ë¬¸ ì»¬ë ‰ì…˜
paper_chunks_store = Chroma(
    collection_name="paper_chunks",
    embedding_function=embeddings,
    persist_directory="data/vectordb/chunks"
)

# 2. ë…¼ë¬¸ ì´ˆë¡ ì»¬ë ‰ì…˜
abstract_store = Chroma(
    collection_name="paper_abstracts",
    embedding_function=embeddings,
    persist_directory="data/vectordb/abstracts"
)

# 3. ìš©ì–´ì§‘ ì»¬ë ‰ì…˜
glossary_store = Chroma(
    collection_name="glossary_embeddings",
    embedding_function=embeddings,
    persist_directory="data/vectordb/glossary"
)
```

---

## 7. ìš©ì–´ì§‘(Glossary) ê´€ë¦¬ ì‹œìŠ¤í…œ â˜… ì¤‘ìš”

### 7.1 ìš©ì–´ì§‘ì˜ í•„ìš”ì„±

ë…¼ë¬¸ì—ëŠ” ì „ë¬¸ ìš©ì–´ê°€ ë§ì´ ë“±ì¥í•˜ë©°, ì´ˆì‹¬ìëŠ” ì´í•´í•˜ê¸° ì–´ë ¤ì›€:
- "Attention Mechanism"
- "Fine-tuning"
- "BLEU Score"
- "Backpropagation"

### 7.2 ìš©ì–´ì§‘ ë°ì´í„° êµ¬ì¡°

**PostgreSQL í…Œì´ë¸”:**

```sql
CREATE TABLE glossary (
    term_id SERIAL PRIMARY KEY,
    term VARCHAR(200) NOT NULL UNIQUE,
    definition TEXT NOT NULL,
    easy_explanation TEXT,  -- ì´ˆì‹¬ììš© ì„¤ëª…
    hard_explanation TEXT,  -- ì „ë¬¸ê°€ìš© ì„¤ëª…
    category VARCHAR(100),  -- ML, NLP, CV, RL ë“±
    difficulty_level VARCHAR(20),  -- beginner, intermediate, advanced
    related_terms TEXT[],  -- ê´€ë ¨ ìš©ì–´
    examples TEXT,  -- ì‚¬ìš© ì˜ˆì‹œ
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì˜ˆì‹œ ë°ì´í„°
INSERT INTO glossary (term, definition, easy_explanation, hard_explanation, category, difficulty_level)
VALUES (
    'Attention Mechanism',
    'A technique that allows models to focus on specific parts of the input when generating output.',
    'ì±…ì„ ì½ì„ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì²˜ëŸ¼, AIê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.',
    'A weighted sum mechanism that computes attention scores between query and key vectors, allowing the model to dynamically focus on relevant input positions during sequence processing.',
    'Deep Learning',
    'intermediate'
);
```

### 7.3 ìš©ì–´ì§‘ RAG í™œìš© ë°©ì•ˆ

#### ë°©ì•ˆ 1: ìš©ì–´ì§‘ì„ Vector DBì— ì„ë² ë”© ì €ì¥

**ì¥ì :**
- ì‚¬ìš©ì ì§ˆë¬¸ì— ìš©ì–´ê°€ í¬í•¨ë˜ë©´ ìë™ìœ¼ë¡œ ê²€ìƒ‰ë¨
- ìœ ì‚¬í•œ ìš©ì–´ë„ í•¨ê»˜ ì°¾ì•„ì¤Œ (ì˜ˆ: "ì–´í…ì…˜" â†’ "Attention")

**êµ¬í˜„:**
```python
# ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ Vector DBì— ì €ì¥
def add_glossary_to_vectordb():
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    cursor.execute("SELECT term, definition, easy_explanation, category FROM glossary")
    glossary_items = cursor.fetchall()

    documents = []
    for term, definition, easy_exp, category in glossary_items:
        doc_content = f"ìš©ì–´: {term}\nì •ì˜: {definition}\nì‰¬ìš´ ì„¤ëª…: {easy_exp}"
        documents.append(Document(
            page_content=doc_content,
            metadata={"term": term, "category": category, "type": "glossary"}
        ))

    glossary_store.add_documents(documents)

add_glossary_to_vectordb()
```

#### ë°©ì•ˆ 2: ì§ˆë¬¸ ë¶„ì„ ì‹œ ìš©ì–´ ìë™ ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€

**êµ¬í˜„:**
```python
def extract_and_add_glossary_context(user_query):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì „ë¬¸ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìš©ì–´ì§‘ ì •ì˜ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    """
    # ìš©ì–´ì§‘ì—ì„œ ìš©ì–´ ê²€ìƒ‰
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    # ì§ˆë¬¸ì—ì„œ ìš©ì–´ ì°¾ê¸° (ê°„ë‹¨í•œ ë§¤ì¹­)
    cursor.execute("""
        SELECT term, definition, easy_explanation
        FROM glossary
        WHERE %s ILIKE '%' || term || '%'
    """, (user_query,))

    terms_found = cursor.fetchall()

    if terms_found:
        glossary_context = "\n\n[ìš©ì–´ ì •ì˜]\n"
        for term, definition, easy_exp in terms_found:
            glossary_context += f"- **{term}**: {easy_exp}\n"

        return glossary_context
    return ""

# ì‚¬ìš© ì˜ˆì‹œ
user_query = "Attention Mechanismì´ ë­ì•¼?"
glossary_context = extract_and_add_glossary_context(user_query)

final_prompt = f"""
{glossary_context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ë‹µë³€:
"""
```

#### ë°©ì•ˆ 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Hybrid Search)

**ìš©ì–´ì§‘ + ë…¼ë¬¸ ë³¸ë¬¸ ë™ì‹œ ê²€ìƒ‰:**

```python
def hybrid_search(query, difficulty="easy"):
    """
    ìš©ì–´ì§‘ê³¼ ë…¼ë¬¸ ë³¸ë¬¸ì„ ë™ì‹œì— ê²€ìƒ‰í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„±
    """
    # 1. ìš©ì–´ì§‘ ê²€ìƒ‰
    glossary_results = glossary_store.similarity_search(query, k=2)

    # 2. ë…¼ë¬¸ ë³¸ë¬¸ ê²€ìƒ‰
    paper_results = paper_chunks_store.similarity_search(query, k=3)

    # 3. ê²°ê³¼ ê²°í•©
    combined_context = "### ìš©ì–´ ì •ì˜:\n"
    for doc in glossary_results:
        combined_context += doc.page_content + "\n\n"

    combined_context += "### ë…¼ë¬¸ ë‚´ìš©:\n"
    for doc in paper_results:
        combined_context += doc.page_content + "\n\n"

    # 4. LLMì— ì „ë‹¬
    if difficulty == "easy":
        prompt = f"{combined_context}\n\nì§ˆë¬¸: {query}\n\nì´ˆì‹¬ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    else:
        prompt = f"{combined_context}\n\nì§ˆë¬¸: {query}\n\nì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

    return llm.invoke(prompt)
```

### 7.4 ìš©ì–´ì§‘ ìë™ ìƒì„±

**ë…¼ë¬¸ì—ì„œ ìë™ìœ¼ë¡œ ìš©ì–´ ì¶”ì¶œ:**

```python
def auto_generate_glossary_from_papers():
    """
    ë…¼ë¬¸ì—ì„œ ì¤‘ìš” ìš©ì–´ë¥¼ ìë™ ì¶”ì¶œí•˜ì—¬ ìš©ì–´ì§‘ì— ì¶”ê°€
    """
    # 1. ë…¼ë¬¸ì—ì„œ ì£¼ìš” ìš©ì–´ ì¶”ì¶œ (NER ë˜ëŠ” LLM í™œìš©)
    extraction_prompt = """
    ë‹¤ìŒ ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•œ ê¸°ìˆ  ìš©ì–´ 5ê°œë¥¼ ì¶”ì¶œí•˜ê³  ê°„ë‹¨íˆ ì •ì˜í•´ì£¼ì„¸ìš”:

    ë…¼ë¬¸ ë‚´ìš©: {paper_content}

    ì¶œë ¥ í˜•ì‹:
    1. ìš©ì–´: ì •ì˜
    2. ìš©ì–´: ì •ì˜
    ...
    """

    # 2. LLMìœ¼ë¡œ ìš©ì–´ ì¶”ì¶œ
    terms = llm.invoke(extraction_prompt)

    # 3. PostgreSQL ìš©ì–´ì§‘ì— ì¶”ê°€
    # (ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€)
```

---

## 8. ê²€ìƒ‰ (Retrieval) ì „ëµ

### 8.1 ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰

```python
# Top-K ê²€ìƒ‰
results = vectorstore.similarity_search(
    query="Transformer architecture",
    k=5  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ì¡°íšŒ
)
```

### 8.2 MMR (Maximal Marginal Relevance) ê²€ìƒ‰

**ëª©ì :** ê´€ë ¨ì„± ë†’ìœ¼ë©´ì„œë„ ë‹¤ì–‘í•œ ë¬¸ì„œ ê²€ìƒ‰

```python
results = vectorstore.max_marginal_relevance_search(
    query="Transformer architecture",
    k=5,
    fetch_k=20,  # ë¨¼ì € 20ê°œ í›„ë³´ ê²€ìƒ‰
    lambda_mult=0.5  # ê´€ë ¨ì„± vs ë‹¤ì–‘ì„± ê· í˜•
)
```

### 8.3 ë©”íƒ€ë°ì´í„° í•„í„°ë§

```python
# íŠ¹ì • ë…„ë„ ë…¼ë¬¸ë§Œ ê²€ìƒ‰
results = vectorstore.similarity_search(
    query="attention mechanism",
    k=5,
    filter={"year": {"$gte": 2020}}  # 2020ë…„ ì´í›„ ë…¼ë¬¸ë§Œ
)
```

### 8.4 Reranking (ì¬ìˆœìœ„í™”)

**Cohere Rerank API ì‚¬ìš©:**

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

# Reranker ì„¤ì •
compressor = CohereRerank(model="rerank-english-v2.0")

retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)

# ì¬ìˆœìœ„í™”ëœ ê²°ê³¼
compressed_docs = retriever.get_relevant_documents(
    "Explain transformer architecture"
)
```

---

## 9. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±

### 9.1 RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¦¬ë·° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë…¼ë¬¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë…¼ë¬¸]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€ ê·œì¹™]
- ì°¸ê³  ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ë…¼ë¬¸ ì œëª©, ì €ì)
- ë…¼ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ë‚œì´ë„: {difficulty} ëª¨ë“œ

ë‹µë³€:
"""
```

### 9.2 ìš©ì–´ì§‘ í¬í•¨ í”„ë¡¬í”„íŠ¸

```python
RAG_WITH_GLOSSARY_PROMPT = """
ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¦¬ë·° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ìš©ì–´ ì •ì˜]
{glossary_context}

[ì°¸ê³  ë…¼ë¬¸]
{paper_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ë‹µë³€:
"""
```

---

## 10. RAG ì²´ì¸ êµ¬í˜„

### 10.1 Langchain RAG Chain

```python
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = PromptTemplate(
    template=RAG_PROMPT_TEMPLATE,
    input_variables=["context", "question", "difficulty"]
)

# RAG ì²´ì¸ êµ¬ì„±
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": prompt}
)

# ì‹¤í–‰
response = rag_chain.run(
    query="Transformer ë…¼ë¬¸ ì„¤ëª…í•´ì¤˜",
    difficulty="easy"
)
```

### 10.2 LangGraphë¥¼ í™œìš©í•œ ë³µì¡í•œ RAG

```python
from langgraph.graph import StateGraph

class RAGState(TypedDict):
    question: str
    difficulty: str
    glossary_context: str
    paper_context: str
    final_answer: str

def glossary_search_node(state: RAGState):
    """ìš©ì–´ì§‘ ê²€ìƒ‰"""
    glossary_docs = glossary_store.similarity_search(state["question"], k=2)
    state["glossary_context"] = "\n".join([doc.page_content for doc in glossary_docs])
    return state

def paper_search_node(state: RAGState):
    """ë…¼ë¬¸ ê²€ìƒ‰"""
    paper_docs = paper_chunks_store.similarity_search(state["question"], k=3)
    state["paper_context"] = "\n".join([doc.page_content for doc in paper_docs])
    return state

def generate_answer_node(state: RAGState):
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    prompt = RAG_WITH_GLOSSARY_PROMPT.format(
        glossary_context=state["glossary_context"],
        paper_context=state["paper_context"],
        question=state["question"]
    )
    state["final_answer"] = llm.invoke(prompt)
    return state

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(RAGState)
workflow.add_node("glossary_search", glossary_search_node)
workflow.add_node("paper_search", paper_search_node)
workflow.add_node("generate_answer", generate_answer_node)

workflow.set_entry_point("glossary_search")
workflow.add_edge("glossary_search", "paper_search")
workflow.add_edge("paper_search", "generate_answer")
workflow.add_edge("generate_answer", END)

rag_graph = workflow.compile()
```

---

## 11. ì°¸ê³  ìë£Œ

- Langchain RAG íŠœí† ë¦¬ì–¼: https://python.langchain.com/docs/tutorials/rag/
- Langchain Vector Stores: https://python.langchain.com/docs/integrations/vectorstores/
- ChromaDB ë¬¸ì„œ: https://docs.trychroma.com/
- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings
- Text Splitters: https://docs.langchain.com/oss/python/integrations/splitters
