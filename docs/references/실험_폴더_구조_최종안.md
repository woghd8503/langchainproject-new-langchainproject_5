# ì‹¤í—˜ í´ë” êµ¬ì¡° ìµœì¢…ì•ˆ (ì¢…í•© ê°€ì´ë“œ)

## ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2025-10-31
- **í”„ë¡œì íŠ¸ëª…**: ë…¼ë¬¸ ë¦¬ë·° ì±—ë´‡ (AI Agent + RAG)
- **íŒ€ëª…**: ì—°ê²°ì˜ ë¯¼ì¡±
- **ëª©ì **: ì‹¤í—˜ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡í•˜ê¸° ìœ„í•œ ì¢…í•© í´ë” êµ¬ì¡° ì •ì˜

---

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ í´ë” êµ¬ì¡°](#ì „ì²´-í´ë”-êµ¬ì¡°)
2. [ê° í´ë” ìƒì„¸ ì„¤ëª…](#ê°-í´ë”-ìƒì„¸-ì„¤ëª…)
   - [metadata.json (ë©”íƒ€ë°ì´í„°)](#1-metadatajson)
   - [tools/ (ë„êµ¬ ì‹¤í–‰ ë¡œê·¸)](#2-tools-í´ë”)
   - [database/ (DB ê´€ë ¨)](#3-database-í´ë”)
   - [prompts/ (í”„ë¡¬í”„íŠ¸)](#4-prompts-í´ë”)
   - [ui/ (Streamlit UI)](#5-ui-í´ë”)
   - [outputs/ (ê²°ê³¼ë¬¼)](#6-outputs-í´ë”)
   - [evaluation/ (í‰ê°€ ì§€í‘œ)](#7-evaluation-í´ë”)
   - [debug/ (ë””ë²„ê·¸)](#8-debug-í´ë”)
3. [ExperimentManager êµ¬í˜„](#experimentmanager-êµ¬í˜„)
4. [ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ](#ì‹¤ì œ-ì‚¬ìš©-ì˜ˆì‹œ)
5. [í´ë” ê²€ìƒ‰ ë° ë¶„ì„](#í´ë”-ê²€ìƒ‰-ë°-ë¶„ì„)

---

## ì „ì²´ í´ë” êµ¬ì¡°

```
experiments/
â””â”€â”€ 20251031/                                # ë‚ ì§œë³„ í´ë” (YYYYMMDD)
    â””â”€â”€ 20251031_103015_session_001/         # ì‹œê°„_session_ID
        â”‚
        â”œâ”€â”€ metadata.json                    # â­ ì „ì²´ ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
        â”œâ”€â”€ chatbot.log                      # ë©”ì¸ ì‹¤í–‰ ë¡œê·¸
        â”œâ”€â”€ config.yaml                      # ì „ì²´ ì„¤ì •
        â”‚
        â”œâ”€â”€ tools/                           # ğŸ”§ ë„êµ¬ ì‹¤í–‰ ë¡œê·¸
        â”‚   â”œâ”€â”€ rag_paper.log                # ë…¼ë¬¸ ê²€ìƒ‰ ë¡œê·¸
        â”‚   â”œâ”€â”€ rag_glossary.log             # ìš©ì–´ì§‘ ê²€ìƒ‰ ë¡œê·¸
        â”‚   â”œâ”€â”€ web_search.log               # ì›¹ ê²€ìƒ‰ ë¡œê·¸
        â”‚   â”œâ”€â”€ summary_paper.log            # ë…¼ë¬¸ ìš”ì•½ ë¡œê·¸
        â”‚   â”œâ”€â”€ file_save.log                # íŒŒì¼ ì €ì¥ ë¡œê·¸
        â”‚   â””â”€â”€ general.log                  # ì¼ë°˜ ë‹µë³€ ë¡œê·¸
        â”‚
        â”œâ”€â”€ database/                        # ğŸ—„ï¸ DB ê´€ë ¨ ê¸°ë¡
        â”‚   â”œâ”€â”€ queries.sql                  # ì‹¤í–‰ëœ SQL ì¿¼ë¦¬ ëª¨ìŒ
        â”‚   â”œâ”€â”€ pgvector_searches.json       # pgvector ê²€ìƒ‰ ê¸°ë¡
        â”‚   â”œâ”€â”€ search_results.json          # DB ê²€ìƒ‰ ê²°ê³¼
        â”‚   â””â”€â”€ db_performance.json          # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ë“±
        â”‚
        â”œâ”€â”€ prompts/                         # ğŸ’¬ í”„ë¡¬í”„íŠ¸ ê¸°ë¡
        â”‚   â”œâ”€â”€ system_prompt.txt            # ì‚¬ìš©ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        â”‚   â”œâ”€â”€ user_prompt.txt              # ì‚¬ìš©ì ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸
        â”‚   â”œâ”€â”€ final_prompt.txt             # LLMì— ì „ë‹¬ëœ ìµœì¢… í”„ë¡¬í”„íŠ¸
        â”‚   â””â”€â”€ prompt_template.yaml         # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ë³´
        â”‚
        â”œâ”€â”€ ui/                              # ğŸ–¥ï¸ UI ê´€ë ¨ ê¸°ë¡
        â”‚   â”œâ”€â”€ streamlit_session.json       # Streamlit ì„¸ì…˜ ìƒíƒœ
        â”‚   â”œâ”€â”€ user_interactions.log        # ì‚¬ìš©ì ì¸í„°ë™ì…˜ ë¡œê·¸
        â”‚   â””â”€â”€ ui_events.json               # UI ì´ë²¤íŠ¸ ê¸°ë¡
        â”‚
        â”œâ”€â”€ outputs/                         # ğŸ“„ ìƒì„±ëœ ê²°ê³¼ë¬¼
        â”‚   â”œâ”€â”€ response.txt                 # ìµœì¢… ë‹µë³€
        â”‚   â”œâ”€â”€ summary.md                   # ìš”ì•½ë³¸ (ìˆì„ ê²½ìš°)
        â”‚   â””â”€â”€ saved_file.txt               # ì‚¬ìš©ìê°€ ì €ì¥ ìš”ì²­í•œ íŒŒì¼
        â”‚
        â”œâ”€â”€ evaluation/                      # ğŸ“Š í‰ê°€ ì§€í‘œ
        â”‚   â”œâ”€â”€ rag_metrics.json             # RAG í‰ê°€ ì§€í‘œ
        â”‚   â”œâ”€â”€ agent_accuracy.json          # Agent ì •í™•ë„
        â”‚   â”œâ”€â”€ latency_report.json          # ì‘ë‹µ ì‹œê°„ ë¶„ì„
        â”‚   â”œâ”€â”€ cost_analysis.json           # ë¹„ìš© ë¶„ì„
        â”‚   â””â”€â”€ test_results.json            # í…ŒìŠ¤íŠ¸ ê²°ê³¼
        â”‚
        â””â”€â”€ debug/                           # ğŸ› ë””ë²„ê·¸ ì •ë³´ (ì„ íƒ)
            â”œâ”€â”€ agent_trace.json             # Agent ì‹¤í–‰ ì¶”ì 
            â”œâ”€â”€ llm_tokens.json              # í† í° ì‚¬ìš©ëŸ‰
            â””â”€â”€ error_trace.log              # ì—ëŸ¬ ë°œìƒ ì‹œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
```

---

## ê° í´ë” ìƒì„¸ ì„¤ëª…

### 1. metadata.json

**ëª©ì **: ì „ì²´ ì‹¤í—˜ì˜ í•µì‹¬ ì •ë³´ë¥¼ í•œ ê³³ì— ìš”ì•½

**ë‚´ìš© ì˜ˆì‹œ**:
```json
{
  "session_id": "001",
  "start_time": "2025-10-31T10:30:15",
  "end_time": "2025-10-31T10:32:45",
  "difficulty": "easy",
  "tool_used": "rag_paper",
  "user_query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
  "success": true,
  "response_time_ms": 2500,
  "response_length": 450,
  "model": "gpt-4",
  "temperature": 0.7,
  "tokens_used": {
    "prompt": 1200,
    "completion": 800,
    "total": 2000
  },
  "db_queries_count": 4,
  "db_total_time_ms": 120
}
```

---

### 2. tools/ í´ë”

**ëª©ì **: 6ê°€ì§€ AI Agent ë„êµ¬ì˜ ì‹¤í–‰ ë¡œê·¸ ì €ì¥

**íŠ¹ì§•**:
- ì‹¤í–‰ëœ ë„êµ¬ë§Œ ë¡œê·¸ íŒŒì¼ ìƒì„± (ë¹ˆ íŒŒì¼ ì—†ìŒ)
- ê° ë„êµ¬ë³„ ë…ë¦½ì ì¸ Logger ì‚¬ìš©

#### 2.1 rag_paper.log (ë…¼ë¬¸ ê²€ìƒ‰)

```
2025-10-31 10:30:25 | ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘
2025-10-31 10:30:25 | ê²€ìƒ‰ ì¿¼ë¦¬: "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜"
2025-10-31 10:30:25 | ì„ë² ë”© ìƒì„± ì™„ë£Œ (dimension: 1536)
2025-10-31 10:30:25 | pgvector similarity_search ì‹¤í–‰
2025-10-31 10:30:25 | Top-5 ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ (45ms)
2025-10-31 10:30:25 | ê´€ë ¨ ë…¼ë¬¸ 5ê°œ ë°œê²¬
2025-10-31 10:30:25 | - [1] Retrieval-Augmented Generation (Patrick Lewis et al., 2020)
2025-10-31 10:30:25 | - [2] Dense Passage Retrieval (Vladimir Karpukhin et al., 2020)
2025-10-31 10:30:25 | - [3] REALM (Kelvin Guu et al., 2020)
2025-10-31 10:30:25 | PostgreSQL papers í…Œì´ë¸” ì¡°íšŒ (12ms)
2025-10-31 10:30:25 | ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ (ì´ 1,234 tokens)
2025-10-31 10:30:25 | ë…¼ë¬¸ ê²€ìƒ‰ ì™„ë£Œ
```

#### 2.2 rag_glossary.log (ìš©ì–´ì§‘ ê²€ìƒ‰)

```
2025-10-31 10:30:26 | ìš©ì–´ì§‘ ê²€ìƒ‰ ì‹œì‘
2025-10-31 10:30:26 | ê²€ìƒ‰ ìš©ì–´: "Attention Mechanism"
2025-10-31 10:30:26 | glossary í…Œì´ë¸” ì¡°íšŒ
2025-10-31 10:30:26 | ìš©ì–´ ë°œê²¬: term_id=42
2025-10-31 10:30:26 | ë‚œì´ë„: easy
2025-10-31 10:30:26 | ì„¤ëª…: "ì±…ì„ ì½ì„ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì²˜ëŸ¼..."
2025-10-31 10:30:26 | ìš©ì–´ì§‘ ê²€ìƒ‰ ì™„ë£Œ (8ms)
```

#### 2.3 web_search.log (ì›¹ ê²€ìƒ‰)

```
2025-10-31 14:25:43 | ì›¹ ê²€ìƒ‰ ì‹œì‘
2025-10-31 14:25:43 | ê²€ìƒ‰ ì¿¼ë¦¬: "ìµœì‹  Transformer ë…¼ë¬¸"
2025-10-31 14:25:43 | Tavily Search API í˜¸ì¶œ
2025-10-31 14:25:44 | ê²€ìƒ‰ ê²°ê³¼ 10ê°œ ìˆ˜ì‹ 
2025-10-31 14:25:44 | - [1] https://arxiv.org/abs/2023...
2025-10-31 14:25:44 | - [2] https://arxiv.org/abs/2024...
2025-10-31 14:25:44 | ì›¹ ê²€ìƒ‰ ì™„ë£Œ (1200ms)
```

#### 2.4 summary_paper.log (ë…¼ë¬¸ ìš”ì•½)

```
2025-10-31 16:48:20 | ë…¼ë¬¸ ìš”ì•½ ì‹œì‘
2025-10-31 16:48:20 | ë…¼ë¬¸ ID: 123
2025-10-31 16:48:20 | ë…¼ë¬¸ ì œëª©: "Attention Is All You Need"
2025-10-31 16:48:20 | ë…¼ë¬¸ ì „ë¬¸ ë¡œë“œ (10,234 tokens)
2025-10-31 16:48:20 | load_summarize_chain ì‹¤í–‰
2025-10-31 16:48:22 | ì´ˆë¡ ìƒì„± ì™„ë£Œ (350 tokens)
2025-10-31 16:48:22 | ë…¼ë¬¸ ìš”ì•½ ì™„ë£Œ (2000ms)
```

#### 2.5 file_save.log (íŒŒì¼ ì €ì¥)

```
2025-10-31 17:10:15 | íŒŒì¼ ì €ì¥ ì‹œì‘
2025-10-31 17:10:15 | íŒŒì¼ëª…: paper_summary_20251031_171015.txt
2025-10-31 17:10:15 | ë‚´ìš© ê¸¸ì´: 1,234 bytes
2025-10-31 17:10:15 | outputs/ í´ë”ì— ì €ì¥
2025-10-31 17:10:15 | íŒŒì¼ ì €ì¥ ì™„ë£Œ
```

#### 2.6 general.log (ì¼ë°˜ ë‹µë³€)

```
2025-10-31 18:20:30 | ì¼ë°˜ ë‹µë³€ ìƒì„± ì‹œì‘
2025-10-31 18:20:30 | ì§ˆë¬¸: "ê³ ë§ˆì›Œ"
2025-10-31 18:20:30 | ì§ˆë¬¸ ìœ í˜•: greeting
2025-10-31 18:20:30 | LLM í˜¸ì¶œ (ê°„ë‹¨ ë‹µë³€ ëª¨ë“œ)
2025-10-31 18:20:31 | ë‹µë³€ ìƒì„± ì™„ë£Œ
2025-10-31 18:20:31 | ì¼ë°˜ ë‹µë³€ ì™„ë£Œ (500ms)
```

---

### 3. database/ í´ë”

**ëª©ì **: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë° ê²€ìƒ‰ ê²°ê³¼ ê¸°ë¡

#### 3.1 queries.sql

**ëª¨ë“  SQL ì¿¼ë¦¬ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ê¸°ë¡**

```sql
-- ì‹¤í–‰ ì‹œê°„: 2025-10-31 10:30:25
-- ë„êµ¬: rag_paper
-- ì„¤ëª…: pgvector ìœ ì‚¬ë„ ê²€ìƒ‰

SELECT paper_id, chunk_text, embedding <=> '[0.123, 0.456, ...]' AS distance
FROM paper_chunks
ORDER BY distance
LIMIT 5;

-- ì‹¤í–‰ ì‹œê°„: 2025-10-31 10:30:25
-- ë„êµ¬: rag_paper
-- ì„¤ëª…: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ

SELECT paper_id, title, authors, publish_date, source, url, category, abstract
FROM papers
WHERE paper_id IN (123, 456, 789, 234, 567);

-- ì‹¤í–‰ ì‹œê°„: 2025-10-31 10:30:26
-- ë„êµ¬: rag_glossary
-- ì„¤ëª…: ìš©ì–´ì§‘ ì¡°íšŒ

SELECT term_id, term, definition, easy_explanation, hard_explanation
FROM glossary
WHERE term = 'Attention Mechanism';

-- ì‹¤í–‰ ì‹œê°„: 2025-10-31 10:30:27
-- ë„êµ¬: system
-- ì„¤ëª…: query_logs í…Œì´ë¸”ì— ì‹¤í–‰ ë¡œê·¸ ì €ì¥

INSERT INTO query_logs
(user_query, difficulty_mode, tool_used, response, response_time_ms, success)
VALUES
('RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜', 'easy', 'rag_paper', 'ë‹µë³€ ë‚´ìš©...', 2500, TRUE);
```

#### 3.2 pgvector_searches.json

**pgvector ë²¡í„° ê²€ìƒ‰ ìƒì„¸ ê¸°ë¡**

```json
[
  {
    "timestamp": "2025-10-31T10:30:25",
    "tool": "rag_paper",
    "collection": "paper_chunks",
    "query_text": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
    "query_embedding_preview": "[0.123, 0.456, 0.789, ...]",
    "embedding_dimension": 1536,
    "embedding_model": "text-embedding-3-small",
    "search_type": "similarity",
    "similarity_metric": "cosine",
    "top_k": 5,
    "execution_time_ms": 45,
    "results_preview": [
      {
        "chunk_id": 1234,
        "paper_id": 123,
        "distance": 0.15,
        "chunk_preview": "RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬..."
      },
      {
        "chunk_id": 5678,
        "paper_id": 456,
        "distance": 0.23,
        "chunk_preview": "Dense Passage Retrievalì€..."
      }
    ]
  },
  {
    "timestamp": "2025-10-31T10:30:26",
    "tool": "rag_glossary",
    "collection": "glossary_embeddings",
    "query_text": "Attention Mechanism",
    "embedding_dimension": 1536,
    "search_type": "mmr",
    "similarity_metric": "cosine",
    "top_k": 3,
    "lambda_mult": 0.5,
    "execution_time_ms": 32
  }
]
```

#### 3.3 search_results.json

**DB ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´**

```json
{
  "rag_paper": {
    "timestamp": "2025-10-31T10:30:25",
    "query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
    "difficulty": "easy",
    "results_count": 5,
    "papers": [
      {
        "paper_id": 123,
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "authors": "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, ...",
        "publish_date": "2020-05-22",
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2005.11401",
        "category": "cs.CL",
        "relevance_score": 0.92,
        "chunk_text": "RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ ìƒì„± ëª¨ë¸ì— í†µí•©í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ parametric memoryì™€ non-parametric memoryë¥¼ ê²°í•©í•©ë‹ˆë‹¤...",
        "chunk_position": "abstract"
      },
      {
        "paper_id": 456,
        "title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "authors": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, ...",
        "publish_date": "2020-04-10",
        "source": "arXiv",
        "url": "https://arxiv.org/abs/2004.04906",
        "category": "cs.CL",
        "relevance_score": 0.87,
        "chunk_text": "DPRì€ ë°€ì§‘ ë²¡í„° í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤..."
      },
      {
        "paper_id": 789,
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "relevance_score": 0.83
      },
      {
        "paper_id": 234,
        "title": "Leveraging Passage Retrieval with Generative Models",
        "relevance_score": 0.78
      },
      {
        "paper_id": 567,
        "title": "FiD: Fusion-in-Decoder for Open-Domain Question Answering",
        "relevance_score": 0.75
      }
    ],
    "total_chunks_scanned": 10000,
    "search_time_ms": 45
  },
  "rag_glossary": {
    "timestamp": "2025-10-31T10:30:26",
    "query": "Attention Mechanism",
    "difficulty": "easy",
    "results_count": 1,
    "terms": [
      {
        "term_id": 42,
        "term": "Attention Mechanism",
        "difficulty_mode": "easy",
        "explanation": "ì±…ì„ ì½ì„ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì²˜ëŸ¼, AIê°€ ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'ë‚˜ëŠ” í•™êµì— ê°„ë‹¤'ë¼ëŠ” ë¬¸ì¥ì—ì„œ 'í•™êµ'ê°€ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ ê·¸ ë‹¨ì–´ì— ë” ë§ì€ ì£¼ì˜ë¥¼ ê¸°ìš¸ì…ë‹ˆë‹¤.",
        "category": "Deep Learning",
        "difficulty_level": "intermediate",
        "related_terms": ["Transformer", "Self-Attention", "Multi-Head Attention"]
      }
    ],
    "search_time_ms": 8
  }
}
```

#### 3.4 db_performance.json

**ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„**

```json
{
  "summary": {
    "total_queries": 4,
    "total_execution_time_ms": 120,
    "avg_query_time_ms": 30,
    "slowest_query_ms": 45,
    "fastest_query_ms": 5
  },
  "queries": [
    {
      "query_id": 1,
      "query_type": "pgvector_similarity",
      "collection": "paper_chunks",
      "execution_time_ms": 45,
      "rows_scanned": 10000,
      "rows_returned": 5,
      "index_used": "paper_chunks_embedding_idx"
    },
    {
      "query_id": 2,
      "query_type": "select",
      "table": "papers",
      "execution_time_ms": 12,
      "rows_scanned": 5,
      "rows_returned": 5,
      "index_used": "papers_pkey"
    },
    {
      "query_id": 3,
      "query_type": "select",
      "table": "glossary",
      "execution_time_ms": 8,
      "rows_scanned": 1,
      "rows_returned": 1,
      "index_used": "idx_glossary_term"
    },
    {
      "query_id": 4,
      "query_type": "insert",
      "table": "query_logs",
      "execution_time_ms": 5,
      "rows_affected": 1,
      "success": true
    }
  ],
  "connection_info": {
    "host": "localhost",
    "port": 5432,
    "database": "papers",
    "pool_size": 10,
    "active_connections": 1
  },
  "tables_accessed": ["paper_chunks", "papers", "glossary", "query_logs"],
  "indexes_used": ["paper_chunks_embedding_idx", "papers_pkey", "idx_glossary_term"]
}
```

---

### 4. prompts/ í´ë”

**ëª©ì **: LLMì— ì „ë‹¬ëœ í”„ë¡¬í”„íŠ¸ ê¸°ë¡ (ì¬í˜„ì„± í™•ë³´)

#### 4.1 system_prompt.txt

```
ë‹¹ì‹ ì€ ë…¼ë¬¸ì„ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ ìš©ì–´ ì‚¬ìš©
- ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ë§ì´ í™œìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
- í•œê¸€ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€

[ë‹µë³€ ê·œì¹™]
1. ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…
2. "ì¦‰", "ì˜ˆë¥¼ ë“¤ì–´" ë“±ì˜ ì—°ê²°ì–´ ì‚¬ìš©
3. í•œ ë¬¸ì¥ì„ ì§§ê²Œ (20ì ì´ë‚´)
4. ë¶ˆí•„ìš”í•œ ì „ë¬¸ ìš©ì–´ ë°°ì œ

===== ë©”íƒ€ë°ì´í„° =====
ë‚œì´ë„: easy
ìƒì„± ì‹œê°„: 2025-10-31T10:30:25
í…œí”Œë¦¿: EASY_SYSTEM_PROMPT
ëª¨ë¸: gpt-4
Temperature: 0.7
```

#### 4.2 user_prompt.txt

```
[ì°¸ê³  ë…¼ë¬¸]

1. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
   ì €ì: Patrick Lewis, Ethan Perez, et al.
   ì—°ë„: 2020
   ì¶œì²˜: arXiv:2005.11401

   ë‚´ìš©: RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ ìƒì„± ëª¨ë¸ì— í†µí•©í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.
   ì‚¬ì „ í•™ìŠµëœ parametric memory (ëª¨ë¸ íŒŒë¼ë¯¸í„°)ì™€ non-parametric memory
   (ì™¸ë¶€ ë¬¸ì„œ ì¸ë±ìŠ¤)ë¥¼ ê²°í•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì§€ì‹ ì§‘ì•½ì ì¸ NLP íƒœìŠ¤í¬ì—ì„œ
   ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤...

2. Dense Passage Retrieval for Open-Domain Question Answering
   ì €ì: Vladimir Karpukhin, Barlas Oguz, et al.
   ì—°ë„: 2020
   ì¶œì²˜: arXiv:2004.04906

   ë‚´ìš©: DPRì€ ë°€ì§‘ ë²¡í„° í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   ì§ˆë¬¸ê³¼ ë¬¸ì„œë¥¼ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³ , ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...

3. REALM: Retrieval-Augmented Language Model Pre-Training
   ...

[ì§ˆë¬¸]
RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜

ìœ„ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

===== ë©”íƒ€ë°ì´í„° =====
ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: 5ê°œ ë…¼ë¬¸
ì»¨í…ìŠ¤íŠ¸ ì´ ê¸¸ì´: 1,234 tokens
ìƒì„± ì‹œê°„: 2025-10-31T10:30:25
ê²€ìƒ‰ ë„êµ¬: rag_paper
ê²€ìƒ‰ ì‹œê°„: 45ms
```

#### 4.3 final_prompt.txt

```
===== SYSTEM =====
ë‹¹ì‹ ì€ ë…¼ë¬¸ì„ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ ìš©ì–´ ì‚¬ìš©
- ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ë§ì´ í™œìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
- í•œê¸€ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€

[ë‹µë³€ ê·œì¹™]
1. ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…
2. "ì¦‰", "ì˜ˆë¥¼ ë“¤ì–´" ë“±ì˜ ì—°ê²°ì–´ ì‚¬ìš©
3. í•œ ë¬¸ì¥ì„ ì§§ê²Œ (20ì ì´ë‚´)
4. ë¶ˆí•„ìš”í•œ ì „ë¬¸ ìš©ì–´ ë°°ì œ

===== USER =====
[ì°¸ê³  ë…¼ë¬¸]

1. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
   ì €ì: Patrick Lewis, Ethan Perez, et al.
   ì—°ë„: 2020
   ...

[ì§ˆë¬¸]
RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜

ìœ„ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

===== ë©”íƒ€ë°ì´í„° =====
ì´ í† í° ìˆ˜: 2,345 tokens (system: 234, user: 2,111)
ëª¨ë¸: gpt-4
Temperature: 0.7
Max Tokens: 2000
Streaming: true
ìƒì„± ì‹œê°„: 2025-10-31T10:30:25
```

#### 4.4 prompt_template.yaml

```yaml
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ë³´

difficulty: easy
template_name: EASY_SYSTEM_PROMPT
template_version: 1.0

variables:
  context: |
    [ì°¸ê³  ë…¼ë¬¸]
    1. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
    ...
  question: "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜"

llm_config:
  provider: openai
  model: gpt-4
  temperature: 0.7
  max_tokens: 2000
  streaming: true
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0

token_count:
  system_prompt: 234
  user_prompt: 2111
  total_input: 2345
  max_output: 2000

prompt_chain:
  - step: 1
    type: system_prompt
    template: EASY_SYSTEM_PROMPT
  - step: 2
    type: user_prompt
    template: RAG_USER_TEMPLATE
  - step: 3
    type: context_injection
    source: rag_paper
    context_length: 1234

metadata:
  created_at: "2025-10-31T10:30:25"
  difficulty: easy
  tool_used: rag_paper
  search_results_count: 5
```

---

### 5. ui/ í´ë”

**ëª©ì **: Streamlit UI ì¸í„°ë™ì…˜ ë° ì„¸ì…˜ ì •ë³´ ê¸°ë¡

#### 5.1 streamlit_session.json

```json
{
  "session_id": "abc123def456",
  "start_time": "2025-10-31T10:30:15",
  "end_time": "2025-10-31T10:32:45",
  "duration_seconds": 150,
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
  "ip_address": "127.0.0.1",
  "browser": "Chrome",
  "os": "Windows 10",
  "screen_resolution": "1920x1080",
  "session_state": {
    "messages": [
      {
        "role": "user",
        "content": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
        "timestamp": "2025-10-31T10:30:20",
        "length": 12
      },
      {
        "role": "assistant",
        "content": "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´, AIê°€ ë‹µë³€í•  ë•Œ ì¸í„°ë„·ì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë¨¼ì € ì°¾ì•„ë³´ê³  ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤...",
        "timestamp": "2025-10-31T10:30:25",
        "length": 450,
        "tool_used": "rag_paper"
      }
    ],
    "difficulty": "easy",
    "conversation_turns": 1,
    "widgets": {
      "difficulty_selector": "easy",
      "chat_input": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜"
    }
  },
  "page_views": [
    {
      "timestamp": "2025-10-31T10:30:15",
      "page": "main"
    }
  ]
}
```

#### 5.2 user_interactions.log

```
2025-10-31 10:30:15 | í˜ì´ì§€ ì ‘ì†
2025-10-31 10:30:16 | ì‚¬ì´ë“œë°” ì—´ê¸°
2025-10-31 10:30:17 | ë‚œì´ë„ ì„ íƒ ë²„íŠ¼ í´ë¦­: easy
2025-10-31 10:30:18 | ì•ˆë‚´ ë¬¸êµ¬ í™•ì¸ (info box í‘œì‹œ)
2025-10-31 10:30:20 | ì§ˆë¬¸ ì…ë ¥ í•„ë“œ í¬ì»¤ìŠ¤
2025-10-31 10:30:20 | ì§ˆë¬¸ ì…ë ¥ ì‹œì‘
2025-10-31 10:30:20 | ì§ˆë¬¸ ì…ë ¥ ì™„ë£Œ: "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜" (12ì)
2025-10-31 10:30:20 | ì§ˆë¬¸ ì œì¶œ ë²„íŠ¼ í´ë¦­ (Enter í‚¤)
2025-10-31 10:30:20 | ë¡œë”© ìŠ¤í”¼ë„ˆ í‘œì‹œ
2025-10-31 10:30:20 | StreamlitCallbackHandler í™œì„±í™”
2025-10-31 10:30:21 | Agent ì‹¤í–‰ ì¤‘ í‘œì‹œ: "ë„êµ¬ ì„ íƒ ì¤‘..."
2025-10-31 10:30:22 | Agent ì‹¤í–‰ ì¤‘ í‘œì‹œ: "ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."
2025-10-31 10:30:23 | Agent ì‹¤í–‰ ì¤‘ í‘œì‹œ: "ë‹µë³€ ìƒì„± ì¤‘..."
2025-10-31 10:30:25 | ë¡œë”© ìŠ¤í”¼ë„ˆ ì œê±°
2025-10-31 10:30:25 | ë‹µë³€ ë Œë”ë§ ì‹œì‘
2025-10-31 10:30:25 | ë‹µë³€ ë Œë”ë§ ì™„ë£Œ (450ì)
2025-10-31 10:30:25 | ì¶œì²˜ expander í‘œì‹œ
2025-10-31 10:30:27 | ìŠ¤í¬ë¡¤ ë‹¤ìš´ (100px)
2025-10-31 10:30:28 | ì¶œì²˜ expander í´ë¦­ (í™•ì¥)
2025-10-31 10:30:30 | ì¶œì²˜ expander í´ë¦­ (ì¶•ì†Œ)
2025-10-31 10:30:32 | í˜ì´ì§€ ì¢…ë£Œ
```

#### 5.3 ui_events.json

```json
[
  {
    "timestamp": "2025-10-31T10:30:15",
    "event_type": "page_load",
    "page": "main",
    "load_time_ms": 234
  },
  {
    "timestamp": "2025-10-31T10:30:16",
    "event_type": "sidebar_toggled",
    "state": "open"
  },
  {
    "timestamp": "2025-10-31T10:30:17",
    "event_type": "difficulty_changed",
    "old_value": null,
    "new_value": "easy"
  },
  {
    "timestamp": "2025-10-31T10:30:20",
    "event_type": "input_focused",
    "widget": "chat_input"
  },
  {
    "timestamp": "2025-10-31T10:30:20",
    "event_type": "message_submitted",
    "message_length": 12,
    "message_preview": "RAGì— ëŒ€í•´ ì•Œë ¤...",
    "submission_method": "enter_key"
  },
  {
    "timestamp": "2025-10-31T10:30:20",
    "event_type": "agent_started",
    "difficulty": "easy"
  },
  {
    "timestamp": "2025-10-31T10:30:25",
    "event_type": "response_received",
    "response_length": 450,
    "response_time_ms": 5000,
    "tool_used": "rag_paper"
  },
  {
    "timestamp": "2025-10-31T10:30:25",
    "event_type": "response_rendered",
    "streaming": true,
    "render_time_ms": 120
  },
  {
    "timestamp": "2025-10-31T10:30:28",
    "event_type": "expander_toggled",
    "expander_label": "ì°¸ê³  ë…¼ë¬¸",
    "state": "expanded"
  },
  {
    "timestamp": "2025-10-31T10:30:30",
    "event_type": "expander_toggled",
    "expander_label": "ì°¸ê³  ë…¼ë¬¸",
    "state": "collapsed"
  },
  {
    "timestamp": "2025-10-31T10:30:32",
    "event_type": "session_ended",
    "total_duration_seconds": 17,
    "total_messages": 2
  }
]
```

---

### 6. outputs/ í´ë”

**ëª©ì **: ì‚¬ìš©ìì—ê²Œ ì œê³µë˜ëŠ” ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥

#### 6.1 response.txt

```
RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´, AIê°€ ë‹µë³€í•  ë•Œ ì¸í„°ë„·ì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë¨¼ì € ì°¾ì•„ë³´ê³  ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì„ ìƒë‹˜ê»˜ ì§ˆë¬¸ì„ í•˜ë©´ ì„ ìƒë‹˜ì´ ì±…ì„ í´ì„œ í™•ì¸í•œ í›„ ë‹µë³€í•´ì£¼ì‹œëŠ” ê²ƒê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤. AIë„ ë§ˆì°¬ê°€ì§€ë¡œ ìì‹ ì˜ ê¸°ì–µ(í•™ìŠµëœ ì§€ì‹)ë§Œìœ¼ë¡œ ë‹µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì™¸ë¶€ ìë£Œë¥¼ ì°¾ì•„ì„œ ë” ì •í™•í•˜ê³  ìµœì‹ ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

RAGì˜ ì¥ì ì€:
1. ë” ì •í™•í•œ ë‹µë³€: ì‹¤ì œ ìë£Œë¥¼ ì°¸ê³ í•˜ë¯€ë¡œ í‹€ë¦° ì •ë³´ë¥¼ ì¤„ í™•ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤
2. ìµœì‹  ì •ë³´: AIê°€ í•™ìŠµí•˜ì§€ ì•Šì€ ìµœì‹  ì •ë³´ë„ ê²€ìƒ‰í•´ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. ì¶œì²˜ ì œê³µ: ì–´ë–¤ ìë£Œë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ì´ ê¸°ìˆ ì€ ë…¼ë¬¸ ê²€ìƒ‰, ê³ ê° ì§€ì›, ì˜ë£Œ ì •ë³´ ì œê³µ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

===== ì°¸ê³  ë…¼ë¬¸ =====
1. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Patrick Lewis et al., 2020)
2. Dense Passage Retrieval for Open-Domain Question Answering (Vladimir Karpukhin et al., 2020)
3. REALM: Retrieval-Augmented Language Model Pre-Training (Kelvin Guu et al., 2020)
```

#### 6.2 summary.md (ë…¼ë¬¸ ìš”ì•½ ì‹œ)

```markdown
# ë…¼ë¬¸ ìš”ì•½: Attention Is All You Need

## ê¸°ë³¸ ì •ë³´
- **ì œëª©**: Attention Is All You Need
- **ì €ì**: Ashish Vaswani, Noam Shazeer, et al.
- **ë°œí‘œ**: 2017ë…„ NIPS
- **ì¶œì²˜**: arXiv:1706.03762

## í•µì‹¬ ë‚´ìš©

### 1. ì—°êµ¬ ë°°ê²½
ê¸°ì¡´ sequence-to-sequence ëª¨ë¸ì€ RNNì´ë‚˜ CNNì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ° ëª¨ë¸ë“¤ì€ ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ë•Œ ëŠë¦¬ê³  ë³‘ë ¬ ì²˜ë¦¬ê°€ ì–´ë µë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

### 2. ì œì•ˆ ë°©ë²•
ì´ ë…¼ë¬¸ì€ **Transformer**ë¼ëŠ” ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. RNNì´ë‚˜ CNN ì—†ì´ ì˜¤ì§ Attention ë©”ì»¤ë‹ˆì¦˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 3. ì£¼ìš” íŠ¹ì§•
- Self-Attention: ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…
- Multi-Head Attention: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— Attention ê³„ì‚°
- Positional Encoding: ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ ì œê³µ

### 4. ì‹¤í—˜ ê²°ê³¼
- ë²ˆì—­ í’ˆì§ˆ: ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ ë›°ì–´ë„˜ìŒ
- í•™ìŠµ ì†ë„: ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„

## ì˜ì˜
TransformerëŠ” ì´í›„ BERT, GPT ë“± í˜„ëŒ€ AI ëª¨ë¸ì˜ ê¸°ë°˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.
```

#### 6.3 saved_file.txt (ì‚¬ìš©ì ì €ì¥ ìš”ì²­)

```
[ì‚¬ìš©ìê°€ "íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜" ìš”ì²­ ì‹œ ìƒì„±ëœ íŒŒì¼]

RAGì— ëŒ€í•œ ì„¤ëª…

RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation)ì˜ ì•½ìì…ë‹ˆë‹¤...

[ì „ì²´ ë‹µë³€ ë‚´ìš©]

ìƒì„± ì¼ì‹œ: 2025-10-31 10:30:25
ë‚œì´ë„: Easy
```

---

### 7. evaluation/ í´ë”

**ëª©ì **: RAG, Agent, DB ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ê¸°ë¡ (PRD 09_í‰ê°€_ê¸°ì¤€ ì°¸ê³ )

**íŠ¹ì§•**:
- ì„±ëŠ¥ í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì •ëŸ‰ì  ì§€í‘œ ìˆ˜ì§‘
- ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„ ê°€ëŠ¥
- ëª©í‘œ ì§€í‘œ ë‹¬ì„± ì—¬ë¶€ ì¶”ì 

#### 7.1 rag_metrics.json

**RAG ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì§€í‘œ**

```json
{
  "timestamp": "2025-10-31T10:30:25",
  "session_id": "001",
  "user_query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
  "difficulty": "easy",
  "tool_used": "rag_paper",

  "retrieval_metrics": {
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 0.8,
    "precision_at_1": 1.0,
    "precision_at_3": 0.67,
    "precision_at_5": 0.4,
    "mrr": 1.0,
    "map": 0.85,
    "ndcg_at_5": 0.92
  },

  "generation_metrics": {
    "faithfulness": 0.95,
    "answer_relevancy": 0.88,
    "context_precision": 0.90,
    "context_recall": 0.85,
    "context_relevancy": 0.92
  },

  "answer_quality": {
    "em_score": 0.0,
    "f1_score": 0.78,
    "bleu_score": 0.45,
    "rouge_l": 0.72
  },

  "retrieved_documents": {
    "total_retrieved": 5,
    "relevant_retrieved": 4,
    "total_relevant": 5,
    "irrelevant_retrieved": 1
  },

  "target_comparison": {
    "recall_at_5_target": 0.6,
    "recall_at_5_actual": 0.8,
    "recall_at_5_status": "PASS",
    "faithfulness_target": 0.9,
    "faithfulness_actual": 0.95,
    "faithfulness_status": "PASS"
  },

  "notes": "RAG ê²€ìƒ‰ í’ˆì§ˆì´ ëª©í‘œì¹˜ë¥¼ ìƒíšŒí•¨. Precisionì€ ë‚®ì§€ë§Œ Recallì´ ë†’ì•„ ì¶©ë¶„í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"
}
```

#### 7.2 agent_accuracy.json

**AI Agent ë„êµ¬ ì„ íƒ ì •í™•ë„**

```json
{
  "timestamp": "2025-10-31T10:30:25",
  "session_id": "001",
  "user_query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
  "difficulty": "easy",

  "routing_decision": {
    "predicted_tool": "rag_paper",
    "expected_tool": "rag_paper",
    "correct": true,
    "confidence": 0.98,
    "routing_time_ms": 200
  },

  "tool_selection_breakdown": {
    "rag_paper": 0.98,
    "rag_glossary": 0.01,
    "web_search": 0.005,
    "summary_paper": 0.003,
    "file_save": 0.001,
    "general": 0.001
  },

  "agent_performance": {
    "total_steps": 3,
    "successful_steps": 3,
    "failed_steps": 0,
    "retry_count": 0,
    "fallback_used": false
  },

  "execution_flow": [
    {
      "step": 1,
      "node": "router_node",
      "decision": "rag_paper",
      "execution_time_ms": 200,
      "success": true
    },
    {
      "step": 2,
      "node": "rag_paper_node",
      "execution_time_ms": 2800,
      "success": true
    },
    {
      "step": 3,
      "node": "generate_answer_node",
      "execution_time_ms": 2000,
      "success": true
    }
  ],

  "target_comparison": {
    "routing_accuracy_target": 0.9,
    "routing_accuracy_actual": 1.0,
    "routing_accuracy_status": "PASS"
  },

  "notes": "Agentê°€ ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ë†’ì€ ì‹ ë¢°ë„ë¡œ ì„ íƒí•¨"
}
```

#### 7.3 latency_report.json

**ì‘ë‹µ ì‹œê°„ ë¶„ì„ (p50, p95, p99)**

```json
{
  "timestamp": "2025-10-31T10:30:25",
  "session_id": "001",
  "difficulty": "easy",
  "tool_used": "rag_paper",

  "total_latency": {
    "total_time_ms": 5000,
    "target_p95_ms": 6000,
    "status": "PASS"
  },

  "breakdown": {
    "routing_time_ms": 200,
    "routing_percentage": 4.0,

    "retrieval_time_ms": 2800,
    "retrieval_percentage": 56.0,
    "retrieval_breakdown": {
      "embedding_generation_ms": 100,
      "pgvector_search_ms": 45,
      "db_metadata_query_ms": 12,
      "context_preparation_ms": 2643
    },

    "generation_time_ms": 2000,
    "generation_percentage": 40.0,
    "generation_breakdown": {
      "llm_call_ms": 1850,
      "prompt_formatting_ms": 80,
      "response_formatting_ms": 70
    }
  },

  "database_latency": {
    "pgvector_search_ms": 45,
    "papers_table_query_ms": 12,
    "glossary_table_query_ms": 0,
    "query_logs_insert_ms": 5,
    "total_db_time_ms": 62
  },

  "llm_latency": {
    "model": "gpt-4",
    "prompt_tokens": 1468,
    "completion_tokens": 877,
    "total_tokens": 2345,
    "time_ms": 1850,
    "tokens_per_second": 1267
  },

  "percentiles": {
    "p50": 5000,
    "p95": 5000,
    "p99": 5000,
    "note": "ë‹¨ì¼ ì¿¼ë¦¬ì´ë¯€ë¡œ ëª¨ë“  percentile ë™ì¼"
  },

  "bottleneck_analysis": {
    "slowest_component": "retrieval",
    "slowest_time_ms": 2800,
    "optimization_suggestions": [
      "context_preparation ì‹œê°„ ìµœì í™” í•„ìš”",
      "pgvector ì¸ë±ìŠ¤ íŠœë‹ ê³ ë ¤"
    ]
  },

  "target_comparison": {
    "total_target_p95_ms": 6000,
    "total_actual_ms": 5000,
    "total_status": "PASS",
    "db_target_p95_ms": 100,
    "db_actual_ms": 62,
    "db_status": "PASS",
    "llm_target_p95_ms": 3000,
    "llm_actual_ms": 1850,
    "llm_status": "PASS"
  }
}
```

#### 7.4 cost_analysis.json

**í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ë¶„ì„**

```json
{
  "timestamp": "2025-10-31T10:30:25",
  "session_id": "001",
  "user_query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
  "difficulty": "easy",
  "tool_used": "rag_paper",

  "llm_usage": {
    "model": "gpt-4",
    "prompt_tokens": 1468,
    "completion_tokens": 877,
    "total_tokens": 2345,
    "streaming": true
  },

  "embedding_usage": {
    "model": "text-embedding-3-small",
    "total_embeddings": 1,
    "total_tokens": 12,
    "dimensions": 1536
  },

  "cost_breakdown_usd": {
    "gpt4_input_cost": 0.01468,
    "gpt4_output_cost": 0.00877,
    "gpt4_total_cost": 0.02345,

    "embedding_cost": 0.0000024,

    "total_cost": 0.0234524,

    "pricing": {
      "gpt4_input_per_1k": 0.01,
      "gpt4_output_per_1k": 0.01,
      "embedding_per_1k": 0.0002
    }
  },

  "cost_breakdown_krw": {
    "gpt4_input_cost": 19.11,
    "gpt4_output_cost": 11.40,
    "gpt4_total_cost": 30.51,

    "embedding_cost": 0.003,

    "total_cost": 30.51,

    "exchange_rate": 1300
  },

  "session_statistics": {
    "total_queries": 1,
    "total_cost_usd": 0.0234524,
    "avg_cost_per_query_usd": 0.0234524,
    "total_cost_krw": 30.51,
    "avg_cost_per_query_krw": 30.51
  },

  "budget_tracking": {
    "daily_budget_krw": 10000,
    "used_so_far_krw": 30.51,
    "remaining_krw": 9969.49,
    "budget_status": "SAFE"
  },

  "optimization_suggestions": [
    "í˜„ì¬ GPT-4 ì‚¬ìš© ì¤‘. ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” GPT-3.5-turbo ì‚¬ìš© ê¶Œì¥ (ë¹„ìš© 1/10)",
    "í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìµœì í™”ë¡œ input token ì ˆê° ê°€ëŠ¥"
  ]
}
```

#### 7.5 test_results.json

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼**

```json
{
  "timestamp": "2025-10-31T10:30:25",
  "test_suite": "rag_basic_test",
  "total_tests": 10,
  "passed": 9,
  "failed": 1,
  "success_rate": 0.9,

  "test_cases": [
    {
      "test_id": 1,
      "test_name": "rag_paper_retrieval_basic",
      "query": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
      "expected_tool": "rag_paper",
      "actual_tool": "rag_paper",
      "expected_papers_count": 5,
      "actual_papers_count": 5,
      "recall_at_5": 0.8,
      "faithfulness": 0.95,
      "response_time_ms": 5000,
      "status": "PASS",
      "notes": "ëª¨ë“  ì§€í‘œê°€ ëª©í‘œì¹˜ ë‹¬ì„±"
    },
    {
      "test_id": 2,
      "test_name": "glossary_easy_mode",
      "query": "Attention Mechanismì´ ë­ì•¼?",
      "expected_tool": "rag_glossary",
      "actual_tool": "rag_glossary",
      "expected_difficulty": "easy",
      "actual_difficulty": "easy",
      "term_found": true,
      "response_time_ms": 800,
      "status": "PASS",
      "notes": "ìš©ì–´ì§‘ ê²€ìƒ‰ ì •ìƒ ë™ì‘"
    },
    {
      "test_id": 3,
      "test_name": "web_search_latest_info",
      "query": "2024ë…„ ìµœì‹  Transformer ë…¼ë¬¸",
      "expected_tool": "web_search",
      "actual_tool": "rag_paper",
      "search_results_count": 0,
      "response_time_ms": 4500,
      "status": "FAIL",
      "notes": "ì›¹ ê²€ìƒ‰ ëŒ€ì‹  RAG ê²€ìƒ‰ ì„ íƒë¨. ë¼ìš°íŒ… ë¡œì§ ê°œì„  í•„ìš”"
    },
    {
      "test_id": 4,
      "test_name": "paper_summary_request",
      "query": "Attention Is All You Need ë…¼ë¬¸ ìš”ì•½í•´ì¤˜",
      "expected_tool": "summary_paper",
      "actual_tool": "summary_paper",
      "summary_generated": true,
      "summary_length_tokens": 350,
      "response_time_ms": 6500,
      "status": "PASS",
      "notes": "ë…¼ë¬¸ ìš”ì•½ ì„±ê³µ"
    },
    {
      "test_id": 5,
      "test_name": "file_save_request",
      "query": "ìœ„ ë‚´ìš© íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜",
      "expected_tool": "file_save",
      "actual_tool": "file_save",
      "file_created": true,
      "file_size_bytes": 1234,
      "response_time_ms": 300,
      "status": "PASS",
      "notes": "íŒŒì¼ ì €ì¥ ì„±ê³µ"
    },
    {
      "test_id": 6,
      "test_name": "general_greeting",
      "query": "ì•ˆë…•í•˜ì„¸ìš”",
      "expected_tool": "general",
      "actual_tool": "general",
      "response_appropriate": true,
      "response_time_ms": 500,
      "status": "PASS",
      "notes": "ì¼ë°˜ ì¸ì‚¬ ì²˜ë¦¬ ì •ìƒ"
    },
    {
      "test_id": 7,
      "test_name": "difficulty_switching_easy",
      "query": "Transformer ì„¤ëª…í•´ì¤˜",
      "difficulty": "easy",
      "explanation_level": "ì´ˆë“±í•™ìƒ ìˆ˜ì¤€",
      "jargon_count": 2,
      "analogy_count": 3,
      "status": "PASS",
      "notes": "Easy ëª¨ë“œ ì„¤ëª… ì ì ˆ"
    },
    {
      "test_id": 8,
      "test_name": "difficulty_switching_hard",
      "query": "Transformer ì„¤ëª…í•´ì¤˜",
      "difficulty": "hard",
      "explanation_level": "ì „ë¬¸ê°€ ìˆ˜ì¤€",
      "jargon_count": 15,
      "technical_depth": "high",
      "status": "PASS",
      "notes": "Hard ëª¨ë“œ ì„¤ëª… ì ì ˆ"
    },
    {
      "test_id": 9,
      "test_name": "context_length_limit",
      "query": "RAG, DPR, REALM, FiD ëª¨ë‘ ì„¤ëª…í•´ì¤˜",
      "expected_papers_count": 20,
      "actual_papers_count": 5,
      "context_tokens": 2111,
      "max_context_tokens": 4000,
      "status": "PASS",
      "notes": "ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë‚´ì—ì„œ ì²˜ë¦¬"
    },
    {
      "test_id": 10,
      "test_name": "error_handling_invalid_query",
      "query": "",
      "expected_error": "empty_query",
      "actual_error": "empty_query",
      "error_handled": true,
      "status": "PASS",
      "notes": "ë¹ˆ ì¿¼ë¦¬ ì—ëŸ¬ ì²˜ë¦¬ ì •ìƒ"
    }
  ],

  "aggregate_metrics": {
    "avg_response_time_ms": 2890,
    "avg_recall_at_5": 0.8,
    "avg_faithfulness": 0.95,
    "agent_routing_accuracy": 0.9,
    "total_cost_usd": 0.234,
    "total_cost_krw": 304.2
  },

  "target_comparison": {
    "success_rate_target": 0.95,
    "success_rate_actual": 0.9,
    "success_rate_status": "NEAR_PASS",
    "p95_latency_target_ms": 6000,
    "p95_latency_actual_ms": 6500,
    "p95_latency_status": "NEAR_PASS"
  },

  "failed_tests_summary": [
    {
      "test_id": 3,
      "test_name": "web_search_latest_info",
      "reason": "Agentê°€ ì›¹ ê²€ìƒ‰ ëŒ€ì‹  RAG ê²€ìƒ‰ ì„ íƒ",
      "action_required": "ë¼ìš°íŒ… ë¡œì§ì— ë‚ ì§œ í‚¤ì›Œë“œ ê°ì§€ ì¶”ê°€"
    }
  ]
}
```

---

### 8. debug/ í´ë”

**ëª©ì **: ê°œë°œ ë° ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì •ë³´ (ì„ íƒ ì‚¬í•­)

#### 8.1 agent_trace.json

```json
{
  "agent_execution": {
    "start_time": "2025-10-31T10:30:20",
    "end_time": "2025-10-31T10:30:25",
    "total_time_ms": 5000,
    "graph_executions": [
      {
        "step": 1,
        "node": "router_node",
        "input": {
          "question": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
          "difficulty": "easy"
        },
        "output": {
          "tool_choice": "rag_paper"
        },
        "execution_time_ms": 200
      },
      {
        "step": 2,
        "node": "rag_paper_node",
        "input": {
          "question": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
          "difficulty": "easy"
        },
        "output": {
          "context": "[ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸...]",
          "papers_count": 5
        },
        "execution_time_ms": 2800
      },
      {
        "step": 3,
        "node": "generate_answer_node",
        "input": {
          "question": "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜",
          "context": "[ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸...]",
          "difficulty": "easy"
        },
        "output": {
          "final_answer": "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±...",
          "answer_length": 450
        },
        "execution_time_ms": 2000
      }
    ],
    "conditional_edges": [
      {
        "from": "router_node",
        "to": "rag_paper_node",
        "condition": "tool_choice == 'rag_paper'"
      },
      {
        "from": "rag_paper_node",
        "to": "generate_answer_node",
        "condition": "context_available == True"
      }
    ]
  }
}
```

#### 8.2 llm_tokens.json

```json
{
  "total_tokens": 2345,
  "total_cost_usd": 0.0234,
  "calls": [
    {
      "call_id": 1,
      "timestamp": "2025-10-31T10:30:23",
      "model": "gpt-4",
      "purpose": "generate_answer",
      "prompt_tokens": 1468,
      "completion_tokens": 877,
      "total_tokens": 2345,
      "cost_usd": 0.0234,
      "execution_time_ms": 2000,
      "temperature": 0.7,
      "max_tokens": 2000,
      "streaming": true
    }
  ],
  "token_breakdown": {
    "system_prompt": 234,
    "user_prompt": 2111,
    "context": 1234,
    "question": 12,
    "completion": 877
  },
  "cost_breakdown": {
    "input_cost": 0.0147,
    "output_cost": 0.0087,
    "total_cost": 0.0234
  }
}
```

#### 8.3 error_trace.log

```
2025-10-31 15:30:25 | [ERROR] pgvector ì—°ê²° ì‹¤íŒ¨
2025-10-31 15:30:25 | Traceback (most recent call last):
2025-10-31 15:30:25 |   File "src/agent/tools/rag_paper.py", line 45, in search_paper_database
2025-10-31 15:30:25 |     results = vectorstore.similarity_search(query, k=5)
2025-10-31 15:30:25 |   File "langchain_postgres/vectorstores.py", line 234, in similarity_search
2025-10-31 15:30:25 |     conn = psycopg2.connect(self.connection_string)
2025-10-31 15:30:25 | psycopg2.OperationalError: could not connect to server
2025-10-31 15:30:25 |
2025-10-31 15:30:25 | [RESOLUTION] Retry 1/3: Attempting reconnection...
2025-10-31 15:30:27 | [SUCCESS] Connection established
```

---

## ExperimentManager êµ¬í˜„

**íŒŒì¼ ê²½ë¡œ**: `src/utils/experiment_manager.py`

```python
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from src.utils.logger import Logger


class ExperimentManager:
    """
    ì‹¤í—˜ í´ë” ìƒì„± ë° ê´€ë¦¬ í´ë˜ìŠ¤

    DB, UI, í”„ë¡¬í”„íŠ¸ ë“± ëª¨ë“  ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡
    """

    def __init__(self):
        """ì‹¤í—˜ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        today = datetime.now().strftime("%Y%m%d")
        time_now = datetime.now().strftime("%H%M%S")

        # ë‹¹ì¼ session ID ìƒì„±
        session_id = self._get_next_session_id(today)

        # ë©”ì¸ í´ë”
        self.experiment_dir = Path(
            f"experiments/{today}/{today}_{time_now}_session_{session_id:03d}"
        )
        self.experiment_dir.mkdir(parents=True, exist_ok=True)

        # ì„œë¸Œ í´ë” ìƒì„±
        self.tools_dir = self.experiment_dir / "tools"
        self.database_dir = self.experiment_dir / "database"
        self.prompts_dir = self.experiment_dir / "prompts"
        self.ui_dir = self.experiment_dir / "ui"
        self.outputs_dir = self.experiment_dir / "outputs"
        self.evaluation_dir = self.experiment_dir / "evaluation"
        self.debug_dir = self.experiment_dir / "debug"

        # í•„ìˆ˜ í´ë” ìƒì„±
        for folder in [self.tools_dir, self.database_dir, self.prompts_dir,
                       self.ui_dir, self.outputs_dir, self.evaluation_dir]:
            folder.mkdir(exist_ok=True)

        # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        self.metadata = {
            'session_id': f"{session_id:03d}",
            'start_time': datetime.now().isoformat(),
            'difficulty': None,
            'tool_used': None,
            'user_query': None,
            'success': None,
            'response_time_ms': None,
            'end_time': None
        }

        self.metadata_file = self.experiment_dir / "metadata.json"

        # Logger ì´ˆê¸°í™”
        self.logger = Logger(str(self.experiment_dir / "chatbot.log"))
        self.logger.write(f"ì„¸ì…˜ ì‹œì‘: session_{session_id:03d}")
        self.logger.write(f"í´ë” ê²½ë¡œ: {self.experiment_dir}")

        # DB ê´€ë ¨ ì´ˆê¸°í™”
        self.db_queries = []
        self.pgvector_searches = []
        self.search_results = {}
        self.db_performance = {
            'summary': {},
            'queries': []
        }

    def _get_next_session_id(self, date: str) -> int:
        """ë‹¹ì¼ ë‹¤ìŒ session ID ë°˜í™˜"""
        date_dir = Path(f"experiments/{date}")
        if not date_dir.exists():
            return 1

        existing_sessions = list(date_dir.glob(f"{date}_*_session_*"))
        if not existing_sessions:
            return 1

        max_id = 0
        for session_dir in existing_sessions:
            try:
                session_id_str = session_dir.name.split('_')[-1]
                session_id = int(session_id_str)
                max_id = max(max_id, session_id)
            except (IndexError, ValueError):
                continue

        return max_id + 1

    # ===== ë„êµ¬ ë¡œê·¸ =====

    def get_tool_logger(self, tool_name: str) -> Logger:
        """
        ë„êµ¬ë³„ Logger ìƒì„±

        Args:
            tool_name: ë„êµ¬ëª… (rag_paper, rag_glossary, web_search ë“±)

        Returns:
            Logger ì¸ìŠ¤í„´ìŠ¤
        """
        log_path = self.tools_dir / f"{tool_name}.log"
        return Logger(str(log_path))

    # ===== DB ê´€ë ¨ =====

    def log_sql_query(
        self,
        query: str,
        description: str = "",
        tool: str = "",
        execution_time_ms: Optional[int] = None
    ):
        """
        SQL ì¿¼ë¦¬ ê¸°ë¡

        Args:
            query: SQL ì¿¼ë¦¬ë¬¸
            description: ì¿¼ë¦¬ ì„¤ëª…
            tool: ì‚¬ìš©í•œ ë„êµ¬ëª…
            execution_time_ms: ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        query_record = f"""-- ì‹¤í–‰ ì‹œê°„: {timestamp}
-- ë„êµ¬: {tool}
-- ì„¤ëª…: {description}
"""
        if execution_time_ms:
            query_record += f"-- ì‹¤í–‰ ì†Œìš”: {execution_time_ms}ms\n"

        query_record += f"\n{query};\n\n"

        self.db_queries.append(query_record)

        # queries.sql íŒŒì¼ì— ì¦‰ì‹œ ì¶”ê°€
        with open(self.database_dir / "queries.sql", 'a', encoding='utf-8') as f:
            f.write(query_record)

        self.logger.write(f"SQL ì¿¼ë¦¬ ê¸°ë¡: {description}")

    def log_pgvector_search(self, search_info: Dict):
        """
        pgvector ê²€ìƒ‰ ê¸°ë¡

        Args:
            search_info: ê²€ìƒ‰ ì •ë³´ ë”•ì…”ë„ˆë¦¬
                - tool: ë„êµ¬ëª…
                - collection: ì»¬ë ‰ì…˜ëª…
                - query_text: ê²€ìƒ‰ ì¿¼ë¦¬
                - top_k: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
                - execution_time_ms: ì‹¤í–‰ ì‹œê°„
                ë“±
        """
        search_info['timestamp'] = datetime.now().isoformat()
        self.pgvector_searches.append(search_info)

        # pgvector_searches.json ì—…ë°ì´íŠ¸
        with open(self.database_dir / "pgvector_searches.json", 'w', encoding='utf-8') as f:
            json.dump(self.pgvector_searches, f, ensure_ascii=False, indent=2)

        self.logger.write(f"pgvector ê²€ìƒ‰ ê¸°ë¡: {search_info.get('tool', 'unknown')}")

    def save_search_results(self, tool_name: str, results: Dict):
        """
        DB ê²€ìƒ‰ ê²°ê³¼ ì €ì¥

        Args:
            tool_name: ë„êµ¬ëª…
            results: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results['timestamp'] = datetime.now().isoformat()
        self.search_results[tool_name] = results

        # search_results.json ì—…ë°ì´íŠ¸
        with open(self.database_dir / "search_results.json", 'w', encoding='utf-8') as f:
            json.dump(self.search_results, f, ensure_ascii=False, indent=2)

        self.logger.write(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {tool_name}")

    def save_db_performance(self, performance_data: Dict):
        """
        DB ì„±ëŠ¥ ì •ë³´ ì €ì¥

        Args:
            performance_data: ì„±ëŠ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(self.database_dir / "db_performance.json", 'w', encoding='utf-8') as f:
            json.dump(performance_data, f, ensure_ascii=False, indent=2)

        self.logger.write("DB ì„±ëŠ¥ ì •ë³´ ì €ì¥ ì™„ë£Œ")

    # ===== í”„ë¡¬í”„íŠ¸ ê´€ë ¨ =====

    def save_system_prompt(self, system_prompt: str, metadata: Optional[Dict] = None):
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥

        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        content = system_prompt

        if metadata:
            content += "\n\n===== ë©”íƒ€ë°ì´í„° =====\n"
            for key, value in metadata.items():
                content += f"{key}: {value}\n"

        with open(self.prompts_dir / "system_prompt.txt", 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.write("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ")

    def save_user_prompt(self, user_prompt: str, metadata: Optional[Dict] = None):
        """
        ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì €ì¥

        Args:
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        content = user_prompt

        if metadata:
            content += "\n\n===== ë©”íƒ€ë°ì´í„° =====\n"
            for key, value in metadata.items():
                content += f"{key}: {value}\n"

        with open(self.prompts_dir / "user_prompt.txt", 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.write("ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ")

    def save_final_prompt(self, final_prompt: str, metadata: Optional[Dict] = None):
        """
        ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥

        Args:
            final_prompt: ìµœì¢… í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        content = final_prompt

        if metadata:
            content += "\n\n===== ë©”íƒ€ë°ì´í„° =====\n"
            for key, value in metadata.items():
                content += f"{key}: {value}\n"

        with open(self.prompts_dir / "final_prompt.txt", 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.write("ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ")

    def save_prompt_template(self, template_info: Dict):
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ë³´ ì €ì¥

        Args:
            template_info: í…œí”Œë¦¿ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        with open(self.prompts_dir / "prompt_template.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(template_info, f, allow_unicode=True, sort_keys=False)

        self.logger.write("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì €ì¥ ì™„ë£Œ")

    # ===== UI ê´€ë ¨ =====

    def save_streamlit_session(self, session_data: Dict):
        """
        Streamlit ì„¸ì…˜ ìƒíƒœ ì €ì¥

        Args:
            session_data: ì„¸ì…˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(self.ui_dir / "streamlit_session.json", 'w', encoding='utf-8') as f:
            json.dump(session_data, f, ensure_ascii=False, indent=2)

        self.logger.write("Streamlit ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")

    def log_ui_interaction(self, interaction: str):
        """
        UI ì¸í„°ë™ì…˜ ë¡œê·¸

        Args:
            interaction: ì¸í„°ë™ì…˜ ì„¤ëª…
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_line = f"{timestamp} | {interaction}\n"

        with open(self.ui_dir / "user_interactions.log", 'a', encoding='utf-8') as f:
            f.write(log_line)

    def log_ui_event(self, event: Dict):
        """
        UI ì´ë²¤íŠ¸ ê¸°ë¡

        Args:
            event: ì´ë²¤íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        event['timestamp'] = datetime.now().isoformat()

        # ê¸°ì¡´ ì´ë²¤íŠ¸ ì½ê¸°
        events_file = self.ui_dir / "ui_events.json"
        if events_file.exists():
            with open(events_file, 'r', encoding='utf-8') as f:
                events = json.load(f)
        else:
            events = []

        events.append(event)

        # ì—…ë°ì´íŠ¸
        with open(events_file, 'w', encoding='utf-8') as f:
            json.dump(events, f, ensure_ascii=False, indent=2)

    # ===== ì¶œë ¥ ê´€ë ¨ =====

    def save_output(self, filename: str, content: str):
        """
        ê²°ê³¼ë¬¼ ì €ì¥

        Args:
            filename: íŒŒì¼ëª…
            content: ë‚´ìš©
        """
        output_path = self.outputs_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)

        self.logger.write(f"ê²°ê³¼ë¬¼ ì €ì¥: {filename}")

    # ===== í‰ê°€ ì§€í‘œ ê´€ë ¨ =====

    def save_rag_metrics(self, metrics: Dict):
        """
        RAG í‰ê°€ ì§€í‘œ ì €ì¥

        Args:
            metrics: RAG í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        metrics['timestamp'] = datetime.now().isoformat()

        with open(self.evaluation_dir / "rag_metrics.json", 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)

        self.logger.write("RAG í‰ê°€ ì§€í‘œ ì €ì¥ ì™„ë£Œ")

    def save_agent_accuracy(self, accuracy_data: Dict):
        """
        Agent ì •í™•ë„ ì €ì¥

        Args:
            accuracy_data: Agent ì •í™•ë„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        accuracy_data['timestamp'] = datetime.now().isoformat()

        with open(self.evaluation_dir / "agent_accuracy.json", 'w', encoding='utf-8') as f:
            json.dump(accuracy_data, f, ensure_ascii=False, indent=2)

        self.logger.write("Agent ì •í™•ë„ ì €ì¥ ì™„ë£Œ")

    def save_latency_report(self, latency_data: Dict):
        """
        ì‘ë‹µ ì‹œê°„ ë¶„ì„ ì €ì¥

        Args:
            latency_data: ì‘ë‹µ ì‹œê°„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        latency_data['timestamp'] = datetime.now().isoformat()

        with open(self.evaluation_dir / "latency_report.json", 'w', encoding='utf-8') as f:
            json.dump(latency_data, f, ensure_ascii=False, indent=2)

        self.logger.write("ì‘ë‹µ ì‹œê°„ ë¶„ì„ ì €ì¥ ì™„ë£Œ")

    def save_cost_analysis(self, cost_data: Dict):
        """
        ë¹„ìš© ë¶„ì„ ì €ì¥

        Args:
            cost_data: ë¹„ìš© ë¶„ì„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        cost_data['timestamp'] = datetime.now().isoformat()

        with open(self.evaluation_dir / "cost_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(cost_data, f, ensure_ascii=False, indent=2)

        self.logger.write("ë¹„ìš© ë¶„ì„ ì €ì¥ ì™„ë£Œ")

    def save_test_results(self, test_data: Dict):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥

        Args:
            test_data: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        test_data['timestamp'] = datetime.now().isoformat()

        with open(self.evaluation_dir / "test_results.json", 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)

        self.logger.write("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    # ===== ë””ë²„ê·¸ ê´€ë ¨ =====

    def save_debug_info(self, filename: str, data: Dict):
        """
        ë””ë²„ê·¸ ì •ë³´ ì €ì¥

        Args:
            filename: íŒŒì¼ëª…
            data: ë””ë²„ê·¸ ë°ì´í„°
        """
        # debug í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        self.debug_dir.mkdir(exist_ok=True)

        debug_path = self.debug_dir / filename
        with open(debug_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    # ===== ë©”íƒ€ë°ì´í„° ê´€ë ¨ =====

    def update_metadata(self, **kwargs):
        """
        ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸

        Args:
            **kwargs: ì—…ë°ì´íŠ¸í•  í‚¤-ê°’ ìŒ
        """
        self.metadata.update(kwargs)

        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

        self.logger.write(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸: {list(kwargs.keys())}")

    def save_config(self, config: Dict):
        """
        ì „ì²´ ì„¤ì • ì €ì¥

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        config_path = self.experiment_dir / "config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, sort_keys=False)

        self.logger.write("ì„¤ì • íŒŒì¼ ì €ì¥ ì™„ë£Œ")

    def close(self):
        """ì‹¤í—˜ ì¢…ë£Œ"""
        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        self.metadata['end_time'] = datetime.now().isoformat()

        # ìµœì¢… ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

        self.logger.write("=" * 50)
        self.logger.write("ì‹¤í—˜ ì¢…ë£Œ")
        self.logger.write("=" * 50)
        self.logger.close()

    def __enter__(self):
        """with ë¬¸ ì§€ì›"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """with ë¬¸ ì¢…ë£Œ ì‹œ ìë™ close"""
        self.close()
```

---

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### main.py

```python
from src.utils.experiment_manager import ExperimentManager
from src.agent.graph import create_agent
from src.llm.client import LLMClient
import time

def main():
    # 1. ì‹¤í—˜ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    with ExperimentManager() as exp:

        # 2. UI ì¸í„°ë™ì…˜ ê¸°ë¡
        exp.log_ui_interaction("í˜ì´ì§€ ì ‘ì†")
        exp.log_ui_interaction("ì‚¬ì´ë“œë°” ì—´ê¸°")

        # 3. ë‚œì´ë„ ì„ íƒ
        difficulty = "easy"
        exp.log_ui_interaction(f"ë‚œì´ë„ ì„ íƒ: {difficulty}")
        exp.log_ui_event({
            "event_type": "difficulty_changed",
            "old_value": None,
            "new_value": difficulty
        })

        # 4. ì‚¬ìš©ì ì§ˆë¬¸
        question = "RAGì— ëŒ€í•´ ì•Œë ¤ì¤˜"
        exp.log_ui_interaction(f"ì§ˆë¬¸ ì…ë ¥: {question}")
        exp.log_ui_event({
            "event_type": "message_submitted",
            "message_length": len(question)
        })

        # 5. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        exp.update_metadata(
            user_query=question,
            difficulty=difficulty
        )

        # 6. í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = """ë‹¹ì‹ ì€ ë…¼ë¬¸ì„ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰¬ìš´ ìš©ì–´ ì‚¬ìš©
- ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ë§ì´ í™œìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
- í•œê¸€ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€"""

        exp.save_system_prompt(system_prompt, {
            "ë‚œì´ë„": difficulty,
            "ìƒì„± ì‹œê°„": time.time(),
            "í…œí”Œë¦¿": "EASY_SYSTEM_PROMPT"
        })

        # 7. Agent ì‹¤í–‰
        exp.logger.write("Agent ì‹¤í–‰ ì‹œì‘")
        start_time = time.time()

        agent = create_agent()
        result = agent.invoke({
            "question": question,
            "difficulty": difficulty
        })

        response_time_ms = int((time.time() - start_time) * 1000)

        # 8. ì‚¬ìš©ëœ ë„êµ¬ì— ë”°ë¼ ë¡œê·¸ ê¸°ë¡
        tool_used = result.get('tool_used', 'unknown')
        exp.update_metadata(tool_used=tool_used)

        if tool_used == 'rag_paper':
            # ë„êµ¬ë³„ Logger
            tool_logger = exp.get_tool_logger('rag_paper')
            tool_logger.write("ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘")
            tool_logger.write(f"ê²€ìƒ‰ ì¿¼ë¦¬: {question}")

            # pgvector ê²€ìƒ‰ ê¸°ë¡
            exp.log_pgvector_search({
                "tool": "rag_paper",
                "collection": "paper_chunks",
                "query_text": question,
                "embedding_dimension": 1536,
                "search_type": "similarity",
                "top_k": 5,
                "execution_time_ms": 45
            })

            # SQL ì¿¼ë¦¬ ê¸°ë¡
            exp.log_sql_query(
                query="""SELECT paper_id, title, authors, publish_date
FROM papers
WHERE paper_id IN (123, 456, 789, 234, 567)""",
                description="ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ",
                tool="rag_paper",
                execution_time_ms=12
            )

            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            exp.save_search_results("rag_paper", {
                "query": question,
                "difficulty": difficulty,
                "results_count": 5,
                "papers": [
                    {
                        "paper_id": 123,
                        "title": "Retrieval-Augmented Generation...",
                        "authors": "Patrick Lewis, et al.",
                        "relevance_score": 0.92
                    }
                    # ... ë” ë§ì€ ê²°ê³¼
                ]
            })

            tool_logger.write("ë…¼ë¬¸ ê²€ìƒ‰ ì™„ë£Œ")
            tool_logger.close()

        # 9. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì €ì¥
        user_prompt = f"""[ì°¸ê³  ë…¼ë¬¸]
1. Retrieval-Augmented Generation (Patrick Lewis et al., 2020)
   ...

[ì§ˆë¬¸]
{question}

ìœ„ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        exp.save_user_prompt(user_prompt, {
            "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜": 5,
            "ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´": 1234,
            "ë„êµ¬": tool_used
        })

        # 10. ìµœì¢… ë‹µë³€ ì €ì¥
        final_answer = result['final_answer']
        exp.save_output('response.txt', final_answer)

        # 11. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ë³´ ì €ì¥
        exp.save_prompt_template({
            "difficulty": difficulty,
            "template_name": "EASY_SYSTEM_PROMPT",
            "llm_config": {
                "model": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 2000
            },
            "token_count": {
                "system": 234,
                "user": 1234,
                "total": 1468
            }
        })

        # 12. UI ì´ë²¤íŠ¸ ê¸°ë¡
        exp.log_ui_event({
            "event_type": "response_received",
            "response_length": len(final_answer),
            "response_time_ms": response_time_ms
        })

        # 13. DB ì„±ëŠ¥ ì •ë³´ ì €ì¥
        exp.save_db_performance({
            "summary": {
                "total_queries": 4,
                "total_execution_time_ms": 120
            },
            "queries": [
                {
                    "query_type": "pgvector_similarity",
                    "execution_time_ms": 45
                },
                {
                    "query_type": "select",
                    "table": "papers",
                    "execution_time_ms": 12
                }
            ]
        })

        # 14. ìµœì¢… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        exp.update_metadata(
            success=True,
            response_time_ms=response_time_ms,
            response_length=len(final_answer)
        )

        exp.logger.write(f"ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ ({response_time_ms}ms)")

        # 15. ExperimentManagerëŠ” with ë¬¸ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ close()

if __name__ == "__main__":
    main()
```

---

## í´ë” ê²€ìƒ‰ ë° ë¶„ì„

### scripts/find_experiments.py

```python
import json
from pathlib import Path
from typing import Optional, List, Dict

def find_experiments(
    difficulty: Optional[str] = None,
    tool: Optional[str] = None,
    date: Optional[str] = None,
    min_response_time: Optional[int] = None,
    max_response_time: Optional[int] = None
) -> List[Dict]:
    """
    ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì‹¤í—˜ ê²€ìƒ‰

    Returns:
        ê²€ìƒ‰ëœ ì‹¤í—˜ ëª©ë¡
    """
    results = []

    # ê²€ìƒ‰ ê²½ë¡œ
    if date:
        search_path = Path(f"experiments/{date}")
        if not search_path.exists():
            return []
    else:
        search_path = Path("experiments")

    # ëª¨ë“  metadata.json ì°¾ê¸°
    for meta_file in search_path.rglob("metadata.json"):
        try:
            with open(meta_file, encoding='utf-8') as f:
                meta = json.load(f)
        except Exception:
            continue

        # í•„í„° ì ìš©
        if difficulty and meta.get('difficulty') != difficulty:
            continue
        if tool and meta.get('tool_used') != tool:
            continue
        if min_response_time and meta.get('response_time_ms', 0) < min_response_time:
            continue
        if max_response_time and meta.get('response_time_ms', float('inf')) > max_response_time:
            continue

        results.append({
            'path': meta_file.parent,
            'metadata': meta
        })

    results.sort(key=lambda x: x['metadata'].get('start_time', ''))
    return results

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    easy_experiments = find_experiments(difficulty="easy")
    print(f"Easy ëª¨ë“œ ì‹¤í—˜: {len(easy_experiments)}ê°œ")

    for exp in easy_experiments:
        print(f"  - {exp['path'].name}: {exp['metadata']['user_query']}")
```

---

## ì°¸ê³  ë¬¸ì„œ

- [05_ë¡œê¹…_ì‹œìŠ¤í…œ.md](../PRD/05_ë¡œê¹…_ì‹œìŠ¤í…œ.md) - Logger í´ë˜ìŠ¤ ì‚¬ìš©ë²•
- [06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md](../PRD/06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md) - ì‹¤í—˜ í´ë” ê·œì¹™
- [11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md](../PRD/11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md) - DB ìŠ¤í‚¤ë§ˆ
- [15_í”„ë¡¬í”„íŠ¸_ì—”ì§€ë‹ˆì–´ë§.md](../PRD/15_í”„ë¡¬í”„íŠ¸_ì—”ì§€ë‹ˆì–´ë§.md) - í”„ë¡¬í”„íŠ¸ ì„¤ê³„
- [16_UI_ì„¤ê³„.md](../PRD/16_UI_ì„¤ê³„.md) - Streamlit UI
- [ë‹´ë‹¹ì—­í• _06_ë¡œê¹…_ëª¨ë‹ˆí„°ë§.md](../roles/ë‹´ë‹¹ì—­í• _06_ë¡œê¹…_ëª¨ë‹ˆí„°ë§.md) - ë¡œê¹… ë‹´ë‹¹ ì—­í• 

---

## ë²„ì „ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|-----------|
| 1.0 | 2025-10-31 | ì´ˆì•ˆ ì‘ì„± - ì¢…í•© í´ë” êµ¬ì¡° ì •ì˜ |
