# ë‹´ë‹¹ì—­í• : ìµœí˜„í™” - AI Agent ë©”ì¸

## ë‹´ë‹¹ì ì •ë³´
- **ì´ë¦„**: ìµœí˜„í™”
- **ì—­í• **: íŒ€ì¥
- **ì°¸ì—¬ ê¸°ê°„**: ì „ì²´ ê¸°ê°„
- **í•µì‹¬ ì—­í• **: AI Agent ê·¸ë˜í”„ ì„¤ê³„ ë° êµ¬í˜„, LLM í´ë¼ì´ì–¸íŠ¸, ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ, í”„ë¡œì íŠ¸ ì´ê´„

---

## ë‹´ë‹¹ ëª¨ë“ˆ ë° ë„êµ¬

### 1. AI Agent ê·¸ë˜í”„ (`src/agent/`)
- LangGraph StateGraph ì„¤ê³„ ë° êµ¬í˜„
- ë¼ìš°í„° ë…¸ë“œ (ì§ˆë¬¸ ë¶„ì„ ë° ë„êµ¬ ì„ íƒ)
- ì¡°ê±´ë¶€ ì—£ì§€ (conditional_edges)
- Agent State ê´€ë¦¬ (TypedDict)
- ë„êµ¬ ë…¸ë“œ ì—°ê²° (6ê°€ì§€ ë„êµ¬)

### 2. LLM í´ë¼ì´ì–¸íŠ¸ (`src/llm/`)
- Langchain ChatOpenAI ë° Solar(Upstage) API ë˜í¼ êµ¬í˜„
- ë‹¤ì¤‘ LLM ì„ íƒ ë¡œì§ (OpenAI + Solar)
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (astream)
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (get_openai_callback)
- Function calling ì„¤ì •

### 3. ëŒ€í™” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ (`src/memory/`)
- Langchain ConversationBufferMemory êµ¬í˜„
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ChatMessageHistory)
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”
- ì„¸ì…˜ ê´€ë¦¬

### 4. ë„êµ¬: ë…¼ë¬¸ ìš”ì•½ ë„êµ¬ (`src/tools/summarize.py`)
- Langchain @tool ë°ì½”ë ˆì´í„° í™œìš©
- load_summarize_chain êµ¬í˜„ (stuff, map_reduce, refine)
- ë‚œì´ë„ë³„ ìš”ì•½ (Easy/Hard)
- ì„¹ì…˜ë³„ ìš”ì•½ ê¸°ëŠ¥

### 5. ë„êµ¬: ì¼ë°˜ ë‹µë³€ ë„êµ¬
- LLM ì§ì ‘ í˜¸ì¶œ (ChatOpenAI)
- ê°„ë‹¨í•œ ì¸ì‚¬, ì¼ë°˜ ìƒì‹ ì§ˆë¬¸ ì²˜ë¦¬
- ë‚œì´ë„ë³„ í”„ë¡¬í”„íŠ¸ ì ìš©

### 6. í”„ë¡œì íŠ¸ ì´ê´„
- ê¸°ëŠ¥ í†µí•© ë° ë””ë²„ê¹…
- main.py ì‘ì„± (LangGraph ì»´íŒŒì¼ ë° ì‹¤í–‰)
- ì½”ë“œ ë¦¬ë·° ë° PR ê´€ë¦¬
- ë°œí‘œ ìë£Œ ì´ê´„

---

## ë„êµ¬ 1: ì¼ë°˜ ë‹µë³€ ë„êµ¬

### ê¸°ëŠ¥ ì„¤ëª…
ê°„ë‹¨í•œ ì¸ì‚¬, ì¼ë°˜ ìƒì‹ ì§ˆë¬¸ì— LLMì˜ ìì²´ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì§ì ‘ ë‹µë³€í•˜ëŠ” ë„êµ¬

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `src/agent/nodes.py`

1. **ì¼ë°˜ ë‹µë³€ ë…¸ë“œ í•¨ìˆ˜ ìƒì„±**
   - AgentStateë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ëŠ” `general_answer_node` í•¨ìˆ˜ ì •ì˜
   - stateì—ì„œ questionê³¼ difficulty ì¶”ì¶œ
   - ë‚œì´ë„ì— ë”°ë¼ ë‹¤ë¥¸ SystemMessage ì„¤ì •
     - Easy: ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ
     - Hard: ì „ë¬¸ì ì´ê³  ê¸°ìˆ ì ì¸ ì–¸ì–´ë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ

2. **LLM í˜¸ì¶œ êµ¬ì„±**
   - langchain_openai.ChatOpenAI ì‚¬ìš©
   - SystemMessageì™€ HumanMessageë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
   - llm.invoke() ë©”ì„œë“œë¡œ ë©”ì‹œì§€ ì „ë‹¬
   - ì‘ë‹µ ê²°ê³¼ë¥¼ state["final_answer"]ì— ì €ì¥

3. **ë¼ìš°í„° ë…¸ë“œì—ì„œ ì¼ë°˜ ë‹µë³€ íŒë‹¨ ë¡œì§**
   - ì‚¬ìš©ì ì§ˆë¬¸ì„ LLMì— ì „ë‹¬í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ ì„ íƒ
   - ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ì‘ì„± (ì¼ë°˜ ì¸ì‚¬, ìƒì‹ ì§ˆë¬¸ ë“±)
   - ì„ íƒëœ ë„êµ¬ë¥¼ state["tool_choice"]ì— ì €ì¥

### ì‚¬ìš©í•˜ëŠ” DB
**DB ì‚¬ìš© ì—†ìŒ** (LLM ìì²´ ì§€ì‹ í™œìš©)

### ì˜ˆì œ ì½”ë“œ

```python
# src/agent/nodes.py

import os
from datetime import datetime
from typing import TypedDict
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "agent_general"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class AgentState(TypedDict):
    question: str
    difficulty: str
    tool_choice: str
    final_answer: str

def general_answer_node(state: AgentState):
    """
    ì¼ë°˜ ë‹µë³€ ë…¸ë“œ: LLMì˜ ìì²´ ì§€ì‹ìœ¼ë¡œ ì§ì ‘ ë‹µë³€
    """
    question = state["question"]
    difficulty = state.get("difficulty", "easy")

    logger.write(f"ì¼ë°˜ ë‹µë³€ ë…¸ë“œ ì‹¤í–‰: {question}")
    logger.write(f"ë‚œì´ë„: {difficulty}")

    # ë‚œì´ë„ì— ë”°ë¥¸ SystemMessage ì„¤ì •
    if difficulty == "easy":
        system_msg = SystemMessage(content="""
ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì´ˆì‹¬ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì „ë¬¸ ìš©ì–´ëŠ” ìµœì†Œí™”í•˜ê³  ì¼ìƒì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """)
    else:  # hard
        system_msg = SystemMessage(content="""
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê¸°ìˆ ì ì¸ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        """)

    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    # ë©”ì‹œì§€ êµ¬ì„± ë° LLM í˜¸ì¶œ
    messages = [system_msg, HumanMessage(content=question)]
    response = llm.invoke(messages)

    logger.write(f"LLM ì‘ë‹µ: {response.content}")

    # ìµœì¢… ë‹µë³€ ì €ì¥
    state["final_answer"] = response.content

    return state
```

---

## ë„êµ¬ 2: ë…¼ë¬¸ ìš”ì•½ ë„êµ¬

### ê¸°ëŠ¥ ì„¤ëª…
íŠ¹ì • ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš©ì„ ë‚œì´ë„ë³„(Easy/Hard)ë¡œ ìš”ì•½í•˜ëŠ” ë„êµ¬

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `src/tools/summarize.py`, `src/llm/chains.py`

1. **ë…¼ë¬¸ ê²€ìƒ‰ ë° ì „ì²´ ë‚´ìš© ì¡°íšŒ** (`src/tools/summarize.py`)
   - @tool ë°ì½”ë ˆì´í„°ë¡œ `summarize_paper` í•¨ìˆ˜ ì •ì˜
   - íŒŒë¼ë¯¸í„°: paper_title (str), difficulty (str)
   - PostgreSQL ì—°ê²° (psycopg2 ì‚¬ìš©)
   - papers í…Œì´ë¸”ì—ì„œ ILIKEë¡œ ë…¼ë¬¸ ì œëª© ê²€ìƒ‰
   - paper_id ì¶”ì¶œ í›„ Vector DBì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
   - filter íŒŒë¼ë¯¸í„°ë¡œ {"paper_id": paper_id} ì „ë‹¬
   - ë‚œì´ë„ì— ë”°ë¼ ì ì ˆí•œ ìš”ì•½ ì²´ì¸ ì„ íƒ í›„ ì‹¤í–‰

2. **ìš”ì•½ ì²´ì¸ êµ¬í˜„** (`src/llm/chains.py`)
   - Easy ëª¨ë“œ í”„ë¡¬í”„íŠ¸: PromptTemplateë¡œ ì´ˆì‹¬ììš© ìš”ì•½ ê·œì¹™ ì •ì˜
     - ì „ë¬¸ ìš©ì–´ ì‰½ê²Œ í’€ì´, í•µì‹¬ ì•„ì´ë””ì–´ 3ê°€ì§€ ì´ë‚´, ì‹¤ìƒí™œ ë¹„ìœ  í¬í•¨
   - Hard ëª¨ë“œ í”„ë¡¬í”„íŠ¸: ì „ë¬¸ê°€ìš© ìš”ì•½ ê·œì¹™ ì •ì˜
     - ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­, ìˆ˜ì‹/ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…, ê´€ë ¨ ì—°êµ¬ ë¹„êµ
   - load_summarize_chainìœ¼ë¡œ ì²´ì¸ ìƒì„±
     - chain_type: "stuff" (ì§§ì€ ë…¼ë¬¸), "map_reduce" (ì¤‘ê°„ ë…¼ë¬¸), "refine" (ê¸´ ë…¼ë¬¸)

3. **ìš”ì•½ ë°©ì‹ ì„ íƒ ë¡œì§**
   - ë…¼ë¬¸ ì²­í¬ ìˆ˜ì— ë”°ë¼ ì ì ˆí•œ chain_type ì„ íƒ
   - 5ê°œ ì´í•˜: stuff (ëª¨ë“  ì²­í¬ í•œ ë²ˆì— ì²˜ë¦¬)
   - 5~15ê°œ: map_reduce (ê° ì²­í¬ ìš”ì•½ í›„ í†µí•©)
   - 15ê°œ ì´ìƒ: refine (ìˆœì°¨ì  ìš”ì•½)

### ì‚¬ìš©í•˜ëŠ” DB

#### PostgreSQL + pgvector (Vector DB)
- **ì»¬ë ‰ì…˜**: `paper_chunks`
- **ì—­í• **: ë…¼ë¬¸ ì „ì²´ ë‚´ìš©ì„ ì²­í¬ë¡œ ë‚˜ëˆ  ì €ì¥ (pgvector extension ì‚¬ìš©)
- **ë©”íƒ€ë°ì´í„° í•„í„°**: `paper_id`ë¡œ íŠ¹ì • ë…¼ë¬¸ì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
- **ê²€ìƒ‰ ë°©ì‹**: ì œëª© ìœ ì‚¬ë„ ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° í•„í„°
- **ë²¡í„° ê²€ìƒ‰**: Cosine Similarity, L2 Distance

#### PostgreSQL (ê´€ê³„í˜• ë°ì´í„°)
- **í…Œì´ë¸”**: `papers`
- **ì—­í• **: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (ì œëª©ìœ¼ë¡œ paper_id ì°¾ê¸°)
- **ì¿¼ë¦¬**: `SELECT * FROM papers WHERE title ILIKE '%{paper_title}%'`

### ì˜ˆì œ ì½”ë“œ

```python
# src/tools/summarize.py

import os
from datetime import datetime
from langchain.tools import tool
from langchain_postgres.vectorstores import PGVector
from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
import psycopg2
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "agent_summarize"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

@tool
def summarize_paper(paper_title: str, difficulty: str = "easy") -> str:
    """
    íŠ¹ì • ë…¼ë¬¸ì„ ìš”ì•½í•©ë‹ˆë‹¤. ë‚œì´ë„ì— ë”°ë¼ ì´ˆì‹¬ììš©/ì „ë¬¸ê°€ìš© ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        paper_title: ë…¼ë¬¸ ì œëª©
        difficulty: 'easy' (ì´ˆì‹¬ì) ë˜ëŠ” 'hard' (ì „ë¬¸ê°€)

    Returns:
        ë…¼ë¬¸ ìš”ì•½ ë‚´ìš©
    """
    logger.write(f"ë…¼ë¬¸ ìš”ì•½ ì‹œì‘: {paper_title}")
    logger.write(f"ë‚œì´ë„: {difficulty}")

    # 1. PostgreSQLì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    cursor.execute(
        "SELECT * FROM papers WHERE title ILIKE %s",
        (f"%{paper_title}%",)
    )
    paper_meta = cursor.fetchone()

    if not paper_meta:
        logger.write(f"ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {paper_title}")
        return f"'{paper_title}' ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    paper_id = paper_meta[0]
    logger.write(f"ë…¼ë¬¸ ID: {paper_id}")

    # 2. Vector DBì—ì„œ ë…¼ë¬¸ ì „ì²´ ë‚´ìš© ì¡°íšŒ
    vectorstore = PGVector(
        collection_name="paper_chunks",
        connection_string="postgresql://user:password@localhost:5432/papers"
    )

    paper_chunks = vectorstore.similarity_search(
        paper_title,
        k=10,
        filter={"paper_id": paper_id}
    )

    logger.write(f"ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(paper_chunks)}")

    # 3. ë‚œì´ë„ë³„ í”„ë¡¬í”„íŠ¸
    if difficulty == "easy":
        prompt_template = """
ë‹¤ìŒ ë…¼ë¬¸ì„ ì´ˆì‹¬ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
- í•µì‹¬ ì•„ì´ë””ì–´ 3ê°€ì§€
- ì‹¤ìƒí™œ ë¹„ìœ  í¬í•¨

ë…¼ë¬¸ ë‚´ìš©: {text}

ì‰¬ìš´ ìš”ì•½:
        """
    else:  # hard
        prompt_template = """
ë‹¤ìŒ ë…¼ë¬¸ì„ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ í¬í•¨
- ìˆ˜ì‹ ë° ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…
- ê´€ë ¨ ì—°êµ¬ì™€ì˜ ë¹„êµ

ë…¼ë¬¸ ë‚´ìš©: {text}

ì „ë¬¸ê°€ìš© ìš”ì•½:
        """

    PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])

    # 4. ìš”ì•½ ì²´ì¸ ì‹¤í–‰
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    chain = load_summarize_chain(llm, chain_type="stuff", prompt=PROMPT)

    logger.write("ìš”ì•½ ì²´ì¸ ì‹¤í–‰ ì¤‘...")
    summary = chain.run(paper_chunks)

    logger.write(f"ìš”ì•½ ì™„ë£Œ: {len(summary)} ê¸€ì")

    return summary
```

---

## Agent ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

### 1. LangGraph Agent êµ¬ì¡°

```mermaid
graph LR
    START([ğŸ”¸ ì‹œì‘]) --> Router{ë¼ìš°í„°<br/>ë…¸ë“œ}

    Router -->|ì¼ë°˜ ì§ˆë¬¸| General[ì¼ë°˜ ë‹µë³€]
    Router -->|ë…¼ë¬¸ ê²€ìƒ‰| RAG[RAG ê²€ìƒ‰]
    Router -->|ì›¹ ê²€ìƒ‰| Web[ì›¹ ê²€ìƒ‰]
    Router -->|ìš©ì–´ ì§ˆë¬¸| Glossary[ìš©ì–´ì§‘]
    Router -->|ìš”ì•½ ìš”ì²­| Summarize[ë…¼ë¬¸ ìš”ì•½]
    Router -->|ì €ì¥ ìš”ì²­| Save[íŒŒì¼ ì €ì¥]

    General --> END([âœ… ì¢…ë£Œ])
    RAG --> END
    Web --> END
    Glossary --> END
    Summarize --> END
    Save --> END

    style START fill:#81c784,stroke:#388e3c,color:#000
    style END fill:#66bb6a,stroke:#2e7d32,color:#000
    style Router fill:#ba68c8,stroke:#7b1fa2,color:#000
    style General fill:#ce93d8,stroke:#7b1fa2,color:#000
    style RAG fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Web fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Glossary fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Summarize fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Save fill:#ce93d8,stroke:#7b1fa2,color:#000
```

### 2. LLM ì„ íƒ ì „ëµ

```mermaid
graph TB
    A[ì‘ì—… ìœ í˜•] --> B{ì‘ì—… ë¶„ë¥˜}

    B -->|ë¼ìš°íŒ…| C[Solar<br/>ë¹ ë¥¸ ì‘ë‹µ]
    B -->|ë‹µë³€ ìƒì„±| D[GPT-4<br/>ë†’ì€ ì •í™•ë„]
    B -->|ìš”ì•½| E[GPT-4<br/>í’ˆì§ˆ ì¤‘ìš”]
    B -->|ê¸°íƒ€| F[GPT-3.5<br/>ë¹„ìš© íš¨ìœ¨]

    C --> G[LLM í˜¸ì¶œ]
    D --> G
    E --> G
    F --> G

    G --> H{ì—ëŸ¬?}
    H -->|Yes| I[ì¬ì‹œë„<br/>ìµœëŒ€ 3íšŒ]
    H -->|No| J[âœ… ê²°ê³¼ ë°˜í™˜]
    I --> G

    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#ba68c8,stroke:#7b1fa2,color:#000
    style C fill:#ce93d8,stroke:#7b1fa2,color:#000
    style D fill:#ce93d8,stroke:#7b1fa2,color:#000
    style E fill:#ce93d8,stroke:#7b1fa2,color:#000
    style F fill:#ce93d8,stroke:#7b1fa2,color:#000
    style G fill:#a5d6a7,stroke:#388e3c,color:#000
    style H fill:#ba68c8,stroke:#7b1fa2
    style I fill:#ffcc80,stroke:#f57c00,color:#000
    style J fill:#66bb6a,stroke:#2e7d32,color:#000
```

### 3. ì—ëŸ¬ í•¸ë“¤ë§ íë¦„

```mermaid
sequenceDiagram
    autonumber
    participant Agent
    participant LLM
    participant Retry

    Agent->>LLM: API í˜¸ì¶œ

    alt ì„±ê³µ
        LLM-->>Agent: âœ… ì‘ë‹µ ë°˜í™˜
    else ì‹¤íŒ¨ (1ì°¨)
        LLM-->>Retry: âŒ ì—ëŸ¬
        Retry->>Retry: ëŒ€ê¸° 2ì´ˆ
        Retry->>LLM: ì¬ì‹œë„ (1/3)

        alt ì„±ê³µ
            LLM-->>Agent: âœ… ì‘ë‹µ ë°˜í™˜
        else ì‹¤íŒ¨ (2ì°¨)
            LLM-->>Retry: âŒ ì—ëŸ¬
            Retry->>Retry: ëŒ€ê¸° 4ì´ˆ
            Retry->>LLM: ì¬ì‹œë„ (2/3)

            alt ì„±ê³µ
                LLM-->>Agent: âœ… ì‘ë‹µ ë°˜í™˜
            else ì‹¤íŒ¨ (3ì°¨)
                LLM-->>Retry: âŒ ì—ëŸ¬
                Retry->>Retry: ëŒ€ê¸° 8ì´ˆ
                Retry->>LLM: ì¬ì‹œë„ (3/3)

                alt ì„±ê³µ
                    LLM-->>Agent: âœ… ì‘ë‹µ ë°˜í™˜
                else ìµœì¢… ì‹¤íŒ¨
                    LLM-->>Agent: âŒ ì—ëŸ¬ ë°˜í™˜
                end
            end
        end
    end
```

---

## LangGraph Agent ê·¸ë˜í”„ êµ¬í˜„

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `src/agent/state.py`, `src/agent/graph.py`

### 1. State ì •ì˜ (`src/agent/state.py`)
- TypedDictë¥¼ ìƒì†í•œ AgentState í´ë˜ìŠ¤ ì •ì˜
- í•„ìˆ˜ í•„ë“œ:
  - question (str): ì‚¬ìš©ì ì§ˆë¬¸
  - difficulty (str): ë‚œì´ë„ (easy/hard)
  - tool_choice (str): ì„ íƒëœ ë„êµ¬
  - tool_result (str): ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
  - final_answer (str): ìµœì¢… ë‹µë³€
  - messages: Annotated[Sequence[BaseMessage], operator.add] - ëŒ€í™” íˆìŠ¤í† ë¦¬

### 2. ê·¸ë˜í”„ êµ¬ì„± (`src/agent/graph.py`)
- `create_agent_graph` í•¨ìˆ˜ ìƒì„±
- StateGraph(AgentState) ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- ë…¸ë“œ ì¶”ê°€:
  - workflow.add_node("router", router_node)
  - workflow.add_node("general", general_answer_node)
  - workflow.add_node("search_paper", search_paper_node)
  - workflow.add_node("web_search", web_search_node)
  - workflow.add_node("search_glossary", glossary_node)
  - workflow.add_node("summarize_paper", summarize_node)
  - workflow.add_node("save_file", save_file_node)
- ì‹œì‘ì  ì„¤ì •: workflow.set_entry_point("router")
- ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •: add_conditional_edgesë¡œ ë¼ìš°í„°ì—ì„œ ê° ë„êµ¬ë¡œ ë¶„ê¸°
- ëª¨ë“  ë„êµ¬ ë…¸ë“œì—ì„œ ENDë¡œ ì—°ê²°
- workflow.compile()ë¡œ ê·¸ë˜í”„ ì»´íŒŒì¼ í›„ ë°˜í™˜

### 3. ë¼ìš°í„° ë…¸ë“œ êµ¬í˜„ (`src/agent/nodes.py`)
- `router_node` í•¨ìˆ˜ ì •ì˜
- ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ ì„ íƒ
- ë„êµ¬ ëª©ë¡ê³¼ ê° ë„êµ¬ì˜ ì‚¬ìš© ì¼€ì´ìŠ¤ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
- LLMì—ê²Œ í”„ë¡¬í”„íŠ¸ ì „ë‹¬í•˜ì—¬ ë„êµ¬ ì´ë¦„ ë°˜í™˜ë°›ê¸°
- ë°˜í™˜ëœ ë„êµ¬ ì´ë¦„ì„ state["tool_choice"]ì— ì €ì¥
- ë¼ìš°íŒ… ê²°ì • ë¡œê·¸ ì¶œë ¥

### 4. ë¼ìš°íŒ… í•¨ìˆ˜ (`src/agent/graph.py`)
- `route_to_tool` í•¨ìˆ˜: state["tool_choice"] ê°’ì„ ë°˜í™˜
- add_conditional_edgesì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •

### ì˜ˆì œ ì½”ë“œ

```python
# src/agent/graph.py

import os
from datetime import datetime
from langgraph.graph import StateGraph, END
from typing import TypedDict
from langchain_openai import ChatOpenAI
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "agent_router"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class AgentState(TypedDict):
    question: str
    difficulty: str
    tool_choice: str
    tool_result: str
    final_answer: str

def router_node(state: AgentState):
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •
    """
    question = state["question"]

    logger.write(f"ë¼ìš°í„° ë…¸ë“œ ì‹¤í–‰: {question}")

    # LLMì—ê²Œ ë¼ìš°íŒ… ê²°ì • ìš”ì²­
    routing_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:

ë„êµ¬ ëª©ë¡:
- search_paper: ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰
- web_search: ì›¹ì—ì„œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰
- glossary: ìš©ì–´ ì •ì˜ ê²€ìƒ‰
- summarize: ë…¼ë¬¸ ìš”ì•½
- save_file: íŒŒì¼ ì €ì¥
- general: ì¼ë°˜ ë‹µë³€

ì§ˆë¬¸: {question}

í•˜ë‚˜ì˜ ë„êµ¬ ì´ë¦„ë§Œ ë°˜í™˜í•˜ì„¸ìš”:
    """

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    tool_choice = llm.invoke(routing_prompt).content.strip()

    logger.write(f"ë¼ìš°íŒ… ê²°ì •: {tool_choice}")

    state["tool_choice"] = tool_choice
    return state

def route_to_tool(state: AgentState):
    """ë¼ìš°íŒ… ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œ ì„ íƒ"""
    return state["tool_choice"]

def create_agent_graph():
    """LangGraph Agent ê·¸ë˜í”„ ìƒì„±"""
    logger.write("Agent ê·¸ë˜í”„ ìƒì„± ì‹œì‘")

    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("router", router_node)
    workflow.add_node("search_paper", search_paper_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("glossary", glossary_node)
    workflow.add_node("summarize", summarize_node)
    workflow.add_node("save_file", save_file_node)
    workflow.add_node("general", general_answer_node)

    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("router")

    # ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •
    workflow.add_conditional_edges(
        "router",
        route_to_tool,
        {
            "search_paper": "search_paper",
            "web_search": "web_search",
            "glossary": "glossary",
            "summarize": "summarize",
            "save_file": "save_file",
            "general": "general"
        }
    )

    # ëª¨ë“  ë…¸ë“œì—ì„œ ì¢…ë£Œ
    for node in ["search_paper", "web_search", "glossary", "summarize", "save_file", "general"]:
        workflow.add_edge(node, END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    agent_executor = workflow.compile()

    logger.write("Agent ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ")

    return agent_executor
```

---

## LLM í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `src/llm/client.py`

### 1. ë‹¤ì¤‘ LLM í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
- `LLMClient` í´ë˜ìŠ¤ ì •ì˜
- __init__ ë©”ì„œë“œ:
  - provider íŒŒë¼ë¯¸í„°ë¡œ "openai" ë˜ëŠ” "solar" ì„ íƒ
  - providerì— ë”°ë¼ ChatOpenAI ë˜ëŠ” ChatUpstage ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  - í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ (OPENAI_API_KEY, UPSTAGE_API_KEY)
  - streaming=True ì„¤ì •

### 2. ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
- tenacity ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ @retry ë°ì½”ë ˆì´í„° ì‚¬ìš©
- `invoke_with_retry` ë©”ì„œë“œ:
  - stop_after_attempt(3): ìµœëŒ€ 3íšŒ ì¬ì‹œë„
  - wait_exponential: ì§€ìˆ˜ ë°±ì˜¤í”„ (2ì´ˆ â†’ 4ì´ˆ â†’ 8ì´ˆ)
  - LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„

### 3. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- `invoke_with_tracking` ë©”ì„œë“œ êµ¬í˜„
- OpenAI ì‚¬ìš© ì‹œ: get_openai_callbackìœ¼ë¡œ í† í° ìˆ˜ì™€ ë¹„ìš© ì¶”ì 
- Solar ì‚¬ìš© ì‹œ: ê¸°ë³¸ ë¡œê·¸ë§Œ ì¶œë ¥
- ê° í˜¸ì¶œë§ˆë‹¤ í† í° ì •ë³´ ì¶œë ¥

### 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
- `astream` ë¹„ë™ê¸° ë©”ì„œë“œ êµ¬í˜„
- async for ë£¨í”„ë¡œ LLM ì‘ë‹µì„ ì²­í¬ ë‹¨ìœ„ë¡œ yield
- Streamlit UIì—ì„œ ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œì— ì‚¬ìš©

### 5. LLM ì„ íƒ ì „ëµ
- `get_llm_for_task` í•¨ìˆ˜ êµ¬í˜„
- ì‘ì—… ìœ í˜•ë³„ ìµœì  LLM ì„ íƒ:
  - routing: Solar (ë¹ ë¥¸ ì‘ë‹µ)
  - generation: GPT-4 (ë†’ì€ ì •í™•ë„)
  - summarization: GPT-4 (í’ˆì§ˆ ì¤‘ìš”)
  - ê¸°ë³¸ê°’: GPT-3.5-turbo (ë¹„ìš© íš¨ìœ¨)

### ì˜ˆì œ ì½”ë“œ

```python
# src/llm/client.py

import os
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_upstage import ChatUpstage
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain.callbacks import get_openai_callback
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "agent_llm"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class LLMClient:
    """ë‹¤ì¤‘ LLM í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, provider="openai", model="gpt-3.5-turbo", temperature=0.7):
        """
        Args:
            provider: "openai" ë˜ëŠ” "solar"
            model: ëª¨ë¸ ì´ë¦„
            temperature: ì°½ì˜ì„± ìˆ˜ì¤€ (0-1)
        """
        self.provider = provider

        logger.write(f"LLM ì´ˆê¸°í™”: provider={provider}, model={model}")

        if provider == "openai":
            self.llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                streaming=True
            )
        elif provider == "solar":
            self.llm = ChatUpstage(
                model="solar-1-mini-chat",
                temperature=temperature,
                api_key=os.getenv("UPSTAGE_API_KEY"),
                streaming=True
            )

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=2, max=8))
    def invoke_with_retry(self, messages):
        """
        ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
        ìµœëŒ€ 3íšŒ ì¬ì‹œë„, ì§€ìˆ˜ ë°±ì˜¤í”„ (2ì´ˆ â†’ 4ì´ˆ â†’ 8ì´ˆ)
        """
        logger.write("LLM í˜¸ì¶œ ì‹œì‘ (ì¬ì‹œë„ ê°€ëŠ¥)")
        return self.llm.invoke(messages)

    def invoke_with_tracking(self, messages):
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
        if self.provider == "openai":
            with get_openai_callback() as cb:
                response = self.llm.invoke(messages)
                logger.write(f"Tokens Used: {cb.total_tokens}")
                logger.write(f"Total Cost: ${cb.total_cost:.4f}")
                return response
        else:
            return self.llm.invoke(messages)

    async def astream(self, messages):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        logger.write("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘")
        async for chunk in self.llm.astream(messages):
            yield chunk


def get_llm_for_task(task_type):
    """ì‘ì—… ìœ í˜•ë³„ ìµœì  LLM ì„ íƒ"""
    logger.write(f"ì‘ì—… ìœ í˜•ë³„ LLM ì„ íƒ: {task_type}")

    if task_type == "routing":
        return LLMClient(provider="solar", model="solar-1-mini-chat", temperature=0)
    elif task_type == "generation":
        return LLMClient(provider="openai", model="gpt-4", temperature=0.7)
    elif task_type == "summarization":
        return LLMClient(provider="openai", model="gpt-4", temperature=0)
    else:
        return LLMClient(provider="openai", model="gpt-3.5-turbo", temperature=0.7)
```

---

## ëŒ€í™” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `src/memory/chat_history.py`

### 1. ChatMemoryManager í´ë˜ìŠ¤
- ConversationBufferMemory ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  - return_messages=True: ë©”ì‹œì§€ ê°ì²´ í˜•íƒœë¡œ ë°˜í™˜
  - memory_key="chat_history": ë©”ëª¨ë¦¬ í‚¤ ì„¤ì •
- `add_user_message`: ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
- `add_ai_message`: AI ë©”ì‹œì§€ ì¶”ê°€
- `get_history`: ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜
- `clear`: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”

### 2. ì„¸ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ (ì„ íƒì‚¬í•­)
- PostgresChatMessageHistory ì‚¬ìš©
- `get_session_history` í•¨ìˆ˜:
  - session_idë¡œ íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
  - PostgreSQLì— ëŒ€í™” ë‚´ìš© ì˜êµ¬ ì €ì¥
  - ì—¬ëŸ¬ ì‚¬ìš©ì ì„¸ì…˜ ê´€ë¦¬ ê°€ëŠ¥

### 3. Agentì™€ ë©”ëª¨ë¦¬ í†µí•©
- Agent ì‹¤í–‰ ì‹œ messages í•„ë“œì— ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ ì „ë‹¬
- ì‘ë‹µ ìƒì„± í›„ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ AI ë©”ì‹œì§€ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
- ì´í›„ ì§ˆë¬¸ì—ì„œ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™œìš©

### ì˜ˆì œ ì½”ë“œ

```python
# src/memory/chat_history.py

from langchain.memory import ConversationBufferMemory
from langchain_postgres import PostgresChatMessageHistory
import os

class ChatMemoryManager:
    """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        """ConversationBufferMemory ì´ˆê¸°í™”"""
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="chat_history"
        )

    def add_user_message(self, message: str):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€"""
        self.memory.chat_memory.add_user_message(message)

    def add_ai_message(self, message: str):
        """AI ë©”ì‹œì§€ ì¶”ê°€"""
        self.memory.chat_memory.add_ai_message(message)

    def get_history(self):
        """ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.memory.load_memory_variables({})

    def clear(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear()


def get_session_history(session_id: str):
    """
    ì„¸ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ (PostgreSQL ì €ì¥)

    Args:
        session_id: ì„¸ì…˜ ID

    Returns:
        PostgresChatMessageHistory ì¸ìŠ¤í„´ìŠ¤
    """
    connection_string = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/papers")

    return PostgresChatMessageHistory(
        session_id=session_id,
        connection_string=connection_string,
        table_name="chat_history"
    )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
    memory_manager = ChatMemoryManager()

    memory_manager.add_user_message("Transformer ë…¼ë¬¸ ì„¤ëª…í•´ì¤˜")
    memory_manager.add_ai_message("TransformerëŠ” 2017ë…„ Googleì—ì„œ ë°œí‘œí•œ...")

    logger.write(f"ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬: {memory_manager.get_history()}")

    # ì„¸ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ì‚¬ìš©
    session_history = get_session_history("user_123")
    session_history.add_user_message("BERT ë…¼ë¬¸ì€?")
    session_history.add_ai_message("BERTëŠ” 2018ë…„ì—...")

    logger.write(f"ì„¸ì…˜ ë©”ì‹œì§€: {session_history.messages}")
    logger.close()
```

---

## ë¡œê¹… ë° ì‹¤í—˜ ì¶”ì  ê´€ë¦¬

### ë¡œê¹… ì‹œìŠ¤í…œ ì‚¬ìš©

**ì¤‘ìš”**: ëª¨ë“  ì¶œë ¥ì€ Logger í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**íŒŒì¼ ê²½ë¡œ**: `src/utils/logger.py`

**ì‚¬ìš© ë°©ë²•**:
1. ì‹¤í—˜ í´ë” ë° Logger ìƒì„±
   ```python
   today = datetime.now().strftime("%Y%m%d")
   time_now = datetime.now().strftime("%H%M%S")
   experiment_name = "agent_main"  # agent_xxx, rag_xxx, feature_xxx
   log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
   os.makedirs(log_dir, exist_ok=True)
   logger = Logger(log_path=f"{log_dir}/experiment.log")
   ```

2. ë¡œê·¸ ê¸°ë¡
   - `logger.write()` ì‚¬ìš© (print() ëŒ€ì‹ )
   - ì˜ˆ: `logger.write(f"ë¼ìš°íŒ… ê²°ì •: {tool_choice}")`

3. ì‹¤í—˜ ì¢…ë£Œ
   - `logger.close()` í•„ìˆ˜ í˜¸ì¶œ

### ì‹¤í—˜ í´ë” êµ¬ì¡°

**ê·œì¹™**: PRD ë¬¸ì„œ 06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md ì°¸ì¡°

```
experiments/
â”œâ”€â”€ {ë‚ ì§œ}/
â”‚   â”œâ”€â”€ {ë‚ ì§œ}_{ì‹œê°„}_{ì‹¤í—˜ëª…}/
â”‚   â”‚   â”œâ”€â”€ experiment.log         # ì‹¤í—˜ ë¡œê·¸
â”‚   â”‚   â”œâ”€â”€ config.yaml            # ì„¤ì • íŒŒì¼
â”‚   â”‚   â””â”€â”€ results.json           # ê²°ê³¼ íŒŒì¼
```

**í•„ìˆ˜ íŒŒì¼**:
- `experiment.log`: logger.write() ì¶œë ¥
- `config.yaml`: LLM ì„¤ì •, ë‚œì´ë„, ëª¨ë¸ ì •ë³´
- `results.json`: ìµœì¢… ë‹µë³€, ë„êµ¬ ì„ íƒ, ì‘ë‹µ ì‹œê°„

### ì˜ˆì œ ì½”ë“œ

```python
import os
from datetime import datetime
from src.utils.logger import Logger
import yaml
import json

# ì‹¤í—˜ í´ë” ìƒì„±
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "agent_main"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)

# Logger ì´ˆê¸°í™”
logger = Logger(log_path=f"{log_dir}/experiment.log")

# Config ì €ì¥
config = {
    "llm_provider": "openai",
    "model": "gpt-4",
    "temperature": 0.7,
    "difficulty": "easy"
}

with open(f"{log_dir}/config.yaml", "w") as f:
    yaml.dump(config, f)

# ì‹¤í–‰ ë¡œê·¸
logger.write("Agent ì‹¤í–‰ ì‹œì‘")
logger.write(f"ì§ˆë¬¸: {question}")
logger.write(f"ì„ íƒëœ ë„êµ¬: {tool_choice}")

# Results ì €ì¥
results = {
    "question": question,
    "tool_choice": tool_choice,
    "final_answer": final_answer,
    "response_time_ms": 1250,
    "timestamp": datetime.now().isoformat()
}

with open(f"{log_dir}/results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# Logger ì¢…ë£Œ
logger.close()
```

---

## ê°œë°œ ì¼ì •

### Phase 1: LLM í´ë¼ì´ì–¸íŠ¸ ë° ê³µí†µ ì¸í”„ë¼
- ChatOpenAI ë˜í¼ êµ¬í˜„
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬

### Phase 2: LangGraph Agent ê·¸ë˜í”„
- State ì •ì˜
- ë¼ìš°í„° ë…¸ë“œ êµ¬í˜„
- ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •
- ì¼ë°˜ ë‹µë³€ ë…¸ë“œ êµ¬í˜„

### Phase 3: ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
- ConversationBufferMemory êµ¬í˜„
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- ì„¸ì…˜ ê´€ë¦¬

### Phase 4: ë…¼ë¬¸ ìš”ì•½ ë„êµ¬
- load_summarize_chain êµ¬í˜„
- ë‚œì´ë„ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ê³„
- ìš”ì•½ ë°©ì‹ ì„ íƒ ë¡œì§

### Phase 5: í†µí•© ì‘ì—…
- main.py ì‘ì„±
- ëª¨ë“  ëª¨ë“ˆ í†µí•©
- ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸

### Phase 6: ë°œí‘œ ì¤€ë¹„
- ë°œí‘œ ìë£Œ ì‘ì„±
- README.md ì‘ì„±
- ìµœì¢… ì ê²€

---

## main.py êµ¬í˜„

### êµ¬í˜„ ë°©ë²•

**íŒŒì¼ ê²½ë¡œ**: `main.py` (í”„ë¡œì íŠ¸ ë£¨íŠ¸)

1. **í•„ìš”í•œ ëª¨ë“ˆ import**
   - src.agent.graphì—ì„œ create_agent_graph
   - src.llm.clientì—ì„œ LLMClient
   - src.memory.chat_historyì—ì„œ ChatMemoryManager

2. **ì´ˆê¸°í™”**
   - LLMClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (model="gpt-4", temperature=0.7)
   - create_agent_graph()ë¡œ Agent ìƒì„±
   - ChatMemoryManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

3. **Agent ì‹¤í–‰ ë£¨í”„**
   - í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (ì§ˆë¬¸, ë‚œì´ë„ íŠœí”Œ)
   - ê° ì§ˆë¬¸ì— ëŒ€í•´:
     - agent.invoke()ë¡œ ì‹¤í–‰ (question, difficulty, messages ì „ë‹¬)
     - ê²°ê³¼ì—ì„œ final_answer ì¶”ì¶œ
     - memory_managerì— ì‚¬ìš©ì ë©”ì‹œì§€ì™€ AI ë©”ì‹œì§€ ì¶”ê°€
     - ê²°ê³¼ ì¶œë ¥

4. **ì‹¤í–‰**
   - if __name__ == "__main__": main() ì¶”ê°€
   - ì»¤ë§¨ë“œë¼ì¸ì—ì„œ python main.pyë¡œ ì‹¤í–‰

---

## Feature ë¸Œëœì¹˜

**3ë‹¨ê³„: AI Agent ë©”ì¸ êµ¬í˜„ (ìµœí˜„í™”)**
- `3-1. feature/llm-client` - LLM í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
- `3-2. feature/memory` - ëŒ€í™” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
- `3-3. feature/agent-graph` - LangGraph ê·¸ë˜í”„ êµ¬í˜„
- `3-4. feature/tool-summarize` - ë…¼ë¬¸ ìš”ì•½ ë„êµ¬
- `3-5. feature/integration` - í†µí•© ë° main.py

---

## ì°¸ê³  PRD ë¬¸ì„œ

ê°œë°œ ì‹œ ë°˜ë“œì‹œ ì°¸ê³ í•´ì•¼ í•  PRD ë¬¸ì„œ ëª©ë¡:

### í•„ìˆ˜ ì°¸ê³  ë¬¸ì„œ
1. [01_í”„ë¡œì íŠ¸_ê°œìš”.md](../PRD/01_í”„ë¡œì íŠ¸_ê°œìš”.md) - í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš” ë° ëª©í‘œ
2. [02_í”„ë¡œì íŠ¸_êµ¬ì¡°.md](../PRD/02_í”„ë¡œì íŠ¸_êµ¬ì¡°.md) - í´ë” êµ¬ì¡° ë° ëª¨ë“ˆ ë°°ì¹˜
3. [05_ë¡œê¹…_ì‹œìŠ¤í…œ.md](../PRD/05_ë¡œê¹…_ì‹œìŠ¤í…œ.md) â­ - Logger í´ë˜ìŠ¤ ì‚¬ìš©ë²• ë° ê·œì¹™
4. [06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md](../PRD/06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md) â­ - ì‹¤í—˜ í´ë” êµ¬ì¡° ë° ëª…ëª… ê·œì¹™
5. [10_ê¸°ìˆ _ìš”êµ¬ì‚¬í•­.md](../PRD/10_ê¸°ìˆ _ìš”êµ¬ì‚¬í•­.md) - ê¸°ìˆ  ìŠ¤íƒ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
6. [12_AI_Agent_ì„¤ê³„.md](../PRD/12_AI_Agent_ì„¤ê³„.md) - LangGraph êµ¬ì¡° ë° ë„êµ¬ ì •ì˜
7. [14_LLM_ì„¤ì •.md](../PRD/14_LLM_ì„¤ì •.md) - LLM ì„ íƒ ì „ëµ ë° ì—ëŸ¬ í•¸ë“¤ë§

### ì°¸ê³  ë¬¸ì„œ
- [03_ë¸Œëœì¹˜_ì „ëµ.md](../PRD/03_ë¸Œëœì¹˜_ì „ëµ.md) - Feature ë¸Œëœì¹˜ ì „ëµ
- [04_ì¼ì •_ê´€ë¦¬.md](../PRD/04_ì¼ì •_ê´€ë¦¬.md) - ê°œë°œ ì¼ì • ë° ë§ˆì¼ìŠ¤í†¤
- [11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md](../PRD/11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md) - DB ìŠ¤í‚¤ë§ˆ (ìš”ì•½ ë„êµ¬ì—ì„œ ì‚¬ìš©)

---

## ì°¸ê³  ìë£Œ

- LangGraph ê³µì‹ ë¬¸ì„œ: https://langchain-ai.github.io/langgraph/
- Langchain ChatOpenAI: https://python.langchain.com/docs/integrations/chat/openai/
- Langchain Memory: https://python.langchain.com/docs/modules/memory/
- Langchain Summarization: https://python.langchain.com/docs/use_cases/summarization/
- Langchain Callbacks: https://python.langchain.com/docs/modules/callbacks/
