# ì¶”ê°€ ì„ íƒ ê¸°ëŠ¥ (ë¨¼ì € ê°œë°œ ì™„ë£Œí•œ íŒ€ì›ì´ ì„ íƒ)

## ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2025-10-30
- **ëª©ì **: í•µì‹¬ ê¸°ëŠ¥ ê°œë°œì„ ë¨¼ì € ì™„ë£Œí•œ íŒ€ì›ì´ ì¶”ê°€ë¡œ ì„ íƒí•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ëª©ë¡
- **ìš°ì„ ìˆœìœ„**: ì„ íƒ ì‚¬í•­ (ì‹œê°„ì´ ìˆì„ ê²½ìš°)

---

## ê°œìš”

í•µì‹¬ ê¸°ëŠ¥(AI Agent, RAG, ìš©ì–´ì§‘, ì›¹ ê²€ìƒ‰, íŒŒì¼ ì €ì¥, UI)ì„ ë¨¼ì € ì™„ë£Œí•œ íŒ€ì›ì´ ì•„ë˜ ê¸°ëŠ¥ ì¤‘ ì„ íƒí•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì„ íƒ ê¸°ì¤€:**
1. í”„ë¡œì íŠ¸ì— ê°€ì‚°ì ì„ ì¤„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥
2. êµ¬í˜„ ë‚œì´ë„ì™€ ì‹œê°„ì„ ê³ ë ¤
3. íŒ€ì›ì˜ ê´€ì‹¬ì‚¬ ë° ê°•ì 

---

## ì„ íƒ ê¸°ëŠ¥ 1: Text-to-SQL ë„êµ¬ (â­â­â­ ê°€ì‚°ì )

### ê¸°ëŠ¥ ì„¤ëª…
ìì—°ì–´ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ë…¼ë¬¸ í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ë„êµ¬

### ì‚¬ìš© ì‹œì 
- "2024ë…„ì— ë°œí‘œëœ ë…¼ë¬¸ ê°œìˆ˜ëŠ”?"
- "ê°€ì¥ ë§ì´ ì¸ìš©ëœ ë…¼ë¬¸ Top 5ëŠ”?"
- "ì €ìë³„ ë…¼ë¬¸ ìˆ˜ ì•Œë ¤ì¤˜"
- "ì¹´í…Œê³ ë¦¬ë³„ ë…¼ë¬¸ ë¶„í¬ëŠ”?"

### Langchain êµ¬í˜„

#### 1. SQLDatabaseChain ì‚¬ìš©
```python
# src/tools/text2sql.py

from langchain.tools import tool
from langchain.chains import SQLDatabaseChain
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI

# PostgreSQL DB ì—°ê²°
db = SQLDatabase.from_uri("postgresql://user:password@localhost/papers")

# SQLDatabaseChain ìƒì„±
sql_chain = SQLDatabaseChain.from_llm(
    llm=ChatOpenAI(model="gpt-4", temperature=0),
    db=db,
    verbose=True
)

@tool
def query_paper_statistics(question: str) -> str:
    """
    ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        question: ìì—°ì–´ ì§ˆë¬¸

    Returns:
        ì¿¼ë¦¬ ê²°ê³¼
    """
    # SQLDatabaseChain ì‹¤í–‰
    result = sql_chain.run(question)

    return f"## ğŸ“Š í†µê³„ ê²°ê³¼\n\n{result}"
```

#### 2. ì»¤ìŠ¤í…€ SQL ìƒì„± (ê³ ê¸‰)
```python
from langchain.prompts import PromptTemplate

# SQL ìƒì„± í”„ë¡¬í”„íŠ¸
SQL_GENERATION_PROMPT = PromptTemplate(
    template="""
    ë‹¤ìŒ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

    ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ:
    - papers (paper_id, title, authors, publish_date, category, citation_count)
    - glossary (term_id, term, definition, category)

    ì§ˆë¬¸: {question}

    SQL ì¿¼ë¦¬ (SELECTë¬¸ë§Œ):
    """,
    input_variables=["question"]
)

@tool
def custom_sql_query(question: str) -> str:
    """ì»¤ìŠ¤í…€ SQL ìƒì„± ë° ì‹¤í–‰"""
    # LLMìœ¼ë¡œ SQL ìƒì„±
    sql_query = llm.invoke(SQL_GENERATION_PROMPT.format(question=question)).content

    # SQL ì‹¤í–‰
    import psycopg2
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    try:
        cursor.execute(sql_query)
        results = cursor.fetchall()

        # ê²°ê³¼ í¬ë§·íŒ…
        output = f"## ğŸ“Š ì¿¼ë¦¬ ê²°ê³¼\n\n**SQL:**\n```sql\n{sql_query}\n```\n\n**ê²°ê³¼:**\n"

        for row in results:
            output += f"- {row}\n"

        return output

    except Exception as e:
        return f"SQL ì‹¤í–‰ ì˜¤ë¥˜: {e}"

    finally:
        cursor.close()
        conn.close()
```

### êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥
1. SQLDatabaseChain ì„¤ì •
2. DB ìŠ¤í‚¤ë§ˆ ì •ë³´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
3. SQL ì¿¼ë¦¬ ìƒì„±
4. ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ í¬ë§·íŒ…
5. ì˜¤ë¥˜ ì²˜ë¦¬ (ì˜ëª»ëœ SQL ìƒì„± ì‹œ)

### ì‚¬ìš©í•˜ëŠ” DB
- **PostgreSQL**: `papers`, `glossary` í…Œì´ë¸”

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**2-3ì¼** (11/02~11/04)

### ì°¸ê³  ìë£Œ
- Langchain SQL Database: https://python.langchain.com/docs/integrations/tools/sql_database
- SQLDatabaseChain: https://python.langchain.com/docs/use_cases/sql/

---

## ì„ íƒ ê¸°ëŠ¥ 2: ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ (â­â­ ê°€ì‚°ì )

### ê¸°ëŠ¥ ì„¤ëª…
ì±—ë´‡ì˜ ë‹µë³€ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ

### í‰ê°€ í•­ëª©
1. **ë‹µë³€ ì •í™•ë„**: ë…¼ë¬¸ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€
2. **ê´€ë ¨ì„±**: ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ê´€ë ¨ìˆëŠ”ì§€
3. **ë‚œì´ë„ ì í•©ì„±**: Easy/Hard ëª¨ë“œì— ë§ëŠ” ë‹µë³€ì¸ì§€
4. **ì¶œì²˜ ëª…ì‹œ**: ë…¼ë¬¸ ì œëª©, ì €ìë¥¼ ëª…ì‹œí–ˆëŠ”ì§€

### Langchain êµ¬í˜„

#### 1. LLM-as-a-Judge í‰ê°€
```python
# src/evaluation/evaluator.py

from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

EVALUATION_PROMPT = PromptTemplate(
    template="""
    ë‹¤ìŒ ì±—ë´‡ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {question}
    ë‹µë³€: {answer}
    ì°¸ê³  ë¬¸ì„œ: {reference_docs}
    ë‚œì´ë„: {difficulty}

    í‰ê°€ ê¸°ì¤€:
    1. ì •í™•ë„ (0-10ì ): ì°¸ê³  ë¬¸ì„œì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
    2. ê´€ë ¨ì„± (0-10ì ): ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ê°€?
    3. ë‚œì´ë„ ì í•©ì„± (0-10ì ): {difficulty} ëª¨ë“œì— ì í•©í•œê°€?
    4. ì¶œì²˜ ëª…ì‹œ (0-10ì ): ë…¼ë¬¸ ì •ë³´ë¥¼ ëª…ì‹œí–ˆëŠ”ê°€?

    í‰ê°€ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
    {{
        "ì •í™•ë„": ì ìˆ˜,
        "ê´€ë ¨ì„±": ì ìˆ˜,
        "ë‚œì´ë„_ì í•©ì„±": ì ìˆ˜,
        "ì¶œì²˜_ëª…ì‹œ": ì ìˆ˜,
        "ì´ì ": ì´ì ,
        "ì½”ë©˜íŠ¸": "í‰ê°€ ì½”ë©˜íŠ¸"
    }}
    """,
    input_variables=["question", "answer", "reference_docs", "difficulty"]
)

class AnswerEvaluator:
    """ë‹µë³€ í‰ê°€ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)

    def evaluate(self, question: str, answer: str, reference_docs: str, difficulty: str):
        """ë‹µë³€ í‰ê°€"""
        prompt = EVALUATION_PROMPT.format(
            question=question,
            answer=answer,
            reference_docs=reference_docs,
            difficulty=difficulty
        )

        result = self.llm.invoke(prompt).content

        # JSON íŒŒì‹±
        import json
        evaluation = json.loads(result)

        return evaluation
```

#### 2. í‰ê°€ ê²°ê³¼ ì €ì¥
```python
def save_evaluation_results(evaluations: list):
    """í‰ê°€ ê²°ê³¼ë¥¼ PostgreSQLì— ì €ì¥"""
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    # í‰ê°€ ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS evaluation_results (
            eval_id SERIAL PRIMARY KEY,
            question TEXT,
            answer TEXT,
            accuracy_score INT,
            relevance_score INT,
            difficulty_score INT,
            citation_score INT,
            total_score INT,
            comment TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)

    # í‰ê°€ ê²°ê³¼ ì‚½ì…
    for eval_result in evaluations:
        cursor.execute("""
            INSERT INTO evaluation_results (
                question, answer, accuracy_score, relevance_score,
                difficulty_score, citation_score, total_score, comment
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            eval_result['question'],
            eval_result['answer'],
            eval_result['ì •í™•ë„'],
            eval_result['ê´€ë ¨ì„±'],
            eval_result['ë‚œì´ë„_ì í•©ì„±'],
            eval_result['ì¶œì²˜_ëª…ì‹œ'],
            eval_result['ì´ì '],
            eval_result['ì½”ë©˜íŠ¸']
        ))

    conn.commit()
    cursor.close()
    conn.close()
```

### êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥
1. LLM-as-a-Judge í”„ë¡¬í”„íŠ¸ ì„¤ê³„
2. í‰ê°€ ê²°ê³¼ JSON íŒŒì‹±
3. PostgreSQL í‰ê°€ ê²°ê³¼ í…Œì´ë¸” ìƒì„±
4. í‰ê°€ ê²°ê³¼ ì €ì¥ ë° ì¡°íšŒ
5. Streamlit UIì— í‰ê°€ ê²°ê³¼ í‘œì‹œ

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**2-3ì¼** (11/02~11/04)

### ì°¸ê³  ìë£Œ
- LangChain Evaluation: https://python.langchain.com/docs/guides/evaluation/

---

## ì„ íƒ ê¸°ëŠ¥ 3: Reranking (ê²€ìƒ‰ ìµœì í™”) (â­â­)

### ê¸°ëŠ¥ ì„¤ëª…
RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ìˆœìœ„í™”í•˜ì—¬ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ìƒìœ„ì— ë°°ì¹˜

### Langchain êµ¬í˜„

#### 1. Cohere Rerank ì‚¬ìš©
```python
# src/rag/reranking.py

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank
from langchain_postgres.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings
import os

def create_rerank_retriever(base_retriever):
    """Rerankingì„ í¬í•¨í•œ Retriever ìƒì„±"""
    # Cohere Rerank ì••ì¶•ê¸°
    compressor = CohereRerank(
        model="rerank-english-v2.0",
        cohere_api_key=os.getenv("COHERE_API_KEY")
    )

    # ContextualCompressionRetriever ìƒì„±
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return compression_retriever

# ì‚¬ìš©
vectorstore = PGVector(
    collection_name="paper_chunks",
    embedding_function=OpenAIEmbeddings(model="text-embedding-3-small"),
    connection_string="postgresql://user:password@localhost:5432/papers"
)
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# Reranking ì ìš©
rerank_retriever = create_rerank_retriever(base_retriever)

# ê²€ìƒ‰ (ìƒìœ„ 20ê°œ ì¤‘ ì¬ìˆœìœ„í™”í•˜ì—¬ ìƒìœ„ 5ê°œ ë°˜í™˜)
docs = rerank_retriever.invoke("Transformer architecture")
```

#### 2. LLMChainExtractor (ì»¤ìŠ¤í…€)
```python
from langchain.retrievers.document_compressors import LLMChainExtractor

def create_llm_extractor_retriever(base_retriever, llm):
    """LLMì„ ì‚¬ìš©í•œ ë¬¸ì„œ ì••ì¶•"""
    compressor = LLMChainExtractor.from_llm(llm)

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return compression_retriever
```

### êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥
1. Cohere API í‚¤ ì„¤ì •
2. ContextualCompressionRetriever êµ¬í˜„
3. RAG ê²€ìƒ‰ ë„êµ¬ì— Reranking ì ìš©
4. ì„±ëŠ¥ ë¹„êµ (Before/After)

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**1-2ì¼** (11/03~11/04)

### ì°¸ê³  ìë£Œ
- ContextualCompressionRetriever: https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/

---

## ì„ íƒ ê¸°ëŠ¥ 4: ë…¼ë¬¸ ë¹„êµ ë„êµ¬ (â­)

### ê¸°ëŠ¥ ì„¤ëª…
ì—¬ëŸ¬ ë…¼ë¬¸ì˜ ì°¨ì´ì ì„ ë¹„êµí•˜ì—¬ í‘œë¡œ ì •ë¦¬

### ì‚¬ìš© ì‹œì 
- "BERTì™€ GPT ë¹„êµí•´ì¤˜"
- "Transformerì™€ RNNì˜ ì°¨ì´ëŠ”?"

### Langchain êµ¬í˜„
```python
# src/tools/paper_comparison.py

from langchain.tools import tool

@tool
def compare_papers(paper1_title: str, paper2_title: str) -> str:
    """
    ë‘ ë…¼ë¬¸ì„ ë¹„êµí•©ë‹ˆë‹¤.

    Args:
        paper1_title: ì²« ë²ˆì§¸ ë…¼ë¬¸ ì œëª©
        paper2_title: ë‘ ë²ˆì§¸ ë…¼ë¬¸ ì œëª©

    Returns:
        ë¹„êµ ê²°ê³¼ (í‘œ í˜•ì‹)
    """
    # 1. ë‘ ë…¼ë¬¸ ê²€ìƒ‰
    paper1_docs = rag_retriever.retrieve(paper1_title)
    paper2_docs = rag_retriever.retrieve(paper2_title)

    # 2. LLMì—ê²Œ ë¹„êµ ìš”ì²­
    prompt = f"""
    ë‹¤ìŒ ë‘ ë…¼ë¬¸ì„ ë¹„êµí•˜ì—¬ í‘œë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

    ë…¼ë¬¸ 1: {paper1_title}
    ë‚´ìš©: {paper1_docs[0].page_content}

    ë…¼ë¬¸ 2: {paper2_title}
    ë‚´ìš©: {paper2_docs[0].page_content}

    ë¹„êµ í•­ëª©:
    - ì£¼ìš” ê¸°ì—¬
    - ëª¨ë¸ êµ¬ì¡°
    - ì¥ì 
    - ë‹¨ì 
    - ì„±ëŠ¥

    Markdown í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
    """

    result = llm.invoke(prompt).content

    return result
```

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**1ì¼** (11/04)

---

## ì„ íƒ ê¸°ëŠ¥ 5: ëŒ€í™” ì´ë ¥ ì‹œê°í™” (â­)

### ê¸°ëŠ¥ ì„¤ëª…
ì‚¬ìš©ì ì§ˆë¬¸ íŒ¨í„´, ìì£¼ ì‚¬ìš©ë˜ëŠ” ë„êµ¬ ë“±ì„ ì‹œê°í™”

### Streamlit êµ¬í˜„
```python
# ui/analytics.py

import streamlit as st
import pandas as pd
import plotly.express as px

def show_analytics():
    """ëŒ€í™” ì´ë ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""
    st.header("ğŸ“Š ëŒ€í™” ì´ë ¥ ë¶„ì„")

    # PostgreSQLì—ì„œ query_logs ì¡°íšŒ
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")

    # ë„êµ¬ ì‚¬ìš© í†µê³„
    df_tools = pd.read_sql("""
        SELECT tool_used, COUNT(*) as count
        FROM query_logs
        GROUP BY tool_used
    """, conn)

    fig = px.bar(df_tools, x="tool_used", y="count", title="ë„êµ¬ ì‚¬ìš© í†µê³„")
    st.plotly_chart(fig)

    # ë‚œì´ë„ë³„ ì§ˆë¬¸ ë¶„í¬
    df_difficulty = pd.read_sql("""
        SELECT difficulty_mode, COUNT(*) as count
        FROM query_logs
        GROUP BY difficulty_mode
    """, conn)

    fig2 = px.pie(df_difficulty, names="difficulty_mode", values="count", title="ë‚œì´ë„ë³„ ì§ˆë¬¸ ë¶„í¬")
    st.plotly_chart(fig2)

    conn.close()
```

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**1ì¼** (11/04)

---

## ì„ íƒ ê¸°ëŠ¥ 6: ë©€í‹°ëª¨ë‹¬ ì§€ì› (ì´ë¯¸ì§€ ë…¼ë¬¸) (â­â­â­)

### ê¸°ëŠ¥ ì„¤ëª…
ë…¼ë¬¸ ë‚´ ê·¸ë˜í”„, í‘œ, ìˆ˜ì‹ ì´ë¯¸ì§€ë¥¼ GPT-4 Visionìœ¼ë¡œ ë¶„ì„

### Langchain êµ¬í˜„
```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# GPT-4 Vision ì‚¬ìš©
llm_vision = ChatOpenAI(model="gpt-4-vision-preview")

def analyze_paper_image(image_path: str, question: str) -> str:
    """ë…¼ë¬¸ ì´ë¯¸ì§€ ë¶„ì„"""
    import base64

    # ì´ë¯¸ì§€ ì¸ì½”ë”©
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode()

    # GPT-4 Vision í˜¸ì¶œ
    message = HumanMessage(
        content=[
            {"type": "text", "text": question},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
        ]
    )

    response = llm_vision.invoke([message])

    return response.content
```

### ì˜ˆìƒ ì†Œìš” ì‹œê°„
**2-3ì¼** (11/03~11/05)

---

## ìš°ì„ ìˆœìœ„ ì¶”ì²œ

### High Priority (ê°€ì‚°ì  í¬ê³  êµ¬í˜„ ê°€ëŠ¥)
1. **Text-to-SQL** (â­â­â­) - 2-3ì¼
2. **ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ** (â­â­) - 2-3ì¼

### Medium Priority (êµ¬í˜„ ê°„ë‹¨)
3. **Reranking** (â­â­) - 1-2ì¼
4. **ë…¼ë¬¸ ë¹„êµ ë„êµ¬** (â­) - 1ì¼
5. **ëŒ€í™” ì´ë ¥ ì‹œê°í™”** (â­) - 1ì¼

### Low Priority (ì‹œê°„ ë§ì´ ì†Œìš”)
6. **ë©€í‹°ëª¨ë‹¬ ì§€ì›** (â­â­â­) - 2-3ì¼

---

## ì„ íƒ ê°€ì´ë“œ

### ì‹œê°„ì´ 2ì¼ ë‚¨ì•˜ë‹¤ë©´
â†’ **Text-to-SQL** ë˜ëŠ” **ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ**

### ì‹œê°„ì´ 1ì¼ ë‚¨ì•˜ë‹¤ë©´
â†’ **Reranking** ë˜ëŠ” **ë…¼ë¬¸ ë¹„êµ ë„êµ¬**

### ì‹œê°„ì´ ì—¬ìœ  ìˆë‹¤ë©´
â†’ **ë©€í‹°ëª¨ë‹¬ ì§€ì›**

---

## ì°¸ê³  ì‚¬í•­

- ì„ íƒ ê¸°ëŠ¥ì€ **í•„ìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤**
- í•µì‹¬ ê¸°ëŠ¥ì„ ë¨¼ì € ì™„ì„±í•œ í›„ ì‹œê°„ì´ ë‚¨ì„ ë•Œë§Œ êµ¬í˜„
- íŒ€ì›ë“¤ê³¼ ìƒì˜í•˜ì—¬ ì—­í•  ë¶„ë°°
- êµ¬í˜„ ì‹œ **Feature ë¸Œëœì¹˜ ìƒì„±** í•„ìˆ˜
