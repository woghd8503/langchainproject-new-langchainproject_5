# ë‹´ë‹¹ì—­í• : ë°•ì¬í™ - ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ ë° DB êµ¬ì¶•

## ë‹´ë‹¹ì ì •ë³´
- **ì´ë¦„**: ë°•ì¬í™
- **ì—­í• **: ë°ì´í„° ì¸í”„ë¼ êµ¬ì¶• ë‹´ë‹¹
- **ì°¸ì—¬ ê¸°ê°„**: ë‹¨ê¸° ì°¸ì—¬ (4ì¼)
- **í•µì‹¬ ì—­í• **: ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘, Langchain Document ì²˜ë¦¬, DB ì ì¬

---

## ë‹´ë‹¹ ëª¨ë“ˆ ë° ì‘ì—…

### 1. ë°ì´í„° ìˆ˜ì§‘ (`scripts/`)
- arXiv APIë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
- Semantic Scholar API ì—°ë™ (ì„ íƒ)
- ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

### 2. Langchain ë¬¸ì„œ ì²˜ë¦¬ (`src/data/`)
- Langchain Document Loader êµ¬í˜„ (PyPDFLoader)
- Langchain Text Splitter êµ¬í˜„ (RecursiveCharacterTextSplitter)
- PDF â†’ Langchain Document ë³€í™˜

### 3. ì„ë² ë”© ë° Vector DB ì ì¬ (`src/data/embeddings.py`)
- OpenAI Embeddingsë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
- Langchain PGVector (PostgreSQL + pgvector) ì—°ë™ ë° ë¬¸ì„œ ì ì¬
- ìš©ì–´ì§‘ ë°ì´í„° ì„ë² ë”© ë° ì €ì¥
- ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

### 4. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸° ì„¤ì •
- PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± (papers, glossary í…Œì´ë¸”)
- pgvector extension ì„¤ì¹˜ ë° ì´ˆê¸°í™”
- ë²¡í„° ì»¬ë ‰ì…˜ ìƒì„±
- í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ

---

## ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨

### 1. ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    subgraph Collection["ğŸ”¸ ìˆ˜ì§‘"]
        A[arXiv API] --> B[ë…¼ë¬¸ ê²€ìƒ‰<br/>í‚¤ì›Œë“œë³„]
        B --> C[ë©”íƒ€ë°ì´í„°<br/>ì¶”ì¶œ]
        C --> D[PDF<br/>ë‹¤ìš´ë¡œë“œ]
    end

    subgraph Processing["ğŸ”¹ ì²˜ë¦¬"]
        D --> E[PyPDFLoader]
        E --> F[Text<br/>Splitting]
        F --> G[Chunks<br/>ìƒì„±]
    end

    subgraph Storage["ğŸ”º ì €ì¥"]
        G --> H[ì„ë² ë”©<br/>ìƒì„±]
        H --> I1[PostgreSQL<br/>ë©”íƒ€ë°ì´í„°]
        H --> I2[pgvector<br/>ë²¡í„°]
    end

    I1 -.-> J[âœ… ì™„ë£Œ]
    I2 -.-> J

    style Collection fill:#e1f5ff,stroke:#01579b,stroke-width:2px,color:#000
    style Processing fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000
    style Storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px,color:#000

    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#64b5f6,stroke:#1976d2,color:#000
    style C fill:#42a5f5,stroke:#1565c0,color:#000
    style D fill:#64b5f6,stroke:#1976d2,color:#000
    style E fill:#ce93d8,stroke:#7b1fa2,color:#000
    style F fill:#ba68c8,stroke:#7b1fa2,color:#000
    style G fill:#ab47bc,stroke:#4a148c,color:#000
    style H fill:#a5d6a7,stroke:#388e3c,color:#000
    style I1 fill:#81c784,stroke:#2e7d32,color:#000
    style I2 fill:#81c784,stroke:#2e7d32,color:#000
    style J fill:#66bb6a,stroke:#2e7d32,color:#000
```

### 2. ë¬¸ì„œ ì²˜ë¦¬ íë¦„

```mermaid
sequenceDiagram
    autonumber
    participant Collector as Collector
    participant arXiv as arXiv API
    participant Loader as PyPDFLoader
    participant Splitter as TextSplitter
    participant Embed as Embeddings
    participant VDB as Vector DB
    participant PG as PostgreSQL

    Collector->>arXiv: í‚¤ì›Œë“œ ê²€ìƒ‰<br/>(Transformer, BERT...)
    arXiv-->>Collector: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸

    loop ê° ë…¼ë¬¸
        Collector->>arXiv: PDF ë‹¤ìš´ë¡œë“œ
        arXiv-->>Collector: PDF íŒŒì¼

        Collector->>Loader: load_pdf(path)
        Loader-->>Collector: Document ë¦¬ìŠ¤íŠ¸

        Collector->>Splitter: split_documents(docs)
        Splitter-->>Collector: ì²­í¬ ë¦¬ìŠ¤íŠ¸

        Collector->>Embed: embed_documents(chunks)
        Embed-->>Collector: ì„ë² ë”© ë²¡í„°

        Collector->>PG: INSERT ë©”íƒ€ë°ì´í„°
        PG-->>Collector: paper_id

        Collector->>VDB: add_documents(chunks)
        VDB-->>Collector: âœ… ì €ì¥ ì™„ë£Œ
    end

    style Collector fill:#90caf9,stroke:#1976d2,color:#000
    style arXiv fill:#ba68c8,stroke:#7b1fa2,color:#000
    style Loader fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Splitter fill:#ab47bc,stroke:#4a148c,color:#000
    style Embed fill:#ffcc80,stroke:#f57c00,color:#000
    style VDB fill:#a5d6a7,stroke:#388e3c,color:#000
    style PG fill:#81c784,stroke:#2e7d32,color:#000
```

---

## í•µì‹¬ ì‘ì—…: ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘

### ëª©í‘œ
**ìµœì†Œ 50-100í¸ ë…¼ë¬¸ ë°ì´í„°**ë¥¼ Langchain íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ Vector DB ì €ì¥ ì™„ë£Œ

### 1. arXiv APIë¡œ ë…¼ë¬¸ ìˆ˜ì§‘

**íŒŒì¼ ê²½ë¡œ**: `scripts/collect_arxiv_papers.py`

**êµ¬í˜„ ë°©ë²•**:
1. `ArxivPaperCollector` í´ë˜ìŠ¤ ì •ì˜
   - ì´ˆê¸°í™” ì‹œ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • (ê¸°ë³¸ê°’: "data/raw/pdfs")
   - ë””ë ‰í† ë¦¬ ìë™ ìƒì„±

2. `collect_papers` ë©”ì„œë“œ êµ¬í˜„
   - arxiv.Search ê°ì²´ ìƒì„± (query, max_results, sort_by ì„¤ì •)
   - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
   - ê° ë…¼ë¬¸ì˜ title, authors, published_date, summary, pdf_url, entry_id, categories, primary_category ì¶”ì¶œ
   - PDF ë‹¤ìš´ë¡œë“œ (arxiv_id ê¸°ë°˜ íŒŒì¼ëª…)
   - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë…¼ë¬¸ ê±´ë„ˆë›°ê¸°
   - ìˆ˜ì§‘í•œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

3. `collect_by_keywords` ë©”ì„œë“œ êµ¬í˜„
   - ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë°˜ë³µ ìˆ˜ì§‘
   - ê° í‚¤ì›Œë“œë‹¹ ì§€ì •ëœ ìˆ˜ë§Œí¼ ë…¼ë¬¸ ìˆ˜ì§‘
   - ì „ì²´ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ í†µí•©
   - ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
   - ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼ ë°˜í™˜

4. `remove_duplicates` ë©”ì„œë“œ êµ¬í˜„
   - ì œëª©ì„ ì†Œë¬¸ìë¡œ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ í™•ì¸
   - ì¤‘ë³µë˜ì§€ ì•Šì€ ë…¼ë¬¸ë§Œ ìœ ì§€

5. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
   - AI/ML ê´€ë ¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì •ì˜
   - í‚¤ì›Œë“œë‹¹ 15í¸ì”© ìˆ˜ì§‘ (ì´ ~100í¸)
   - ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

### ì˜ˆì œ ì½”ë“œ

```python
# scripts/collect_arxiv_papers.py

import arxiv
import os
import json
from datetime import datetime
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "data_collection"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class ArxivPaperCollector:
    """arXivì—ì„œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, save_dir="data/raw/pdfs"):
        """
        Args:
            save_dir: PDF íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def collect_papers(self, query, max_results=50):
        """
        arXivì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜

        Returns:
            ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        papers_data = []

        for result in search.results():
            try:
                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                paper_info = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "published_date": result.published.strftime("%Y-%m-%d"),
                    "summary": result.summary,
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories,
                    "primary_category": result.primary_category
                }

                papers_data.append(paper_info)

                # PDF ë‹¤ìš´ë¡œë“œ
                arxiv_id = result.entry_id.split('/')[-1]
                pdf_filename = f"{self.save_dir}/{arxiv_id}.pdf"

                result.download_pdf(filename=pdf_filename)

                logger.write(f"Downloaded: {result.title} ({arxiv_id})")

            except Exception as e:
                logger.write(f"Error downloading {result.title}: {e}")
                continue

        return papers_data

    def collect_by_keywords(self, keywords, per_keyword=15):
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            per_keyword: í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘í•  ë…¼ë¬¸ ìˆ˜

        Returns:
            ì „ì²´ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        all_papers = []

        for keyword in keywords:
            logger.write(f"\nCollecting papers for keyword: {keyword}")
            papers = self.collect_papers(keyword, max_results=per_keyword)
            all_papers.extend(papers)

        # ì¤‘ë³µ ì œê±°
        unique_papers = self.remove_duplicates(all_papers)

        return unique_papers

    def remove_duplicates(self, papers):
        """
        ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë…¼ë¬¸ ì œê±°

        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        seen_titles = set()
        unique_papers = []

        for paper in papers:
            title_normalized = paper['title'].lower().strip()

            if title_normalized not in seen_titles:
                unique_papers.append(paper)
                seen_titles.add(title_normalized)

        return unique_papers


# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    collector = ArxivPaperCollector()

    # AI/ML ê´€ë ¨ í‚¤ì›Œë“œ
    keywords = [
        "transformer attention",
        "BERT GPT",
        "large language model",
        "retrieval augmented generation",
        "neural machine translation",
        "question answering",
        "AI agent"
    ]

    # í‚¤ì›Œë“œë‹¹ 15í¸ì”© ìˆ˜ì§‘
    papers = collector.collect_by_keywords(keywords, per_keyword=15)

    logger.write(f"\nì´ {len(papers)}í¸ì˜ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")

    # ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    with open("data/raw/arxiv_papers_metadata.json", "w", encoding="utf-8") as f:
        json.dump(papers, f, indent=2, ensure_ascii=False)

    logger.write("ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: data/raw/arxiv_papers_metadata.json")

    logger.close()
```

---

## Langchain Document Loader êµ¬í˜„

### 2. PDF â†’ Langchain Document ë³€í™˜

**íŒŒì¼ ê²½ë¡œ**: `src/data/document_loader.py`

**êµ¬í˜„ ë°©ë²•**:
1. `PaperDocumentLoader` í´ë˜ìŠ¤ ì •ì˜
   - RecursiveCharacterTextSplitter ì´ˆê¸°í™”
   - chunk_size: 1000 (ì²­í¬ í¬ê¸°)
   - chunk_overlap: 200 (ì²­í¬ ê°„ ì¤‘ë³µ, ë§¥ë½ ìœ ì§€)
   - separators: ["\n\n", "\n", ". ", " ", ""] (ë¶„í•  ìš°ì„ ìˆœìœ„)

2. `load_pdf` ë©”ì„œë“œ êµ¬í˜„
   - PyPDFLoaderë¡œ PDF íŒŒì¼ ë¡œë“œ
   - ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê° ë¬¸ì„œì— ì—…ë°ì´íŠ¸
   - Langchain Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

3. `load_and_split` ë©”ì„œë“œ êµ¬í˜„
   - PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 
   - load_pdf()ë¡œ PDF ë¡œë“œ
   - text_splitter.split_documents()ë¡œ ì²­í¬ ë¶„í• 
   - ê° ì²­í¬ì— chunk_id ë©”íƒ€ë°ì´í„° ì¶”ê°€
   - ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

4. `load_all_pdfs` ë©”ì„œë“œ êµ¬í˜„
   - JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ
   - arXiv IDë¡œ ë©”íƒ€ë°ì´í„° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
   - ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDF íŒŒì¼ ìˆœíšŒ
   - ê° PDFì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ë° ë¡œë“œ
   - load_and_split()ìœ¼ë¡œ ì²­í¬ ë¶„í• 
   - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ íŒŒì¼ ê±´ë„ˆë›°ê¸°
   - ëª¨ë“  ì²­í¬ë¥¼ í†µí•©í•˜ì—¬ ë°˜í™˜

### ì˜ˆì œ ì½”ë“œ

```python
# src/data/document_loader.py

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import json
from datetime import datetime
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "document_loader"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class PaperDocumentLoader:
    """ë…¼ë¬¸ PDFë¥¼ Langchain Documentë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        """RecursiveCharacterTextSplitter ì´ˆê¸°í™”"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # ì²­í¬ í¬ê¸°
            chunk_overlap=200,  # ì²­í¬ ê°„ ì¤‘ë³µ (ë§¥ë½ ìœ ì§€)
            separators=["\n\n", "\n", ". ", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
        )

    def load_pdf(self, pdf_path, metadata=None):
        """
        PDF íŒŒì¼ ë¡œë“œ

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            Langchain Document ë¦¬ìŠ¤íŠ¸
        """
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if metadata:
            for doc in documents:
                doc.metadata.update(metadata)

        return documents

    def load_and_split(self, pdf_path, metadata=None):
        """
        PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        # PDF ë¡œë“œ
        documents = self.load_pdf(pdf_path, metadata)

        # ì²­í¬ ë¶„í• 
        chunks = self.text_splitter.split_documents(documents)

        # ê° ì²­í¬ì— chunk_id ì¶”ê°€
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i

        return chunks

    def load_all_pdfs(self, pdf_dir, metadata_json_path):
        """
        ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„í• 

        Args:
            pdf_dir: PDF íŒŒì¼ ë””ë ‰í† ë¦¬
            metadata_json_path: ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            ëª¨ë“  ì²­í¬ë¥¼ í†µí•©í•œ Document ë¦¬ìŠ¤íŠ¸
        """
        # JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_json_path, 'r', encoding='utf-8') as f:
            papers_metadata = json.load(f)

        # arXiv IDë¡œ ë©”íƒ€ë°ì´í„° ë§¤í•‘
        metadata_map = {}
        for paper in papers_metadata:
            arxiv_id = paper['entry_id'].split('/')[-1]
            metadata_map[arxiv_id] = paper

        all_chunks = []

        # ëª¨ë“  PDF íŒŒì¼ ìˆœíšŒ
        for filename in os.listdir(pdf_dir):
            if not filename.endswith('.pdf'):
                continue

            arxiv_id = filename.replace('.pdf', '')
            pdf_path = os.path.join(pdf_dir, filename)

            # ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            metadata = metadata_map.get(arxiv_id, {})

            try:
                # ì²­í¬ ë¶„í• 
                chunks = self.load_and_split(pdf_path, metadata)
                all_chunks.extend(chunks)

                logger.write(f"Loaded and split: {filename} ({len(chunks)} chunks)")

            except Exception as e:
                logger.write(f"Error loading {filename}: {e}")
                continue

        return all_chunks


# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    loader = PaperDocumentLoader()

    # ëª¨ë“  PDF ë¡œë“œ ë° ë¶„í• 
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    logger.write(f"\nì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")

    logger.close()
```

---

## ì„ë² ë”© ë° Vector DB ì ì¬

### 3. OpenAI Embeddings ìƒì„± ë° pgvector ì €ì¥

**íŒŒì¼ ê²½ë¡œ**: `src/data/embeddings.py`

**êµ¬í˜„ ë°©ë²•**:
1. `PaperEmbeddingManager` í´ë˜ìŠ¤ ì •ì˜
   - OpenAI Embeddings ì´ˆê¸°í™” (ëª¨ë¸: text-embedding-3-small)
   - API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
   - PostgreSQL + pgvector VectorStore ì´ˆê¸°í™”
   - ì»¬ë ‰ì…˜ëª…: "paper_chunks"
   - PostgreSQL ì—°ê²° ë¬¸ìì—´ ì„¤ì •

2. `add_documents` ë©”ì„œë“œ êµ¬í˜„
   - Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
   - OpenAI API ì†ë„ ì œí•œ ëŒ€ì‘ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
   - ê° ë°°ì¹˜ë¥¼ vectorstore.add_documents()ë¡œ ì €ì¥
   - ì§„í–‰ ìƒí™© ì¶œë ¥
   - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ ê±´ë„ˆë›°ê¸°

3. `add_documents_with_paper_id` ë©”ì„œë“œ êµ¬í˜„
   - ê° ë¬¸ì„œì˜ URLì—ì„œ arXiv ID ì¶”ì¶œ
   - paper_id_mappingì—ì„œ PostgreSQL paper_id ì¡°íšŒ
   - ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— paper_id ì¶”ê°€
   - add_documents()ë¡œ Vector DBì— ì €ì¥

4. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
   - PaperDocumentLoaderë¡œ PDF ë¡œë“œ ë° ë¶„í• 
   - PaperEmbeddingManagerë¡œ ì„ë² ë”© ë° Vector DB ì €ì¥
   - ë°°ì¹˜ í¬ê¸°: 50

### ì˜ˆì œ ì½”ë“œ

```python
# src/data/embeddings.py

from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector
import os
from datetime import datetime
from src.utils.logger import Logger

# Logger ì´ˆê¸°í™”
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "data_embedding"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class PaperEmbeddingManager:
    """ë…¼ë¬¸ ì„ë² ë”© ë° Vector DB ì €ì¥ í´ë˜ìŠ¤"""

    def __init__(self):
        """OpenAI Embeddings ë° pgvector VectorStore ì´ˆê¸°í™”"""
        # OpenAI Embeddings ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # PostgreSQL + pgvector VectorStore ì´ˆê¸°í™”
        self.vectorstore = PGVector(
            collection_name="paper_chunks",
            embedding_function=self.embeddings,
            connection_string=os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/papers")
        )

    def add_documents(self, documents, batch_size=50):
        """
        Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ Vector DBì— ì €ì¥

        Args:
            documents: Langchain Document ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (OpenAI API ì†ë„ ì œí•œ ëŒ€ì‘)

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        total_docs = len(documents)
        saved_count = 0

        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, total_docs, batch_size):
            batch = documents[i:i + batch_size]

            try:
                self.vectorstore.add_documents(batch)
                saved_count += len(batch)

                logger.write(f"Saved batch {i//batch_size + 1}: {saved_count}/{total_docs} documents")

            except Exception as e:
                logger.write(f"Error saving batch {i//batch_size + 1}: {e}")
                continue

        return saved_count

    def add_documents_with_paper_id(self, documents, paper_id_mapping):
        """
        ê° ë¬¸ì„œì˜ URLì—ì„œ arXiv IDë¥¼ ì¶”ì¶œí•˜ì—¬ paper_id ì¶”ê°€ í›„ ì €ì¥

        Args:
            documents: Langchain Document ë¦¬ìŠ¤íŠ¸
            paper_id_mapping: arXiv ID -> paper_id ë§¤í•‘ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        # ê° ë¬¸ì„œì— paper_id ì¶”ê°€
        for doc in documents:
            pdf_url = doc.metadata.get('pdf_url', '')
            if pdf_url:
                arxiv_id = pdf_url.split('/')[-1].replace('.pdf', '')
                paper_id = paper_id_mapping.get(arxiv_id)

                if paper_id:
                    doc.metadata['paper_id'] = paper_id

        # Vector DBì— ì €ì¥
        return self.add_documents(documents)


# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    import json
    from src.data.document_loader import PaperDocumentLoader

    # 1. PDF ë¡œë“œ ë° ë¶„í• 
    loader = PaperDocumentLoader()
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    logger.write(f"\nì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")

    # 2. paper_id_mapping ë¡œë“œ
    with open("data/processed/paper_id_mapping.json", 'r') as f:
        paper_id_mapping = json.load(f)

    # 3. ì„ë² ë”© ë° Vector DB ì €ì¥
    embedding_manager = PaperEmbeddingManager()
    saved_count = embedding_manager.add_documents_with_paper_id(
        chunks,
        paper_id_mapping
    )

    logger.write(f"\nì´ {saved_count}ê°œì˜ ë¬¸ì„œë¥¼ Vector DBì— ì €ì¥ ì™„ë£Œ")

    logger.close()
```

---

## PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •

### 4. ìŠ¤í‚¤ë§ˆ ìƒì„± ë° ë©”íƒ€ë°ì´í„° ì €ì¥

**íŒŒì¼ ê²½ë¡œ**: `scripts/setup_database.py`

**êµ¬í˜„ ë°©ë²•**:
1. `create_tables` í•¨ìˆ˜ êµ¬í˜„
   - PostgreSQL ì—°ê²° ë° ì»¤ì„œ ìƒì„±
   - papers í…Œì´ë¸” ìƒì„± (paper_id, title, authors, publish_date, source, url, category, citation_count, abstract, created_at)
   - glossary í…Œì´ë¸” ìƒì„± (term_id, term, definition, easy_explanation, hard_explanation, category, difficulty_level, related_terms, examples, created_at)
   - ì¸ë±ìŠ¤ ìƒì„± (papersì˜ title, category, date ë° glossaryì˜ term)
   - ì»¤ë°‹ ë° ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥

2. `insert_paper_metadata` í•¨ìˆ˜ êµ¬í˜„
   - JSON ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ
   - ê° ë…¼ë¬¸ ë°ì´í„°ë¥¼ papers í…Œì´ë¸”ì— INSERT
   - ON CONFLICT (url) DO NOTHINGìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
   - RETURNING paper_idë¡œ ì‚½ì…ëœ ID ì¡°íšŒ
   - arxiv_idì™€ paper_id ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
   - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë…¼ë¬¸ ê±´ë„ˆë›°ê¸°
   - ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

3. `insert_glossary_data` í•¨ìˆ˜ êµ¬í˜„
   - ìš©ì–´ì§‘ ì´ˆê¸° ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì •ì˜ (Attention Mechanism, Fine-tuning, BLEU Score ë“±)
   - ê° ìš©ì–´ ë°ì´í„°ë¥¼ glossary í…Œì´ë¸”ì— INSERT
   - ON CONFLICT (term) DO NOTHINGìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
   - ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ìš©ì–´ ê±´ë„ˆë›°ê¸°

4. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
   - PostgreSQL ì—°ê²°
   - create_tables()ë¡œ í…Œì´ë¸” ë° ì¸ë±ìŠ¤ ìƒì„±
   - JSON íŒŒì¼ì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ
   - insert_paper_metadata()ë¡œ ë…¼ë¬¸ ë°ì´í„° ì‚½ì…
   - insert_glossary_data()ë¡œ ìš©ì–´ì§‘ ë°ì´í„° ì‚½ì…
   - paper_id_mappingì„ JSON íŒŒì¼ë¡œ ì €ì¥

---

## ì¸ìˆ˜ì¸ê³„ ë¬¸ì„œ

### ì™„ë£Œ í•­ëª© ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] arXivì—ì„œ ìµœì†Œ 50í¸ ë…¼ë¬¸ ìˆ˜ì§‘
- [ ] PDF â†’ Langchain Document ë³€í™˜ ì™„ë£Œ
- [ ] PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± (papers, glossary)
- [ ] PostgreSQL + pgvector ì»¬ë ‰ì…˜ ìƒì„± (paper_chunks, glossary_embeddings)
- [ ] ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° PostgreSQL ì €ì¥
- [ ] ë…¼ë¬¸ ì„ë² ë”© pgvector ì €ì¥
- [ ] ìš©ì–´ì§‘ ì´ˆê¸° ë°ì´í„° ì‚½ì…
- [ ] paper_id_mapping.json ìƒì„±

### ì¸ìˆ˜ì¸ê³„ ë‚´ìš©

#### 1. ë°ì´í„° ìœ„ì¹˜
- **PDF íŒŒì¼**: `data/raw/pdfs/`
- **ë©”íƒ€ë°ì´í„°**: `data/raw/arxiv_papers_metadata.json`
- **paper_id ë§¤í•‘**: `data/processed/paper_id_mapping.json`
- **Vector DB**: PostgreSQL + pgvector (connection: `postgresql://user:password@localhost:5432/papers`)

#### 2. ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•
```bash
# ì¶”ê°€ ë…¼ë¬¸ ìˆ˜ì§‘
python scripts/collect_arxiv_papers.py --query "ìƒˆë¡œìš´ í‚¤ì›Œë“œ" --max-results 50

# ì„ë² ë”© ë° DB ì €ì¥
python src/data/embeddings.py
```

#### 3. DB ì—°ê²° ì •ë³´
- PostgreSQL + pgvector: `postgresql://user:password@localhost:5432/papers`
  - ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°: `papers` í…Œì´ë¸”
  - ìš©ì–´ì§‘: `glossary` í…Œì´ë¸”
  - ë²¡í„° ì„ë² ë”©: pgvector extension ì‚¬ìš©

#### 4. ì£¼ì˜ì‚¬í•­
- OpenAI API í‚¤ í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜: `OPENAI_API_KEY`
- ì„ë² ë”© ìƒì„± ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ (API ì†ë„ ì œí•œ)
- PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ í•„ìš”

---

## ë¡œê¹… ë° ì‹¤í—˜ ì¶”ì  ê´€ë¦¬

### ë¡œê¹… ì‹œìŠ¤í…œ ì‚¬ìš©

**ì¤‘ìš”**: ëª¨ë“  ì¶œë ¥ì€ Logger í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**íŒŒì¼ ê²½ë¡œ**: `src/utils/logger.py`

**ì‚¬ìš© ë°©ë²•**:
1. Logger ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
   - ì‹¤í—˜ í´ë” ìƒì„± ë° Logger ì´ˆê¸°í™”
   ```python
   import os
   from datetime import datetime
   from src.utils.logger import Logger

   # ì‹¤í—˜ í´ë” ìƒì„±
   today = datetime.now().strftime("%Y%m%d")
   time_now = datetime.now().strftime("%H%M%S")
   experiment_name = "data_collection"  # ë˜ëŠ” "data_embedding" ë“±
   log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
   os.makedirs(log_dir, exist_ok=True)

   # Logger ì´ˆê¸°í™”
   logger = Logger(log_path=f"{log_dir}/experiment.log")
   ```

2. ë¡œê·¸ ê¸°ë¡
   - `logger.write()` ì‚¬ìš© (print() ëŒ€ì‹ )
   - ì˜ˆ: `logger.write(f"Downloaded: {title}")`

3. ì‹¤í—˜ ì¢…ë£Œ
   - `logger.close()` í•„ìˆ˜ í˜¸ì¶œ

### ì‹¤í—˜ í´ë” êµ¬ì¡°

PRD ë¬¸ì„œ 06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md ì°¸ì¡°

---

## Feature ë¸Œëœì¹˜

**1ë‹¨ê³„: ë°ì´í„° ì¸í”„ë¼ êµ¬ì¶• (ë°•ì¬í™)**
- `1-1. feature/data-collection` - arXiv ë°ì´í„° ìˆ˜ì§‘
- `1-2. feature/document-processing` - Document Loader/Splitter
- `1-3. feature/database-setup` - PostgreSQL + pgvector ì„¤ì •

---

## ì°¸ê³  PRD ë¬¸ì„œ

ê°œë°œ ì‹œ ë°˜ë“œì‹œ ì°¸ê³ í•´ì•¼ í•  PRD ë¬¸ì„œ ëª©ë¡:

### í•„ìˆ˜ ì°¸ê³  ë¬¸ì„œ
1. [01_í”„ë¡œì íŠ¸_ê°œìš”.md](../PRD/01_í”„ë¡œì íŠ¸_ê°œìš”.md) - í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”
2. [02_í”„ë¡œì íŠ¸_êµ¬ì¡°.md](../PRD/02_í”„ë¡œì íŠ¸_êµ¬ì¡°.md) - í´ë” êµ¬ì¡° (data/, scripts/)
3. [05_ë¡œê¹…_ì‹œìŠ¤í…œ.md](../PRD/05_ë¡œê¹…_ì‹œìŠ¤í…œ.md) â­ - Logger ì‚¬ìš©ë²•
4. [06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md](../PRD/06_ì‹¤í—˜_ì¶”ì _ê´€ë¦¬.md) â­ - ì‹¤í—˜ í´ë” êµ¬ì¡°
5. [10_ê¸°ìˆ _ìš”êµ¬ì‚¬í•­.md](../PRD/10_ê¸°ìˆ _ìš”êµ¬ì‚¬í•­.md) - arXiv API, PyPDFLoader, OpenAI Embeddings
6. [11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md](../PRD/11_ë°ì´í„°ë² ì´ìŠ¤_ì„¤ê³„.md) - papers í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
7. [13_RAG_ì‹œìŠ¤í…œ_ì„¤ê³„.md](../PRD/13_RAG_ì‹œìŠ¤í…œ_ì„¤ê³„.md) - Document ì²˜ë¦¬ ë° Text Splitting

### ì°¸ê³  ë¬¸ì„œ
- [03_ë¸Œëœì¹˜_ì „ëµ.md](../PRD/03_ë¸Œëœì¹˜_ì „ëµ.md) - Feature ë¸Œëœì¹˜
- [04_ì¼ì •_ê´€ë¦¬.md](../PRD/04_ì¼ì •_ê´€ë¦¬.md) - ê°œë°œ ì¼ì •

---

## ì°¸ê³  ìë£Œ

- arXiv API: https://info.arxiv.org/help/api/index.html
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- Langchain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Langchain Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- pgvector ë¬¸ì„œ: https://github.com/pgvector/pgvector

