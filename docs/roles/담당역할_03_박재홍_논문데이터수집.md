# 담당역할: 박재홍 - 논문 데이터 수집 및 DB 구축

## 담당자 정보
- **이름**: 박재홍
- **역할**: 데이터 인프라 구축 담당
- **참여 기간**: 단기 참여 (4일)
- **핵심 역할**: 논문 데이터 수집, Langchain Document 처리, DB 적재

---

## 담당 모듈 및 작업

### 1. 데이터 수집 (`scripts/`)
- arXiv API로 논문 수집 스크립트
- Semantic Scholar API 연동 (선택)
- 논문 메타데이터 수집

### 2. Langchain 문서 처리 (`src/data/`)
- Langchain Document Loader 구현 (PyPDFLoader)
- Langchain Text Splitter 구현 (RecursiveCharacterTextSplitter)
- PDF → Langchain Document 변환

### 3. 임베딩 및 Vector DB 적재 (`src/data/embeddings.py`)
- OpenAI Embeddings를 사용한 임베딩 생성
- Langchain PGVector (PostgreSQL + pgvector) 연동 및 문서 적재
- 용어집 데이터 임베딩 및 저장
- 배치 처리 최적화

### 4. 데이터베이스 초기 설정
- PostgreSQL 스키마 생성 (papers, glossary 테이블)
- pgvector extension 설치 및 초기화
- 벡터 컬렉션 생성
- 테스트 데이터 로드

---

## 핵심 작업: 논문 데이터 수집

### 목표
**최소 50-100편 논문 데이터**를 Langchain 파이프라인으로 처리하여 Vector DB 저장 완료

### 1. arXiv API로 논문 수집

**파일 경로**: `scripts/collect_arxiv_papers.py`

**구현 방법**:
1. `ArxivPaperCollector` 클래스 정의
   - 초기화 시 저장 디렉토리 설정 (기본값: "data/raw/pdfs")
   - 디렉토리 자동 생성

2. `collect_papers` 메서드 구현
   - arxiv.Search 객체 생성 (query, max_results, sort_by 설정)
   - 검색 결과를 순회하며 메타데이터 수집
   - 각 논문의 title, authors, published_date, summary, pdf_url, entry_id, categories, primary_category 추출
   - PDF 다운로드 (arxiv_id 기반 파일명)
   - 오류 발생 시 해당 논문 건너뛰기
   - 수집한 논문 메타데이터 리스트 반환

3. `collect_by_keywords` 메서드 구현
   - 여러 키워드로 반복 수집
   - 각 키워드당 지정된 수만큼 논문 수집
   - 전체 논문 리스트 통합
   - 중복 제거 (제목 기준)
   - 최종 수집 결과 반환

4. `remove_duplicates` 메서드 구현
   - 제목을 소문자로 정규화하여 중복 확인
   - 중복되지 않은 논문만 유지

5. 실행 스크립트 작성
   - AI/ML 관련 키워드 리스트 정의
   - 키워드당 15편씩 수집 (총 ~100편)
   - 메타데이터를 JSON 파일로 저장

### 예제 코드

```python
# scripts/collect_arxiv_papers.py

import arxiv
import os
import json
from datetime import datetime

class ArxivPaperCollector:
    """arXiv에서 논문을 수집하는 클래스"""

    def __init__(self, save_dir="data/raw/pdfs"):
        """
        Args:
            save_dir: PDF 파일 저장 디렉토리
        """
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def collect_papers(self, query, max_results=50):
        """
        arXiv에서 논문 수집

        Args:
            query: 검색 쿼리
            max_results: 최대 수집 논문 수

        Returns:
            논문 메타데이터 리스트
        """
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        papers_data = []

        for result in search.results():
            try:
                # 메타데이터 수집
                paper_info = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "published_date": result.published.strftime("%Y-%m-%d"),
                    "summary": result.summary,
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories,
                    "primary_category": result.primary_category
                }

                papers_data.append(paper_info)

                # PDF 다운로드
                arxiv_id = result.entry_id.split('/')[-1]
                pdf_filename = f"{self.save_dir}/{arxiv_id}.pdf"

                result.download_pdf(filename=pdf_filename)

                print(f"Downloaded: {result.title} ({arxiv_id})")

            except Exception as e:
                print(f"Error downloading {result.title}: {e}")
                continue

        return papers_data

    def collect_by_keywords(self, keywords, per_keyword=15):
        """
        여러 키워드로 논문 수집

        Args:
            keywords: 키워드 리스트
            per_keyword: 키워드당 수집할 논문 수

        Returns:
            전체 논문 메타데이터 리스트
        """
        all_papers = []

        for keyword in keywords:
            print(f"\nCollecting papers for keyword: {keyword}")
            papers = self.collect_papers(keyword, max_results=per_keyword)
            all_papers.extend(papers)

        # 중복 제거
        unique_papers = self.remove_duplicates(all_papers)

        return unique_papers

    def remove_duplicates(self, papers):
        """
        제목 기준으로 중복 논문 제거

        Args:
            papers: 논문 리스트

        Returns:
            중복 제거된 논문 리스트
        """
        seen_titles = set()
        unique_papers = []

        for paper in papers:
            title_normalized = paper['title'].lower().strip()

            if title_normalized not in seen_titles:
                unique_papers.append(paper)
                seen_titles.add(title_normalized)

        return unique_papers


# 실행 스크립트
if __name__ == "__main__":
    collector = ArxivPaperCollector()

    # AI/ML 관련 키워드
    keywords = [
        "transformer attention",
        "BERT GPT",
        "large language model",
        "retrieval augmented generation",
        "neural machine translation",
        "question answering",
        "AI agent"
    ]

    # 키워드당 15편씩 수집
    papers = collector.collect_by_keywords(keywords, per_keyword=15)

    print(f"\n총 {len(papers)}편의 논문 수집 완료")

    # 메타데이터를 JSON 파일로 저장
    with open("data/raw/arxiv_papers_metadata.json", "w", encoding="utf-8") as f:
        json.dump(papers, f, indent=2, ensure_ascii=False)

    print("메타데이터 저장 완료: data/raw/arxiv_papers_metadata.json")
```

---

## Langchain Document Loader 구현

### 2. PDF → Langchain Document 변환

**파일 경로**: `src/data/document_loader.py`

**구현 방법**:
1. `PaperDocumentLoader` 클래스 정의
   - RecursiveCharacterTextSplitter 초기화
   - chunk_size: 1000 (청크 크기)
   - chunk_overlap: 200 (청크 간 중복, 맥락 유지)
   - separators: ["\n\n", "\n", ". ", " ", ""] (분할 우선순위)

2. `load_pdf` 메서드 구현
   - PyPDFLoader로 PDF 파일 로드
   - 추가 메타데이터가 있으면 각 문서에 업데이트
   - Langchain Document 리스트 반환

3. `load_and_split` 메서드 구현
   - PDF 파일을 로드하고 청크로 분할
   - load_pdf()로 PDF 로드
   - text_splitter.split_documents()로 청크 분할
   - 각 청크에 chunk_id 메타데이터 추가
   - 분할된 Document 리스트 반환

4. `load_all_pdfs` 메서드 구현
   - JSON 메타데이터 파일 로드
   - arXiv ID로 메타데이터 매핑 딕셔너리 생성
   - 디렉토리의 모든 PDF 파일 순회
   - 각 PDF에 대해 메타데이터 조회 및 로드
   - load_and_split()으로 청크 분할
   - 오류 발생 시 해당 파일 건너뛰기
   - 모든 청크를 통합하여 반환

### 예제 코드

```python
# src/data/document_loader.py

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import json

class PaperDocumentLoader:
    """논문 PDF를 Langchain Document로 변환하는 클래스"""

    def __init__(self):
        """RecursiveCharacterTextSplitter 초기화"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # 청크 크기
            chunk_overlap=200,  # 청크 간 중복 (맥락 유지)
            separators=["\n\n", "\n", ". ", " ", ""]  # 분할 우선순위
        )

    def load_pdf(self, pdf_path, metadata=None):
        """
        PDF 파일 로드

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가 메타데이터

        Returns:
            Langchain Document 리스트
        """
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 추가 메타데이터 업데이트
        if metadata:
            for doc in documents:
                doc.metadata.update(metadata)

        return documents

    def load_and_split(self, pdf_path, metadata=None):
        """
        PDF 파일을 로드하고 청크로 분할

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가 메타데이터

        Returns:
            분할된 Document 리스트
        """
        # PDF 로드
        documents = self.load_pdf(pdf_path, metadata)

        # 청크 분할
        chunks = self.text_splitter.split_documents(documents)

        # 각 청크에 chunk_id 추가
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i

        return chunks

    def load_all_pdfs(self, pdf_dir, metadata_json_path):
        """
        디렉토리의 모든 PDF 파일을 로드하고 분할

        Args:
            pdf_dir: PDF 파일 디렉토리
            metadata_json_path: 메타데이터 JSON 파일 경로

        Returns:
            모든 청크를 통합한 Document 리스트
        """
        # JSON 메타데이터 로드
        with open(metadata_json_path, 'r', encoding='utf-8') as f:
            papers_metadata = json.load(f)

        # arXiv ID로 메타데이터 매핑
        metadata_map = {}
        for paper in papers_metadata:
            arxiv_id = paper['entry_id'].split('/')[-1]
            metadata_map[arxiv_id] = paper

        all_chunks = []

        # 모든 PDF 파일 순회
        for filename in os.listdir(pdf_dir):
            if not filename.endswith('.pdf'):
                continue

            arxiv_id = filename.replace('.pdf', '')
            pdf_path = os.path.join(pdf_dir, filename)

            # 메타데이터 조회
            metadata = metadata_map.get(arxiv_id, {})

            try:
                # 청크 분할
                chunks = self.load_and_split(pdf_path, metadata)
                all_chunks.extend(chunks)

                print(f"Loaded and split: {filename} ({len(chunks)} chunks)")

            except Exception as e:
                print(f"Error loading {filename}: {e}")
                continue

        return all_chunks


# 실행 스크립트
if __name__ == "__main__":
    loader = PaperDocumentLoader()

    # 모든 PDF 로드 및 분할
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    print(f"\n총 {len(chunks)}개의 청크 생성 완료")
```

---

## 임베딩 및 Vector DB 적재

### 3. OpenAI Embeddings 생성 및 pgvector 저장

**파일 경로**: `src/data/embeddings.py`

**구현 방법**:
1. `PaperEmbeddingManager` 클래스 정의
   - OpenAI Embeddings 초기화 (모델: text-embedding-3-small)
   - API 키를 환경변수에서 로드
   - PostgreSQL + pgvector VectorStore 초기화
   - 컬렉션명: "paper_chunks"
   - PostgreSQL 연결 문자열 설정

2. `add_documents` 메서드 구현
   - Document 리스트를 배치로 나누어 처리
   - OpenAI API 속도 제한 대응을 위한 배치 처리
   - 각 배치를 vectorstore.add_documents()로 저장
   - 진행 상황 출력
   - 오류 발생 시 해당 배치 건너뛰기

3. `add_documents_with_paper_id` 메서드 구현
   - 각 문서의 URL에서 arXiv ID 추출
   - paper_id_mapping에서 PostgreSQL paper_id 조회
   - 문서 메타데이터에 paper_id 추가
   - add_documents()로 Vector DB에 저장

4. 실행 스크립트 작성
   - PaperDocumentLoader로 PDF 로드 및 분할
   - PaperEmbeddingManager로 임베딩 및 Vector DB 저장
   - 배치 크기: 50

### 예제 코드

```python
# src/data/embeddings.py

from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector
import os

class PaperEmbeddingManager:
    """논문 임베딩 및 Vector DB 저장 클래스"""

    def __init__(self):
        """OpenAI Embeddings 및 pgvector VectorStore 초기화"""
        # OpenAI Embeddings 초기화
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # PostgreSQL + pgvector VectorStore 초기화
        self.vectorstore = PGVector(
            collection_name="paper_chunks",
            embedding_function=self.embeddings,
            connection_string=os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/papers")
        )

    def add_documents(self, documents, batch_size=50):
        """
        Document 리스트를 배치로 나누어 Vector DB에 저장

        Args:
            documents: Langchain Document 리스트
            batch_size: 배치 크기 (OpenAI API 속도 제한 대응)

        Returns:
            저장된 문서 수
        """
        total_docs = len(documents)
        saved_count = 0

        # 배치 처리
        for i in range(0, total_docs, batch_size):
            batch = documents[i:i + batch_size]

            try:
                self.vectorstore.add_documents(batch)
                saved_count += len(batch)

                print(f"Saved batch {i//batch_size + 1}: {saved_count}/{total_docs} documents")

            except Exception as e:
                print(f"Error saving batch {i//batch_size + 1}: {e}")
                continue

        return saved_count

    def add_documents_with_paper_id(self, documents, paper_id_mapping):
        """
        각 문서의 URL에서 arXiv ID를 추출하여 paper_id 추가 후 저장

        Args:
            documents: Langchain Document 리스트
            paper_id_mapping: arXiv ID -> paper_id 매핑 딕셔너리

        Returns:
            저장된 문서 수
        """
        # 각 문서에 paper_id 추가
        for doc in documents:
            pdf_url = doc.metadata.get('pdf_url', '')
            if pdf_url:
                arxiv_id = pdf_url.split('/')[-1].replace('.pdf', '')
                paper_id = paper_id_mapping.get(arxiv_id)

                if paper_id:
                    doc.metadata['paper_id'] = paper_id

        # Vector DB에 저장
        return self.add_documents(documents)


# 실행 스크립트
if __name__ == "__main__":
    import json
    from src.data.document_loader import PaperDocumentLoader

    # 1. PDF 로드 및 분할
    loader = PaperDocumentLoader()
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    print(f"\n총 {len(chunks)}개의 청크 생성 완료")

    # 2. paper_id_mapping 로드
    with open("data/processed/paper_id_mapping.json", 'r') as f:
        paper_id_mapping = json.load(f)

    # 3. 임베딩 및 Vector DB 저장
    embedding_manager = PaperEmbeddingManager()
    saved_count = embedding_manager.add_documents_with_paper_id(
        chunks,
        paper_id_mapping
    )

    print(f"\n총 {saved_count}개의 문서를 Vector DB에 저장 완료")
```

---

## PostgreSQL 데이터베이스 설정

### 4. 스키마 생성 및 메타데이터 저장

**파일 경로**: `scripts/setup_database.py`

**구현 방법**:
1. `create_tables` 함수 구현
   - PostgreSQL 연결 및 커서 생성
   - papers 테이블 생성 (paper_id, title, authors, publish_date, source, url, category, citation_count, abstract, created_at)
   - glossary 테이블 생성 (term_id, term, definition, easy_explanation, hard_explanation, category, difficulty_level, related_terms, examples, created_at)
   - 인덱스 생성 (papers의 title, category, date 및 glossary의 term)
   - 커밋 및 완료 메시지 출력

2. `insert_paper_metadata` 함수 구현
   - JSON 메타데이터 리스트를 순회
   - 각 논문 데이터를 papers 테이블에 INSERT
   - ON CONFLICT (url) DO NOTHING으로 중복 방지
   - RETURNING paper_id로 삽입된 ID 조회
   - arxiv_id와 paper_id 매핑 딕셔너리 생성
   - 오류 발생 시 해당 논문 건너뛰기
   - 매핑 딕셔너리 반환

3. `insert_glossary_data` 함수 구현
   - 용어집 초기 데이터 리스트 정의 (Attention Mechanism, Fine-tuning, BLEU Score 등)
   - 각 용어 데이터를 glossary 테이블에 INSERT
   - ON CONFLICT (term) DO NOTHING으로 중복 방지
   - 오류 발생 시 해당 용어 건너뛰기

4. 실행 스크립트 작성
   - PostgreSQL 연결
   - create_tables()로 테이블 및 인덱스 생성
   - JSON 파일에서 논문 메타데이터 로드
   - insert_paper_metadata()로 논문 데이터 삽입
   - insert_glossary_data()로 용어집 데이터 삽입
   - paper_id_mapping을 JSON 파일로 저장

---

## 인수인계 문서

### 완료 항목 체크리스트
- [ ] arXiv에서 최소 50편 논문 수집
- [ ] PDF → Langchain Document 변환 완료
- [ ] PostgreSQL 스키마 생성 (papers, glossary)
- [ ] PostgreSQL + pgvector 컬렉션 생성 (paper_chunks, glossary_embeddings)
- [ ] 논문 메타데이터 PostgreSQL 저장
- [ ] 논문 임베딩 pgvector 저장
- [ ] 용어집 초기 데이터 삽입
- [ ] paper_id_mapping.json 생성

### 인수인계 내용

#### 1. 데이터 위치
- **PDF 파일**: `data/raw/pdfs/`
- **메타데이터**: `data/raw/arxiv_papers_metadata.json`
- **paper_id 매핑**: `data/processed/paper_id_mapping.json`
- **Vector DB**: PostgreSQL + pgvector (connection: `postgresql://user:password@localhost:5432/papers`)

#### 2. 추가 데이터 수집 방법
```bash
# 추가 논문 수집
python scripts/collect_arxiv_papers.py --query "새로운 키워드" --max-results 50

# 임베딩 및 DB 저장
python src/data/embeddings.py
```

#### 3. DB 연결 정보
- PostgreSQL + pgvector: `postgresql://user:password@localhost:5432/papers`
  - 논문 메타데이터: `papers` 테이블
  - 용어집: `glossary` 테이블
  - 벡터 임베딩: pgvector extension 사용

#### 4. 주의사항
- OpenAI API 키 환경변수 설정 필수: `OPENAI_API_KEY`
- 임베딩 생성 시 배치 크기 조절 (API 속도 제한)
- PDF 다운로드 실패 시 재시도 로직 필요

---

## Feature 브랜치

- `feature/data-collection` - arXiv 데이터 수집
- `feature/document-processing` - Document Loader/Splitter
- `feature/database-setup` - PostgreSQL + pgvector 설정

---

## 참고 자료

- arXiv API: https://info.arxiv.org/help/api/index.html
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- Langchain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Langchain Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- pgvector 문서: https://github.com/pgvector/pgvector
