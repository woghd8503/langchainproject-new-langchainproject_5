"""FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë‚´ìš© í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/inspect_vector_db.py
    
ë²¡í„° DBì— ì €ì¥ëœ ë¬¸ì„œ, ë©”íƒ€ë°ì´í„°, ì¸ë±ìŠ¤ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (PyTorch ê²½ê³  ë°©ì§€)
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*torch.classes.*")

# LangChain ì„í¬íŠ¸
from langchain_core.documents import Document
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ê²½ë¡œ ì„¤ì •
FAISS_DIR = ROOT / "data" / "vectordb" / "papers_faiss"


def inspect_vector_db():
    """FAISS ë²¡í„° DB ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    print("=" * 80)
    print("FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—´ëŒ")
    print("=" * 80)
    
    # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
    if not FAISS_DIR.exists():
        print(f"âŒ ë²¡í„° DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {FAISS_DIR}")
        print("   ë¨¼ì € ì•±ì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ìƒ‰ì¸ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    index_files = list(FAISS_DIR.glob("*.faiss")) + list(FAISS_DIR.glob("*.pkl"))
    if not index_files:
        print(f"âŒ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {FAISS_DIR}")
        return
    
    print(f"ğŸ“ ë²¡í„° DB ê²½ë¡œ: {FAISS_DIR}")
    print(f"ğŸ“Š ì¸ë±ìŠ¤ íŒŒì¼ ìˆ˜: {len(index_files)}")
    print()
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    print("ğŸ”„ FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
    try:
        vs = FAISS.load_local(
            FAISS_DIR.as_posix(), 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    print()
    print("=" * 80)
    print("ğŸ“Š ì¸ë±ìŠ¤ í†µê³„")
    print("=" * 80)
    
    # ì¸ë±ìŠ¤ ê¸°ë³¸ ì •ë³´
    try:
        total_docs = vs.index.ntotal
        print(f"ì´ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {total_docs}")
        
        if hasattr(vs.index, 'd'):
            print(f"ë²¡í„° ì°¨ì›: {vs.index.d}")
        
        print(f"ì¸ë±ìŠ¤ íƒ€ì…: {type(vs.index).__name__}")
    except Exception as e:
        print(f"âš ï¸ ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        total_docs = 0
    
    print()
    
    # ì €ì¥ëœ ë¬¸ì„œ í™•ì¸
    print("=" * 80)
    print("ğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡")
    print("=" * 80)
    
    if total_docs == 0:
        print("âš ï¸ ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ ì‹œë„ (FAISSëŠ” ì§ì ‘ ë¬¸ì„œ ì ‘ê·¼ì´ ì œí•œì )
    # ëŒ€ì‹  ìƒ˜í”Œ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ í™•ì¸
    print("\nğŸ” ìƒ˜í”Œ ê²€ìƒ‰ìœ¼ë¡œ ì €ì¥ëœ ë¬¸ì„œ í™•ì¸...\n")
    
    # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    sample_queries = ["paper", "research", "arxiv", "title", "summary"]
    
    all_retrieved_docs = []
    seen_contents = set()
    
    for query in sample_queries:
        try:
            docs = vs.similarity_search(query, k=5)
            for doc in docs:
                # ì¤‘ë³µ ì œê±°
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_retrieved_docs.append(doc)
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì‹¤ì œ ë°œê²¬ëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜
    unique_docs = len(all_retrieved_docs)
    print(f"ë°œê²¬ëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜: {unique_docs}")
    print()
    
    # ìƒìœ„ 10ê°œ ë¬¸ì„œ ìƒì„¸ ì¶œë ¥
    print("=" * 80)
    print(f"ğŸ“‹ ìƒìœ„ {min(10, unique_docs)}ê°œ ë¬¸ì„œ ìƒì„¸")
    print("=" * 80)
    
    for idx, doc in enumerate(all_retrieved_docs[:10], 1):
        print(f"\n[ë¬¸ì„œ {idx}]")
        print("-" * 80)
        
        # ë©”íƒ€ë°ì´í„° ì¶œë ¥
        if doc.metadata:
            print("ğŸ“Œ ë©”íƒ€ë°ì´í„°:")
            for key, value in doc.metadata.items():
                print(f"   {key}: {value}")
            print()
        
        # ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥ (ìµœëŒ€ 300ì)
        content = doc.page_content
        if len(content) > 300:
            print(f"ğŸ“ ë³¸ë¬¸ (ì²˜ìŒ 300ì):\n{content[:300]}...")
        else:
            print(f"ğŸ“ ë³¸ë¬¸:\n{content}")
        
        print()
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print()
    print("=" * 80)
    print("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    test_queries = [
        "machine learning",
        "neural network", 
        "transformer",
    ]
    
    for query in test_queries:
        print(f"\nê²€ìƒ‰ì–´: '{query}'")
        try:
            results = vs.similarity_search_with_score(query, k=3)
            print(f"  â†’ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            for i, (doc, score) in enumerate(results[:3], 1):
                print(f"\n  [{i}] ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                if doc.metadata:
                    title = doc.metadata.get("Title") or doc.metadata.get("title") or "ì œëª© ì—†ìŒ"
                    print(f"      ì œëª©: {title[:80]}...")
                print(f"      ë³¸ë¬¸: {doc.page_content[:150]}...")
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    print()
    print("=" * 80)
    print("âœ… ì—´ëŒ ì™„ë£Œ")
    print("=" * 80)


if __name__ == "__main__":
    inspect_vector_db()
