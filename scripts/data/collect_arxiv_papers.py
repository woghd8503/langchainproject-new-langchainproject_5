"""arXiv ë…¼ë¬¸ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸.

ArxivClientë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ë©° ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
ExperimentManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì¶”ì í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/data/collect_arxiv_papers.py
"""

from __future__ import annotations

import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT))

from src.papers.domain.dto import PaperDTO
from src.papers.infra.arxiv_client import ArxivClient
from src.utils.experiment_manager import ExperimentManager


class ArxivPaperCollector:
    """arXiv ë…¼ë¬¸ ìˆ˜ì§‘ í´ë˜ìŠ¤.

    ArxivClientë¥¼ ë˜í•‘í•˜ì—¬ PDF ë‹¤ìš´ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° JSON ì €ì¥ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        save_dir: str | Path = "data/raw/pdfs",
        metadata_dir: str | Path = "data/raw/json",
        exp_manager: Optional[ExperimentManager] = None,
    ) -> None:
        """ArxivPaperCollector ì´ˆê¸°í™”.

        Args:
            save_dir: PDF íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            metadata_dir: ë©”íƒ€ë°ì´í„° JSON ì €ì¥ ë””ë ‰í† ë¦¬
            exp_manager: ExperimentManager ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
        """
        self.save_dir = Path(save_dir)
        self.metadata_dir = Path(metadata_dir)
        self.exp_manager = exp_manager
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_dir.mkdir(parents=True, exist_ok=True)
        
        # ArxivClient ì´ˆê¸°í™”
        self.client = ArxivClient(max_results_default=50)

    def collect_papers(self, query: str, max_results: int = 50) -> List[Dict]:
        """arXivì—ì„œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜

        Returns:
            ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœ)
        """
        if self.exp_manager:
            self.exp_manager.logger.write(f"ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘: query='{query}', max_results={max_results}")

        # ArxivClientë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
        paper_dtos = self.client.search(query=query, max_results=max_results)
        
        papers_data = []
        downloaded_count = 0
        failed_count = 0

        # ì§€ì—° ì„í¬íŠ¸
        import arxiv  # type: ignore

        for dto in paper_dtos:
            try:
                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                paper_info = {
                    "title": dto.title,
                    "authors": dto.authors,
                    "published_date": dto.published_at.strftime("%Y-%m-%d") if dto.published_at else None,
                    "summary": dto.abstract,
                    "pdf_url": dto.pdf_url or f"https://arxiv.org/pdf/{dto.arxiv_id}.pdf",
                    "entry_id": f"https://arxiv.org/abs/{dto.arxiv_id}",
                    "categories": [],  # ArxivClientì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì—†ìŒ
                    "primary_category": None,
                }

                # PDF ë‹¤ìš´ë¡œë“œ
                pdf_filename = self.save_dir / f"{dto.arxiv_id}.pdf"
                
                if pdf_filename.exists():
                    if self.exp_manager:
                        self.exp_manager.logger.write(f"PDF ì´ë¯¸ ì¡´ì¬: {dto.arxiv_id}.pdf")
                    papers_data.append(paper_info)
                    continue

                try:
                # PDF URLë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
                import urllib.request
                
                pdf_url = dto.pdf_url or f"https://arxiv.org/pdf/{dto.arxiv_id}.pdf"
                
                # ì¬ì‹œë„ ë¡œì§
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        urllib.request.urlretrieve(pdf_url, pdf_filename)
                        break
                    except Exception as retry_e:  # noqa: BLE001
                        if attempt == max_retries - 1:
                            raise retry_e
                        time.sleep(1)
                    
                    downloaded_count += 1
                    if self.exp_manager:
                        self.exp_manager.logger.write(f"PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dto.arxiv_id}.pdf")
                    
                    # ë‹¤ìš´ë¡œë“œ í›„ ì§§ì€ ëŒ€ê¸° (API Rate Limit ë°©ì§€)
                    time.sleep(0.5)
                    
                except Exception as e:  # noqa: BLE001
                    failed_count += 1
                    if self.exp_manager:
                        self.exp_manager.logger.write(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {dto.arxiv_id}.pdf - {e}")
                    # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨í•´ë„ ë©”íƒ€ë°ì´í„°ëŠ” ì €ì¥
                    continue

                papers_data.append(paper_info)

            except Exception as e:  # noqa: BLE001
                failed_count += 1
                if self.exp_manager:
                    self.exp_manager.logger.write(f"ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {dto.arxiv_id} - {e}")
                continue

        if self.exp_manager:
            self.exp_manager.logger.write(
                f"ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(paper_dtos)}ê±´, "
                f"ì„±ê³µ {downloaded_count}ê±´, ì‹¤íŒ¨ {failed_count}ê±´"
            )

        return papers_data

    def collect_by_keywords(
        self, keywords: List[str], per_keyword: int = 15
    ) -> List[Dict]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            per_keyword: í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘í•  ë…¼ë¬¸ ìˆ˜

        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if self.exp_manager:
            self.exp_manager.logger.write(
                f"í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ, "
                f"í‚¤ì›Œë“œë‹¹ {per_keyword}í¸"
            )

        all_papers = []
        
        for idx, keyword in enumerate(keywords, 1):
            if self.exp_manager:
                self.exp_manager.logger.write(
                    f"[{idx}/{len(keywords)}] í‚¤ì›Œë“œ ìˆ˜ì§‘ ì¤‘: '{keyword}'"
                )
            
            papers = self.collect_papers(query=keyword, max_results=per_keyword)
            all_papers.extend(papers)
            
            # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸° (API Rate Limit ë°©ì§€)
            if idx < len(keywords):
                time.sleep(2)

        # ì¤‘ë³µ ì œê±°
        unique_papers = self.remove_duplicates(all_papers)

        if self.exp_manager:
            self.exp_manager.logger.write(
                f"í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_papers)}ê±´, "
                f"ì¤‘ë³µ ì œê±° í›„ {len(unique_papers)}ê±´"
            )

        return unique_papers

    @staticmethod
    def remove_duplicates(papers: List[Dict]) -> List[Dict]:
        """ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë…¼ë¬¸ì„ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            papers: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        seen_titles = set()
        unique_papers = []

        for paper in papers:
            # ì œëª©ì„ ì†Œë¬¸ìë¡œ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ í™•ì¸
            title_lower = paper.get("title", "").lower().strip()
            
            if title_lower and title_lower not in seen_titles:
                seen_titles.add(title_lower)
                unique_papers.append(paper)

        return unique_papers

    def save_metadata(self, papers: List[Dict], filename: str = "arxiv_papers_metadata.json") -> Path:
        """ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            papers: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            filename: ì €ì¥í•  íŒŒì¼ëª…

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        metadata_path = self.metadata_dir / filename
        
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(papers, f, indent=2, ensure_ascii=False)

        if self.exp_manager:
            self.exp_manager.logger.write(
                f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path} ({len(papers)}ê±´)"
            )

        return metadata_path


def main() -> int:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    
    # AI/ML í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    keywords = [
        "transformer attention",
        "BERT GPT",
        "large language model",
        "retrieval augmented generation",
        "neural machine translation",
        "question answering",
        "AI agent",
    ]

    # ExperimentManagerë¡œ ì‹¤í—˜ ì¶”ì 
    with ExperimentManager() as exp:
        exp.logger.write("arXiv ë…¼ë¬¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        exp.update_metadata(
            user_query="arXiv ë…¼ë¬¸ ìˆ˜ì§‘",
            difficulty="easy",
            tool_used="arxiv_collector"
        )

        # Collector ì´ˆê¸°í™”
        collector = ArxivPaperCollector(
            save_dir=ROOT / "data/raw/pdfs",
            metadata_dir=ROOT / "data/raw/json",
            exp_manager=exp,
        )

        # í‚¤ì›Œë“œë³„ ìˆ˜ì§‘
        papers = collector.collect_by_keywords(keywords, per_keyword=15)

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = collector.save_metadata(papers)
        
        exp.logger.write(f"ì´ {len(papers)}ê°œ ê³ ìœ  ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        exp.update_metadata(
            success=True,
            response_time_ms=None,
        )

        print(f"\nâœ… ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(papers)}ê°œ")
        print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
        print(f"ğŸ“ PDF íŒŒì¼ ì €ì¥: {collector.save_dir}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

