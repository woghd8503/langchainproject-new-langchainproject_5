"""PGVector ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë‚´ìš© í™•ì¸ ìŠ¤í¬ë¦½íŠ¸.

ì‚¬ìš©ë²•:
    python scripts/data/inspect_pgvector_db.py
    
PGVectorì— ì €ì¥ëœ ë¬¸ì„œ, ë©”íƒ€ë°ì´í„°, ì¸ë±ìŠ¤ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT))

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(ROOT / ".env")


def inspect_pgvector_db():
    """PGVector ë²¡í„° DB ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    print("=" * 80)
    print("PGVector ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—´ëŒ")
    print("=" * 80)
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        print("âŒ DATABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— DATABASE_URLì„ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤: {database_url.split('@')[-1] if '@' in database_url else database_url}")
    print()
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    try:
        embeddings = OpenAIEmbeddings(
            model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"),
            api_key=openai_api_key,
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # PGVector ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    print("ğŸ”„ PGVector ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì¤‘...")
    try:
        vectorstore = PGVector(
            collection_name="paper_chunks",
            connection=database_url,
            embeddings=embeddings,
        )
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    print()
    print("=" * 80)
    print("ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´")
    print("=" * 80)
    
    # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸ (Langchain PGVector ë‚´ë¶€ í…Œì´ë¸” ì¡°íšŒ)
    try:
        import psycopg2
        
        # ì—°ê²° ë¬¸ìì—´ íŒŒì‹±
        conn = psycopg2.connect(database_url)
        cur = conn.cursor()
        
        # ì»¬ë ‰ì…˜ ê°œìˆ˜ í™•ì¸
        cur.execute("""
            SELECT COUNT(*) 
            FROM langchain_pg_collection 
            WHERE collection_name = 'paper_chunks'
        """)
        collection_count = cur.fetchone()[0]
        print(f"ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€: {'âœ… ìˆìŒ' if collection_count > 0 else 'âŒ ì—†ìŒ'}")
        
        # ë²¡í„° ìˆ˜ í™•ì¸
        cur.execute("""
            SELECT COUNT(*) 
            FROM langchain_pg_embedding 
            WHERE collection_id IN (
                SELECT uuid FROM langchain_pg_collection 
                WHERE collection_name = 'paper_chunks'
            )
        """)
        total_vectors = cur.fetchone()[0]
        print(f"ì´ ì €ì¥ëœ ë²¡í„° ìˆ˜: {total_vectors}")
        
        cur.close()
        conn.close()
        
        if total_vectors == 0:
            print("\nâš ï¸ ì €ì¥ëœ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¨¼ì € load_embeddings.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë²¡í„°ë¥¼ ì €ì¥í•˜ì„¸ìš”.")
            return
        
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        total_vectors = 0
    
    print()
    
    # ìƒ˜í”Œ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ í™•ì¸
    print("=" * 80)
    print("ğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸")
    print("=" * 80)
    
    sample_queries = ["paper", "research", "arxiv", "transformer", "neural"]
    
    all_retrieved_docs = []
    seen_contents = set()
    
    for query in sample_queries:
        try:
            docs = vectorstore.similarity_search(query, k=5)
            for doc in docs:
                # ì¤‘ë³µ ì œê±°
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_retrieved_docs.append(doc)
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì‹¤ì œ ë°œê²¬ëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜
    unique_docs = len(all_retrieved_docs)
    print(f"ë°œê²¬ëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜: {unique_docs}")
    print()
    
    # ìƒìœ„ 5ê°œ ë¬¸ì„œ ìƒì„¸ ì¶œë ¥
    print("=" * 80)
    print(f"ğŸ“‹ ìƒìœ„ {min(5, unique_docs)}ê°œ ë¬¸ì„œ ìƒì„¸")
    print("=" * 80)
    
    for idx, doc in enumerate(all_retrieved_docs[:5], 1):
        print(f"\n[ë¬¸ì„œ {idx}]")
        print("-" * 80)
        
        # ë©”íƒ€ë°ì´í„° ì¶œë ¥
        if doc.metadata:
            print("ğŸ“Œ ë©”íƒ€ë°ì´í„°:")
            for key, value in doc.metadata.items():
                print(f"   {key}: {value}")
            print()
        
        # ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥ (ìµœëŒ€ 300ì)
        content = doc.page_content
        if len(content) > 300:
            print(f"ğŸ“ ë³¸ë¬¸ (ì²˜ìŒ 300ì):\n{content[:300]}...")
        else:
            print(f"ğŸ“ ë³¸ë¬¸:\n{content}")
        
        print()
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print()
    print("=" * 80)
    print("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    test_queries = [
        "machine learning",
        "neural network",
        "transformer",
    ]
    
    for query in test_queries:
        print(f"\nê²€ìƒ‰ì–´: '{query}'")
        try:
            results = vectorstore.similarity_search_with_score(query, k=3)
            print(f"  â†’ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            for i, (doc, score) in enumerate(results[:3], 1):
                print(f"\n  [{i}] ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                if doc.metadata:
                    paper_id = doc.metadata.get("paper_id") or "ì—†ìŒ"
                    arxiv_id = doc.metadata.get("arxiv_id") or "ì—†ìŒ"
                    print(f"      paper_id: {paper_id}, arxiv_id: {arxiv_id}")
                print(f"      ë³¸ë¬¸: {doc.page_content[:150]}...")
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    print()
    print("=" * 80)
    print("âœ… ì—´ëŒ ì™„ë£Œ")
    print("=" * 80)


if __name__ == "__main__":
    inspect_pgvector_db()

