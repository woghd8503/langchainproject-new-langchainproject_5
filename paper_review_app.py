"""Streamlit ì•±: ë…¼ë¬¸ ë¦¬ë·° RAG & ì—ì´ì „íŠ¸ (ìµœì†Œ ê¸°ëŠ¥ ë²„ì „)

êµ¬ì„± ê°œìš” (ìµœì†Œ ë‹¨ìœ„):
- ì›¹ ê²€ìƒ‰(duckduckgo, arXiv)ì„ í†µí•´ ë…¼ë¬¸ í›„ë³´ íƒìƒ‰
- ì„ íƒí•œ ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• /ì„ë² ë”©í•˜ì—¬ ë¡œì»¬ FAISS ë²¡í„°DB(`/data/vectordb`)ì— ì €ì¥
- ë©”íƒ€ë°ì´í„°ëŠ” SQLite(`/data/rdbms/papers.db`)ë¡œ ê´€ë¦¬
- ìš©ì–´ì§‘(ìì£¼ ë“±ì¥ ìš©ì–´) ìë™ ì¶”ì¶œ â†’ RAGì‹œ ì°¸ì¡°
- ì§ˆë¬¸ ì‘ë‹µ(RAG) ì‹œ EZ(ì´ˆì‹¬ì)/HARD(ëŒ€í•™ì› ìˆ˜ì¤€) ëª¨ë“œë¡œ ì„¤ëª…
- LangChain ë„êµ¬í™”: retriever, glossary, explain (ì›¹ê²€ìƒ‰/íŒŒì¼ì €ì¥ì€ ì œì™¸í•˜ê³  ì‹ ê·œ ë„êµ¬ ì¤‘ì‹¬)

ì£¼ì˜ì‚¬í•­:
- ì™¸ë¶€ LLM í‚¤ê°€ ì—†ì„ ê²½ìš° HuggingFace ì„ë² ë”© + ë¡œì»¬/ëŒ€ì²´ LLM ì‹œë„ë¡œ ë™ì‘(ê°€ëŠ¥í•˜ë©´ OPENAI ë˜ëŠ” GROQ ë˜ëŠ” Ollama)
- ë³¸ ì•±ì€ í•™ìŠµ/ì‹¤í—˜ìš© ìµœì†Œêµ¬í˜„ì´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°/ê³ ì„±ëŠ¥ ì„¤ì •ì€ ë³„ë„ í™•ì¥ í•„ìš”

Project ê·œì¹™ ë°˜ì˜:
- ë°ì´í„°ëŠ” `/data` í•˜ìœ„ ì‚¬ìš©, ì¬ì‚¬ìš© ì½”ë“œëŠ” ì—¬ê¸° íŒŒì¼ì— ìµœì†Œí™”ë¡œ í¬í•¨(ì‹¤í—˜ í´ë”)
- í•¨ìˆ˜/í´ë˜ìŠ¤ëŠ” snake_case/PascalCase, Google ìŠ¤íƒ€ì¼ docstring ì‚¬ìš©
"""

from __future__ import annotations

import os
import re
import sqlite3
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ìµœìš°ì„  - ëª¨ë“  ì„í¬íŠ¸ ì „ì— ì„¤ì •)
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
# TORCH_LOGSëŠ” ì œê±° (ìœ íš¨í•œ ê°’ì´ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ)
# ëŒ€ì‹  logging ëª¨ë“ˆë¡œ ë¡œê¹… ë ˆë²¨ ì œì–´
if "TORCH_LOGS" in os.environ and os.environ["TORCH_LOGS"] == "+error":
    del os.environ["TORCH_LOGS"]  # ì˜ëª»ëœ ê°’ ì œê±°
# PyTorch ì¤‘ë³µ ë¡œë“œ ë°©ì§€
if "TORCH_LOADED" not in os.environ:
    os.environ["TORCH_LOADED"] = "1"

import streamlit as st
from dotenv import load_dotenv
import warnings
import logging
import sys

# PyTorch ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¹€ (ê°€ì¥ ë¨¼ì €)
warnings.filterwarnings("ignore", message=".*Examining the path of torch.classes.*")
warnings.filterwarnings("ignore", message=".*torch.classes.*")
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*TORCH_LIBRARY.*")

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª…ì‹œì  ê²½ë¡œ ì§€ì •) - OpenAI API í‚¤ ë¡œë“œ
env_path = Path(__file__).resolve().parents[1] / ".env"
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
else:
    load_dotenv(override=True)

# ë¡œê¹… ë ˆë²¨ ì¡°ì •
logging.getLogger("torch").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)

# PyTorch ì¤‘ë³µ ì„í¬íŠ¸ ë°©ì§€ - ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
if "torch" in sys.modules:
    # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ transformersë§Œ ì•ˆì „í•˜ê²Œ ë¡œë“œ
    pass

# LangChain ê´€ë ¨ (PyTorch ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ í•µì‹¬ ì„í¬íŠ¸ë§Œ)
from langchain_core.documents import Document
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# PyTorch ê´€ë ¨ ì„í¬íŠ¸ëŠ” í•„ìš”í•  ë•Œë§Œ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
RecursiveCharacterTextSplitter = None
FAISS = None
HuggingFaceEmbeddings = None

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except Exception as e:
    # PyTorch ì—ëŸ¬ë¥¼ í¬í•¨í•œ ëª¨ë“  ì„í¬íŠ¸ ì—ëŸ¬ë¥¼ ë¬´ì‹œí•˜ê³  ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
    if "TORCH_LIBRARY" in str(e) or "torch" in str(e).lower():
        pass  # ë‚˜ì¤‘ì— ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì‹œë„

try:
    from langchain_community.vectorstores import FAISS
except Exception as e:
    if "TORCH_LIBRARY" in str(e) or "torch" in str(e).lower():
        pass

try:
    # langchain-huggingfaceë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë„ (ìƒˆ íŒ¨í‚¤ì§€)
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
    except ImportError:
        # ëŒ€ì²´: ê¸°ì¡´ íŒ¨í‚¤ì§€ ì‚¬ìš© (deprecated)
        from langchain_community.embeddings import HuggingFaceEmbeddings
except Exception as e:
    if "TORCH_LIBRARY" in str(e) or "torch" in str(e).lower():
        pass

try:
    from langchain_community.chat_models import ChatOllama
except Exception:  # noqa: BLE001
    ChatOllama = None  # type: ignore[assignment, misc]

# ê²€ìƒ‰ ë„êµ¬(í‚¤ ë¶ˆí•„ìš” ìš°ì„ )ê³¼ arXiv
from langchain_community.tools import DuckDuckGoSearchRun
try:
    # êµ¬ì¡°í™” ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ” ë„êµ¬ (ê°€ëŠ¥ ì‹œ ì‚¬ìš©)
    from langchain_community.tools import DuckDuckGoSearchResults
except Exception:  # noqa: BLE001
    DuckDuckGoSearchResults = None  # type: ignore[assignment]

# Google Scholar ê°„ë‹¨ ê²€ìƒ‰ì„ ìœ„í•œ ì„í¬íŠ¸
try:
    from googlesearch import search as google_search
except ImportError:
    try:
        from googlesearch_python import search as google_search
    except ImportError:
        google_search = None
from langchain_community.document_loaders import ArxivLoader

try:
    from langchain_openai import ChatOpenAI
except Exception:  # noqa: BLE001
    ChatOpenAI = None  # type: ignore[assignment]

try:
    from langchain_groq import ChatGroq
except Exception:  # noqa: BLE001
    ChatGroq = None  # type: ignore[assignment]


# ========= ê²½ë¡œ/ìƒìˆ˜ ì„¤ì • =========
ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data"
VDB_DIR = DATA_DIR / "vectordb"
RDBMS_DIR = DATA_DIR / "rdbms"
RDBMS_DIR.mkdir(parents=True, exist_ok=True)
VDB_DIR.mkdir(parents=True, exist_ok=True)

SQLITE_PATH = RDBMS_DIR / "papers.db"
FAISS_DIR = VDB_DIR / "papers_faiss"


# ========= ìœ í‹¸ë¦¬í‹° =========
def ensure_sqlite_schema(db_path: Path) -> None:
    """SQLite ìŠ¤í‚¤ë§ˆë¥¼ ì´ˆê¸°í™”í•œë‹¤.

    Args:
        db_path: SQLite íŒŒì¼ ê²½ë¡œ
    """
    with sqlite3.connect(db_path.as_posix()) as conn:
        cur = conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS papers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT NOT NULL,
                title TEXT,
                url TEXT,
                paper_id TEXT,
                summary TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            """
        )
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS glossary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                term TEXT NOT NULL,
                definition TEXT,
                score REAL DEFAULT 0.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            """
        )
        conn.commit()


def insert_paper(
    source: str,
    title: Optional[str],
    url: Optional[str],
    paper_id: Optional[str],
    summary: Optional[str],
) -> None:
    """ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ DBì— ì €ì¥í•œë‹¤.

    Args:
        source: ìˆ˜ì§‘ ì¶œì²˜(e.g., "arxiv", "web")
        title: ë…¼ë¬¸ ì œëª©
        url: ì›ë¬¸ URL
        paper_id: ì‹ë³„ì(e.g., arXiv ID)
        summary: ê°„ë‹¨ ìš”ì•½
    """
    ensure_sqlite_schema(SQLITE_PATH)
    with sqlite3.connect(SQLITE_PATH.as_posix()) as conn:
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO papers (source, title, url, paper_id, summary) VALUES (?, ?, ?, ?, ?)",
            (source, title, url, paper_id, summary),
        )
        conn.commit()


def upsert_glossary(terms: List[Tuple[str, str, float]]) -> None:
    """ìš©ì–´ì§‘ ìš©ì–´ë¥¼ upsertì— ì¤€í•˜ê²Œ ë‹¨ìˆœ ì‚½ì…(ì¤‘ë³µ ë¬´ì‹œ)í•œë‹¤.

    Args:
        terms: (term, definition, score) ë¦¬ìŠ¤íŠ¸
    """
    ensure_sqlite_schema(SQLITE_PATH)
    with sqlite3.connect(SQLITE_PATH.as_posix()) as conn:
        cur = conn.cursor()
        cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_glossary_term ON glossary(term)")
        for term, definition, score in terms:
            try:
                cur.execute(
                    "INSERT OR IGNORE INTO glossary (term, definition, score) VALUES (?, ?, ?)",
                    (term, definition, score),
                )
            except sqlite3.Error:
                continue
        conn.commit()


def load_glossary() -> List[Tuple[str, str, float]]:
    """ì €ì¥ëœ ìš©ì–´ì§‘ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.

    Returns:
        ìš©ì–´, ì •ì˜, ì ìˆ˜ íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    ensure_sqlite_schema(SQLITE_PATH)
    with sqlite3.connect(SQLITE_PATH.as_posix()) as conn:
        cur = conn.cursor()
        cur.execute("SELECT term, definition, score FROM glossary ORDER BY score DESC, term ASC")
        rows = cur.fetchall()
    return [(r[0], r[1] or "", float(r[2] or 0.0)) for r in rows]


# ========= ì„ë² ë”©/LLM ì„ íƒ =========
def get_embeddings_model():
    """í‚¤ê°€ í•„ìš” ì—†ëŠ” ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤.

    Returns:
        HuggingFaceEmbeddings ì¸ìŠ¤í„´ìŠ¤
    """
    # ì§€ì—° ë¡œë”© ì²˜ë¦¬
    global HuggingFaceEmbeddings
    if HuggingFaceEmbeddings is None:
        try:
            # langchain-huggingfaceë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë„ (ìƒˆ íŒ¨í‚¤ì§€)
            try:
                from langchain_huggingface import HuggingFaceEmbeddings
            except ImportError:
                # ëŒ€ì²´: ê¸°ì¡´ íŒ¨í‚¤ì§€ ì‚¬ìš© (deprecated)
                from langchain_community.embeddings import HuggingFaceEmbeddings
        except Exception as e:
            raise RuntimeError(
                f"ì„ë² ë”© ëª¨ë¸ì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\n"
                f"PyTorch ì¶©ëŒ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ë³´ì„¸ìš”."
            )
    
    try:
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        # TensorFlow ì¶©ëŒ ì‹œ ëŒ€ì²´ ë°©ë²•
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        st.info("ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
        import time
        time.sleep(1)
        # ì¬ì‹œë„
        try:
            return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        except Exception:
            raise RuntimeError("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. TensorFlow/sentence-transformers ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


def get_chat_model() -> BaseChatModel:
    """OpenAI LLMì„ ë°˜í™˜í•œë‹¤.

    Returns:
        ChatOpenAI ì¸ìŠ¤í„´ìŠ¤

    Raises:
        RuntimeError: OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
    """
    if ChatOpenAI is None:
        raise RuntimeError("langchain_openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-openai")
    
    # .env íŒŒì¼ì—ì„œ ë‹¤ì‹œ ë¡œë“œ í™•ì¸
    env_path = Path(__file__).resolve().parents[1] / ".env"
    if env_path.exists():
        load_dotenv(dotenv_path=env_path, override=True)
    else:
        load_dotenv(override=True)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        env_path = Path(__file__).resolve().parents[1] / ".env"
        raise RuntimeError(
            f"OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            f".env íŒŒì¼ ìœ„ì¹˜: {env_path}\n"
            f".env íŒŒì¼ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ì„¸ìš”:\nOPENAI_API_KEY=sk-proj-..."
        )
    
    openai_key = openai_key.strip()
    # API í‚¤ í˜•ì‹ ê²€ì¦
    if not (openai_key.startswith("sk-") or openai_key.startswith("sk-proj-")):
        raise RuntimeError(
            f"OPENAI_API_KEY í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            f"í‚¤ëŠ” 'sk-' ë˜ëŠ” 'sk-proj-'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            f"í˜„ì¬ í‚¤ ì‹œì‘: {openai_key[:10]}"
        )
    
    if len(openai_key) < 20:
        raise RuntimeError(
            f"OPENAI_API_KEYê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.\n"
            f"ì˜¬ë°”ë¥¸ í‚¤ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. (í˜„ì¬ ê¸¸ì´: {len(openai_key)}ì)"
        )
    
    try:
        model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        model = ChatOpenAI(
            model=model_name,
            temperature=0.2,
            timeout=30,
            max_retries=2,  # ì¬ì‹œë„ ì„¤ì •
        )
        return model
    except Exception as e:
        error_msg = str(e)
        if "api_key" in error_msg.lower() or "authentication" in error_msg.lower():
            raise RuntimeError(
                f"OpenAI API í‚¤ ì¸ì¦ ì‹¤íŒ¨.\n"
                f"ì˜¤ë¥˜: {error_msg}\n"
                f".env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        else:
            raise RuntimeError(
                f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg}\n"
                f"ì¸í„°ë„· ì—°ê²° ë° API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )


# ========= ìˆ˜ì§‘/ë¶„í• /ìƒ‰ì¸ =========
def chunk_documents(texts: List[str], source: str) -> List[Document]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ Documentë¡œ ë³€í™˜í•˜ê³  ë¶„í• í•œë‹¤.

    Args:
        texts: ì›ë¬¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        source: ì¶œì²˜ ì‹ë³„ ë¬¸ìì—´

    Returns:
        Document ë¶„í•  ë¦¬ìŠ¤íŠ¸
    """
    # ì§€ì—° ë¡œë”© ì²˜ë¦¬
    global RecursiveCharacterTextSplitter
    if RecursiveCharacterTextSplitter is None:
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
        except Exception as e:
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°ë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    docs: List[Document] = []
    for idx, text in enumerate(texts):
        base = Document(page_content=text, metadata={"source": source, "idx": idx})
        # split_documentsëŠ” Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ extend ì‚¬ìš©
        split_docs = splitter.split_documents([base])
        docs.extend(split_docs)
    return docs


def build_or_load_faiss(docs: List[Document]):
    """FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

    Args:
        docs: ìƒ‰ì¸í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œë”©ë§Œ)

    Returns:
        FAISS ë²¡í„°ìŠ¤í† ì–´
    """
    # ì§€ì—° ë¡œë”© ì²˜ë¦¬
    global FAISS
    if FAISS is None:
        try:
            from langchain_community.vectorstores import FAISS
        except Exception as e:
            raise RuntimeError(f"FAISSë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    try:
        embeddings = get_embeddings_model()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise
    
    # Document íƒ€ì… ê²€ì¦
    if docs:
        valid_docs = []
        for d in docs:
            if isinstance(d, Document) and hasattr(d, 'page_content'):
                valid_docs.append(d)
            else:
                st.warning(f"ì˜ëª»ëœ ë¬¸ì„œ í˜•ì‹: {type(d)}")
        docs = valid_docs
    
    try:
        if FAISS_DIR.exists():
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            vs = FAISS.load_local(FAISS_DIR.as_posix(), embeddings, allow_dangerous_deserialization=True)
            if docs:
                # ìƒˆ ë¬¸ì„œ ì¶”ê°€
                vs.add_documents(docs)
                vs.save_local(FAISS_DIR.as_posix())
        elif docs:
            # ë¬¸ì„œê°€ ìˆì„ ë•Œ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            vs = FAISS.from_documents(docs, embeddings)
            vs.save_local(FAISS_DIR.as_posix())
        else:
            # ë¬¸ì„œë„ ì—†ê³  ê¸°ì¡´ ì¸ë±ìŠ¤ë„ ì—†ì„ ë•ŒëŠ” ë”ë¯¸ ë¬¸ì„œë¡œ ì´ˆê¸°í™”
            dummy_doc = Document(page_content="dummy", metadata={})
            vs = FAISS.from_documents([dummy_doc], embeddings)
        return vs
    except Exception as e:
        st.error(f"FAISS ìƒ‰ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        st.code(traceback.format_exc())
        raise


# ========= ê°„ë‹¨ ê²€ìƒ‰(DuckDuckGo + arXiv) =========
def search_papers(query: str, num_results: int = 5) -> List[Tuple[str, str]]:
    """arXivì™€ DuckDuckGoë¡œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•œë‹¤.

    Args:
        query: ê²€ìƒ‰ ì§ˆì˜
        num_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        (title, url) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    results: List[Tuple[str, str]] = []
    
    # 1) arXiv ìš°ì„  ê²€ìƒ‰ (ì•ˆì •ì )
    try:
        arxiv_docs = load_arxiv(query, max_results=3)
        for d in arxiv_docs:
            title = d.metadata.get("Title") or d.metadata.get("title") or "arXiv ë…¼ë¬¸"
            url = d.metadata.get("pdf_url") or d.metadata.get("entry_id") or ""
            if url and url.startswith("http"):
                results.append((title, url))
                if len(results) >= num_results:
                    break
    except Exception as e:
        st.write(f"arXiv ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # 2) DuckDuckGo ì¶”ê°€ ê²€ìƒ‰ (ë³´ì¡°)
    if len(results) < num_results:
        try:
            search = DuckDuckGoSearchRun()
            search_query = f"{query} arxiv research paper"
            search_text = search.run(search_query)
            
            # URL ì¶”ì¶œ ë° arxiv í•„í„°ë§
            urls = list({u for u in re.findall(r"https?://[^\s)]+", search_text) if "arxiv" in u.lower()})
            for url in urls[:num_results - len(results)]:
                results.append((f"DuckDuckGo Result {len(results)+1}", url))
        except Exception as e:
            st.write(f"DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    return results[:num_results]


def load_arxiv(query: str, max_results: int = 3) -> List[Document]:
    """arXivì—ì„œ ê°„ë‹¨íˆ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

    Args:
        query: ê²€ìƒ‰ ì§ˆì˜
        max_results: ê²°ê³¼ ìˆ˜

    Returns:
        Document ë¦¬ìŠ¤íŠ¸
    """
    try:
        # PDF ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ
        import arxiv
        client = arxiv.Client()
        search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)
        results = list(client.results(search))
        
        docs: List[Document] = []
        for result in results:
            # ë©”íƒ€ë°ì´í„°ë§Œ ì‚¬ìš©
            metadata = {
                "Title": result.title,
                "Authors": ", ".join([str(a) for a in result.authors]),
                "Summary": result.summary,
                "Published": str(result.published),
                "id": result.entry_id,
                "pdf_url": result.pdf_url if hasattr(result, 'pdf_url') else result.entry_id,
            }
            # ì œëª©ê³¼ ìš”ì•½ë§Œìœ¼ë¡œ Document ìƒì„±
            page_content = f"Title: {result.title}\n\nSummary: {result.summary}"
            docs.append(Document(page_content=page_content, metadata=metadata))
        
        return docs
    except Exception as e:
        st.write(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []


# ========= ìš©ì–´ì§‘ ì¶”ì¶œ =========
def extract_glossary_terms(llm: BaseChatModel, texts: List[str], top_k: int = 20) -> List[Tuple[str, str, float]]:
    """í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë“±ì¥/ì¤‘ìš”í•œ ìš©ì–´ë¥¼ ê°„ë‹¨íˆ ì¶”ì¶œí•œë‹¤.

    Args:
        llm: ì‚¬ìš© LLM (OpenAI)
        texts: ì›ë¬¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        top_k: ìµœëŒ€ ìš©ì–´ ìˆ˜

    Returns:
        (term, definition, score) ë¦¬ìŠ¤íŠ¸
    """
    if not texts:
        return []
    
    joined = "\n\n".join(texts)[:120000]
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ë„ˆëŠ” ê³¼í•™/ê³µí•™ ë…¼ë¬¸ì˜ í•µì‹¬ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¡°ìˆ˜ì´ë‹¤.
                - ì „ë¬¸ ìš©ì–´ë¥¼ í•œêµ­ì–´ë¡œ ì ê³ , ê°€ëŠ¥í•œ ê²½ìš° ê°„ëµ ì •ì˜ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œê³µí•˜ë¼.
                - ì¤‘ìš”ë„ ì ìˆ˜(0-1)ë¥¼ í•¨ê»˜ ì¶”ì •í•˜ë¼.
                - JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ë¼. ê° í•­ëª© í˜•ì‹: term, definition, score
                """.strip(),
            ),
            (
                "human",
                """
                ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ìš©ì–´ë¥¼ {k}ê°œê¹Œì§€ ë½‘ì•„ì„œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ë¼.
                í…ìŠ¤íŠ¸:
                {text}
                """.strip(),
            ),
        ]
    )
    
    try:
        chain = prompt | llm
        resp = chain.invoke({"text": joined, "k": top_k})
        content = resp.content if hasattr(resp, "content") else str(resp)
    except Exception as e:
        error_str = str(e)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜¤ë¥˜
        if isinstance(e, KeyError) or "missing variables" in error_str.lower():
            st.error(f"âŒ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜¤ë¥˜: {error_str}")
            return []
        
        # OpenAI API ì˜¤ë¥˜
        if "AuthenticationError" in error_str or "invalid_api_key" in error_str.lower():
            st.error("âŒ OpenAI API í‚¤ ì¸ì¦ ì‹¤íŒ¨")
            st.info("ğŸ’¡ .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        elif "timeout" in error_str.lower():
            st.error("âŒ OpenAI API íƒ€ì„ì•„ì›ƒ")
            st.info("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        elif "RateLimit" in error_str or "429" in error_str:
            st.error("âŒ OpenAI API ì‚¬ìš©ëŸ‰ ì œí•œ")
            st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        else:
            st.error(f"âŒ OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {error_str[:300]}")
        
        return []
    
    # ë§¤ìš° ê´€ëŒ€í•œ JSON ì¶”ì¶œ
    json_blob = re.findall(r"\[(?:.|\n)*?\]", content)
    items: List[Tuple[str, str, float]] = []
    if json_blob:
        import json
        try:
            parsed = json.loads(json_blob[0])
            for it in parsed:
                term = str(it.get("term", "")).strip()
                definition = str(it.get("definition", "")).strip()
                score = float(it.get("score", 0.0))
                if term:
                    items.append((term, definition, max(0.0, min(1.0, score))))
        except Exception:
            items = []

    # LLM ê²°ê³¼ê°€ ì—†ìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹±(ë¹ˆë„ ê¸°ë°˜)ìœ¼ë¡œ ëŒ€ì²´ ì¶”ì¶œ
    if not items:
        raw = joined.lower()
        # ê°„ë‹¨ í† í¬ë‚˜ì´ì¦ˆ: í•œê¸€/ì˜ë¬¸/ìˆ«ì ì¡°í•© í† í°ë§Œ ë‚¨ê¹€
        tokens = re.findall(r"[\wê°€-í£]+", raw)
        stop = {
            "the","and","for","that","with","this","from","have","has","are","was","were","can","will","into","your","you","our","their","but","not","all","any","each","other","more","most","some","such","no","nor","too","very","of","in","to","on","by","as","at","an","a","is","it","be","or","we",
            "ê·¸ë¦¬ê³ ","ë˜ëŠ”","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜","ê·¸ë¦¬ê³ ","ì´ëŠ”","ìˆëŠ”","ì—†ëŠ”","ì—ì„œ","ìœ¼ë¡œ","ì—ê²Œ","ê·¸","ì´","ì €","ê²ƒ","ìˆ˜","ë“±","ë°","í•˜ë©´","í•˜ì˜€ë‹¤","í•˜ëŠ”","í•˜ì˜€ë‹¤","ëŒ€í•œ","ê¹Œì§€","ì—ì„œ","ìœ¼ë¡œ","í•˜ëŠ”","í•˜ëŠ”ë°"
        }
        freq: dict[str, int] = {}
        for t in tokens:
            if len(t) < 2 or t in stop:
                continue
            freq[t] = freq.get(t, 0) + 1
        # ìƒìœ„ í† í° ì„ ì •
        top = sorted(freq.items(), key=lambda x: x[1], reverse=True)[: top_k]
        if top:
            max_c = max(c for _, c in top) or 1
            for term, c in top:
                score = c / max_c
                items.append((term, "", float(score)))
            # íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© ì•ˆë‚´
            st.info("LLM ì‘ë‹µì´ ë¶ˆì•ˆì •í•˜ì—¬ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ìš©ì–´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

    return items[:top_k]


# ========= RAG ì²´ì¸ =========
def build_rag_chain(llm: BaseChatModel, retriever) -> RunnableLambda:
    """RAG ì²´ì¸ì„ êµ¬ì„±í•œë‹¤.

    Args:
        llm: ì‚¬ìš© LLM
        retriever: langchain retriever

    Returns:
        Runnable ì²´ì¸
    """
    system_tmpl = (
        """
        ë„ˆëŠ” ë…¼ë¬¸ ë¦¬ë·° ë„ìš°ë¯¸ì´ë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‹ ë¢°í•˜ê³  í•œêµ­ì–´ë¡œ ë‹µí•œë‹¤.
        - EZ ëª¨ë“œ: ì´ˆì‹¬ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¹„ìœ /ì˜ˆì‹œì™€ í•¨ê»˜ ì‰½ê²Œ ì„¤ëª…
        - HARD ëª¨ë“œ: ëŒ€í•™ì› ìˆ˜ì¤€ìœ¼ë¡œ ìˆ˜ì‹/ì •ì˜/ê·¼ê±°ë¥¼ ê°„ê²°íˆ í¬í•¨
        - ì¶œì²˜ ë¬¸ë§¥ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
        """.strip()
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_tmpl),
            (
                "human",
                """
                ëª¨ë“œ: {mode}
                ì§ˆë¬¸: {question}
                ë¬¸ë§¥:
                {context}
                """.strip(),
            ),
        ]
    )

    def format_docs(docs: List[Document]) -> str:
        return "\n\n".join([d.page_content for d in docs])

    return ({"context": retriever | format_docs} | prompt | llm)  # type: ignore[return-value]


# ========= ê°„ë‹¨ ì—ì´ì „íŠ¸í˜• ë„êµ¬ =========
def tool_retrieve(vs: FAISS, query: str, k: int = 5) -> List[Document]:
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì¡°íšŒí•œë‹¤.

    Args:
        vs: FAISS ë²¡í„°ìŠ¤í† ì–´
        query: ì§ˆì˜
        k: ìƒìœ„ ë¬¸ì„œ ìˆ˜

    Returns:
        ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    return vs.as_retriever(search_kwargs={"k": k}).get_relevant_documents(query)


def tool_explain_with_mode(llm: BaseChatModel, context_docs: List[Document], question: str, mode: str) -> str:
    """ë¬¸ë§¥ê³¼ ëª¨ë“œì— ë”°ë¼ ì„¤ëª…ì„ ìƒì„±í•œë‹¤.

    Args:
        llm: ì‚¬ìš© LLM
        context_docs: ë¬¸ë§¥ ë¬¸ì„œë“¤
        question: ì§ˆë¬¸ í…ìŠ¤íŠ¸
        mode: "EZ" ë˜ëŠ” "HARD"

    Returns:
        í•œêµ­ì–´ ì„¤ëª… ë¬¸ìì—´
    """
    rag = build_rag_chain(llm, retriever=context_docs)
    # retriever ìë¦¬ì— ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë„˜ê¸°ê¸° ìœ„í•´ ê°„ë‹¨ ì–´ëŒ‘í„°
    def _retrieve(_: str) -> List[Document]:
        return context_docs

    rag = build_rag_chain(llm, retriever=RunnableLambda(_retrieve))
    ans = rag.invoke({"mode": mode, "question": question})
    return ans.content if hasattr(ans, "content") else str(ans)


# ========= Streamlit UI =========
def ui_search_and_ingest(llm: BaseChatModel) -> None:
    """íƒ­: ê²€ìƒ‰ ë° ìˆ˜ì§‘/ìƒ‰ì¸.

    Args:
        llm: ì‚¬ìš© LLM (ìš”ì•½ ë“±ì— ì´ìš©)
    """
    st.subheader("1) ë…¼ë¬¸ ê²€ìƒ‰ â†’ ì„ íƒ ìˆ˜ì§‘/ìƒ‰ì¸")
    q = st.text_input("ê²€ìƒ‰ ì§ˆì˜(í‚¤ì›Œë“œ)", value="Large Language Model alignment survey")
    if st.button("ë…¼ë¬¸ ê²€ìƒ‰"):
        with st.spinner("ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."):
            results = search_papers(q, num_results=10)
        st.session_state["web_results"] = results
        if not results:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    web_results = st.session_state.get("web_results", [])

    if web_results:
        st.write("ì›¹ ê²€ìƒ‰ ê²°ê³¼:")
        for title, url in web_results:
            st.markdown(f"- [{title}]({url})")

    # Scholarë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ arXiv ë¬¸ì„œ í‘œì‹œëŠ” ì œê±°

    st.markdown("---")
    st.write("ì„ íƒ ìˆ˜ì§‘/ìƒ‰ì¸")
    with st.form("ingest_form"):
        use_web = st.checkbox("ê²€ìƒ‰ ê²°ê³¼(URL)ë§Œ ìƒ‰ì¸")
        submitted = st.form_submit_button("ìƒ‰ì¸ ì‹¤í–‰")

    if submitted:
        texts: List[str] = []
        if use_web and web_results:
            # ìµœì†Œêµ¬í˜„: URL ìì²´ë¥¼ ë¬¸ì„œë¡œ ì €ì¥(ì‹¤ì„œë²„ë¼ë©´ í¬ë¡¤ë§/íŒŒì‹± í•„ìš”)
            texts.extend([f"URL ì°¸ê³ : {u}" for _, u in web_results])
            for _, u in web_results:
                insert_paper(source="web", title=None, url=u, paper_id=None, summary=None)

        # Scholarë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ arXiv ìƒ‰ì¸ ë¡œì§ ì œê±°

        if not texts:
            st.warning("ìƒ‰ì¸í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ í›„ ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            return

        docs = chunk_documents(texts, source="ingested")
        with st.spinner("ì„ë² ë”©/ìƒ‰ì¸ ì¤‘..."):
            try:
                vs = build_or_load_faiss(docs)
                st.success(f"ìƒ‰ì¸ ì™„ë£Œ! í˜„ì¬ ë²¡í„°DB í¬ê¸°: {vs.index.ntotal}")
            except RuntimeError as e:
                st.error(f"ìƒ‰ì¸ ì‹¤íŒ¨: {e}")
                st.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ë¬¸ì œì…ë‹ˆë‹¤. TensorFlow/sentence-transformers ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            except Exception as e:
                st.error(f"ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                st.code(traceback.format_exc())


def ui_glossary(llm: BaseChatModel) -> None:
    """íƒ­: ìš©ì–´ì§‘ ìƒì„±/ë³´ê¸°.

    Args:
        llm: ì‚¬ìš© LLM
    """
    st.subheader("2) ìš©ì–´ì§‘ ìƒì„±")
    top_k = st.slider("ìµœëŒ€ ìš©ì–´ ìˆ˜", 5, 50, 20)
    if st.button("í˜„ì¬ ì½”í¼ìŠ¤ì—ì„œ ìš©ì–´ì§‘ ì¶”ì¶œ"):
        if not FAISS_DIR.exists():
            st.warning("ë¨¼ì € ìƒ‰ì¸ì„ ìƒì„±í•˜ì„¸ìš”.")
            return
        
        try:
            vs = build_or_load_faiss([])
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì‹¤íŒ¨: {e}")
            return
        
        # ê°„ë‹¨íˆ ìƒìœ„ ì„ì˜ ë¬¸ì„œ ì¼ë¶€ë§Œ ì·¨í•´ ìš©ì–´ ì¶”ì¶œ(ìµœì†Œ êµ¬í˜„)
        try:
            sample_docs = vs.similarity_search("overview", k=20)
            texts = [d.page_content for d in sample_docs]
        except Exception as e:
            st.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return
        
        if not texts:
            st.warning("ìƒ‰ì¸ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        with st.spinner("ìš©ì–´ ì¶”ì¶œ ì¤‘..."):
            terms = extract_glossary_terms(llm, texts, top_k=top_k)
        
        if not terms:
            st.warning("ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return
        
        try:
            upsert_glossary(terms)
            st.success(f"{len(terms)}ê°œ ìš©ì–´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ìš©ì–´ ì €ì¥ ì‹¤íŒ¨: {e}")

    st.markdown("---")
    st.write("ì €ì¥ëœ ìš©ì–´ì§‘:")
    rows = load_glossary()
    if not rows:
        st.info("ì•„ì§ ìš©ì–´ì§‘ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        for term, definition, score in rows[:200]:
            st.markdown(f"- **{term}** (ì ìˆ˜ {score:.2f}) â€” {definition}")


def ui_rag_qa(llm: BaseChatModel) -> None:
    """íƒ­: RAG ì§ˆë¬¸ ì‘ë‹µ(EZ/HARD ëª¨ë“œ).

    Args:
        llm: ì‚¬ìš© LLM
    """
    st.subheader("3) RAG ì§ˆë¬¸ ì‘ë‹µ")
    if not FAISS_DIR.exists():
        st.warning("ë¨¼ì € ìƒ‰ì¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        return

    try:
        vs = build_or_load_faiss([])
        retriever = vs.as_retriever(search_kwargs={"k": 6})
        chain = build_rag_chain(llm, retriever)
    except Exception as e:
        st.error(f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return

    mode = st.radio("ì„¤ëª… ëª¨ë“œ", options=["EZ", "HARD"], horizontal=True)
    question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", "ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
    if st.button("ë‹µë³€ ìƒì„±"):
        if not question.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
            return
        
        try:
            with st.spinner("ìƒì„± ì¤‘..."):
                ans = chain.invoke({"mode": mode, "question": question})
            st.markdown(ans.content if hasattr(ans, "content") else str(ans))
        except Exception as e:
            error_msg = str(e)
            if "AuthenticationError" in error_msg or "invalid_api_key" in error_msg.lower():
                st.error("âŒ OpenAI API í‚¤ ì¸ì¦ ì‹¤íŒ¨")
                st.info("ğŸ’¡ .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {error_msg[:300]}")


def main() -> None:
    """Streamlit ë©”ì¸ ì§„ì…ì ."""
    st.set_page_config(page_title="ë…¼ë¬¸ ë¦¬ë·° RAG/Agent (ì‹¤í—˜)", page_icon="ğŸ“„", layout="wide")
    st.title("ë…¼ë¬¸ ë¦¬ë·° RAG & ì—ì´ì „íŠ¸ - ìµœì†Œ ê¸°ëŠ¥ ë°ëª¨")
    st.caption("ì›¹ê²€ìƒ‰/ìƒ‰ì¸ â†’ ìš©ì–´ì§‘ â†’ RAG(EZ/HARD)")

    with st.sidebar:
        st.markdown("### ì„¤ì •")
        
        # LLM ìƒíƒœ í‘œì‹œ
        try:
            llm = get_chat_model()
            st.success(f"âœ… OpenAI ì—°ê²°ë¨")
            
            # API í‚¤ ìƒíƒœ í‘œì‹œ
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if openai_key:
                key_preview = openai_key[:7] + "..." + openai_key[-4:] if len(openai_key) > 11 else "ì„¤ì •ë¨"
                st.caption(f"ğŸ”‘ API í‚¤: {key_preview}")
                # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
                st.caption(f"ğŸ¤– ëª¨ë¸: {model_name}")
            else:
                st.caption("ğŸ”‘ API í‚¤: ë¯¸ì„¤ì •")
        except RuntimeError as e:
            st.error(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨")
            st.code(str(e))
            env_path = Path(__file__).resolve().parents[1] / ".env"
            if env_path.exists():
                st.info(f"ğŸ’¡ .env íŒŒì¼ ìœ„ì¹˜: {env_path}")
                # .env íŒŒì¼ ë‚´ìš© í™•ì¸ (í‚¤ë§Œ)
                try:
                    with open(env_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        if "OPENAI_API_KEY" in content:
                            st.success("âœ… .env íŒŒì¼ì— OPENAI_API_KEYê°€ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            st.warning("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception:
                    pass
            else:
                st.warning(f"âš ï¸ .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {env_path}")
            llm = None
        
        st.markdown("---")
        st.markdown("ë°ì´í„° ê²½ë¡œ")
        st.code(f"VDB: {FAISS_DIR}\nDB: {SQLITE_PATH}")

    if llm is None:
        return

    tabs = st.tabs(["ê²€ìƒ‰/ìƒ‰ì¸", "ìš©ì–´ì§‘", "RAG Q&A"])
    with tabs[0]:
        ui_search_and_ingest(llm)
    with tabs[1]:
        ui_glossary(llm)
    with tabs[2]:
        ui_rag_qa(llm)


if __name__ == "__main__":
    main()


